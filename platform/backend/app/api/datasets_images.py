"""
Dataset Images API - Individual image management for datasets.

Phase 11.5: Refactored to use Labeler API instead of Platform DB.
- Labeler is Single Source of Truth for dataset metadata
- Platform accesses R2 storage using storage_path from Labeler
- Permission checking delegated to Labeler API

This module provides APIs for:
- Listing images in a dataset (with presigned URLs)
- Generating presigned URLs for individual images
- Accessing dataset files (annotations.json, etc.)
"""

import logging
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, Path as PathParam, Query
from pydantic import BaseModel

from app.db.models import User
from app.clients.labeler_client import labeler_client
from app.utils.dual_storage import dual_storage
from app.utils.dependencies import get_current_user

logger = logging.getLogger(__name__)

router = APIRouter()


# ==================== Response Models ====================

class ImageInfo(BaseModel):
    """Information about a single image"""
    filename: str
    presigned_url: str
    size: Optional[int] = None


class ImageListResponse(BaseModel):
    """Response model for image list"""
    status: str
    dataset_id: str
    total_images: int
    images: List[ImageInfo]


class PresignedUrlResponse(BaseModel):
    """Response model for presigned URL"""
    status: str
    image_path: str
    presigned_url: str
    expires_in: int  # seconds


class FileContentResponse(BaseModel):
    """Response model for file content"""
    status: str
    dataset_id: str
    filename: str
    content: dict  # JSON content


# ==================== Helper Functions ====================

async def get_dataset_with_permission(
    dataset_id: str,
    user_id: int,
    require_write: bool = False
) -> dict:
    """
    Get dataset metadata from Labeler and check permissions.

    Args:
        dataset_id: Dataset ID (UUID)
        user_id: User ID for permission check
        require_write: If True, require owner permission

    Returns:
        Dataset metadata dict with storage_path

    Raises:
        HTTPException: If dataset not found or permission denied
    """
    import httpx

    try:
        # Get dataset metadata from Labeler
        dataset = await labeler_client.get_dataset(dataset_id, user_id=user_id)

        # Check permission via Labeler
        permission = await labeler_client.check_permission(dataset_id, user_id)

        if require_write:
            if not permission.get('is_owner'):
                raise HTTPException(
                    status_code=403,
                    detail="Permission denied: Only dataset owner can modify"
                )
        else:
            if not permission.get('has_access'):
                raise HTTPException(
                    status_code=403,
                    detail="Permission denied: You don't have access to this dataset"
                )

        return dataset

    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            raise HTTPException(status_code=404, detail=f"Dataset {dataset_id} not found")
        elif e.response.status_code == 403:
            raise HTTPException(status_code=403, detail="Access denied to dataset")
        else:
            logger.error(f"Labeler API error: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to fetch dataset: {e.response.text}")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching dataset {dataset_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch dataset: {str(e)}")


# ==================== API Endpoints ====================

@router.get("/{dataset_id}/images", response_model=ImageListResponse)
async def list_dataset_images(
    dataset_id: str = PathParam(..., description="Dataset ID"),
    current_user: User = Depends(get_current_user)
):
    """
    List all images in a dataset with presigned URLs.

    **Architecture:**
    - Dataset metadata from Labeler API (Single Source of Truth)
    - Images stored in R2/S3 at storage_path location
    - Presigned URLs generated by Platform for direct browser access

    **Response:**
    - `status`: "success" or "error"
    - `dataset_id`: Dataset identifier
    - `total_images`: Number of images
    - `images`: List of image information with presigned URLs

    **Presigned URL:**
    - Valid for 1 hour (3600 seconds)
    - Direct browser access to R2/S3
    - No server load for image delivery
    """
    try:
        # 1. Get dataset from Labeler (includes permission check)
        dataset = await get_dataset_with_permission(dataset_id, current_user.id)
        storage_path = dataset.get('storage_path', '')

        logger.info(f"[IMAGES] Listing images for dataset {dataset_id}, storage_path: {storage_path}")

        # 2. List images from R2 using storage_path
        # storage_path format: "datasets/{uuid}/" or "users/{id}/datasets/{name}/"
        images_prefix = f"{storage_path}images/" if not storage_path.endswith('/') else f"{storage_path[:-1]}/images/"
        if storage_path.endswith('/'):
            images_prefix = f"{storage_path}images/"
        else:
            images_prefix = f"{storage_path}/images/"

        logger.info(f"[IMAGES] Searching in prefix: {images_prefix}")

        # List objects with prefix
        image_keys = dual_storage.list_datasets(prefix=images_prefix)
        logger.info(f"[IMAGES] Found {len(image_keys)} objects in {images_prefix}")

        # 3. Generate presigned URLs for each image
        images = []
        for image_key in image_keys:
            # Skip directories (keys ending with /)
            if image_key.endswith('/'):
                continue

            # Generate presigned URL
            presigned_url = dual_storage.generate_dataset_presigned_url(
                dataset_key=image_key,
                expiration=3600  # 1 hour
            )

            if presigned_url:
                # Extract just the filename from the full key
                filename = image_key.split('/')[-1]
                if filename:  # Skip empty filenames
                    images.append(ImageInfo(
                        filename=filename,
                        presigned_url=presigned_url
                    ))

        logger.info(f"[IMAGES] Generated presigned URLs for {len(images)} images in dataset {dataset_id}")

        return ImageListResponse(
            status="success",
            dataset_id=dataset_id,
            total_images=len(images),
            images=images
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[IMAGES] Error listing images: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to list images: {str(e)}")


@router.get("/{dataset_id}/images/{image_filename}/url", response_model=PresignedUrlResponse)
async def get_image_presigned_url(
    dataset_id: str = PathParam(..., description="Dataset ID"),
    image_filename: str = PathParam(..., description="Image filename"),
    expiration: int = Query(default=3600, description="URL expiration in seconds"),
    current_user: User = Depends(get_current_user)
):
    """
    Get a presigned URL for a specific image.

    **Use Case:**
    - Labeling tool: Load single image for annotation
    - Image viewer: Display full-resolution image
    - Download: Allow direct download link

    **Parameters:**
    - `dataset_id`: Dataset identifier
    - `image_filename`: Image filename (e.g., "photo.jpg")
    - `expiration`: URL validity in seconds (default: 3600 = 1 hour)

    **Response:**
    - `status`: "success"
    - `image_path`: Relative path in dataset
    - `presigned_url`: Direct R2 URL (valid for specified time)
    - `expires_in`: Expiration time in seconds
    """
    try:
        # 1. Get dataset from Labeler (includes permission check)
        dataset = await get_dataset_with_permission(dataset_id, current_user.id)
        storage_path = dataset.get('storage_path', '')

        # 2. Construct full object key
        if storage_path.endswith('/'):
            object_key = f"{storage_path}images/{image_filename}"
        else:
            object_key = f"{storage_path}/images/{image_filename}"

        logger.info(f"[IMAGES] Generating presigned URL for {object_key}")

        # 3. Generate presigned URL
        presigned_url = dual_storage.generate_dataset_presigned_url(
            dataset_key=object_key,
            expiration=expiration
        )

        if not presigned_url:
            raise HTTPException(status_code=500, detail="Failed to generate presigned URL")

        return PresignedUrlResponse(
            status="success",
            image_path=f"images/{image_filename}",
            presigned_url=presigned_url,
            expires_in=expiration
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[IMAGES] Error generating presigned URL: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to generate URL: {str(e)}")


@router.get("/{dataset_id}/file/{filename:path}")
async def get_dataset_file(
    dataset_id: str = PathParam(..., description="Dataset ID"),
    filename: str = PathParam(..., description="File path within dataset (e.g., annotations.json)"),
    current_user: User = Depends(get_current_user)
):
    """
    Get a file from the dataset (annotations.json, metadata, etc.).

    **Use Case:**
    - Load annotations.json for labeling information
    - Access dataset metadata files
    - Retrieve any JSON/text file from dataset storage

    **Parameters:**
    - `dataset_id`: Dataset identifier
    - `filename`: File path within dataset (e.g., "annotations.json", "metadata/info.json")

    **Response:**
    - JSON content of the file
    """
    import json

    try:
        # 1. Get dataset from Labeler (includes permission check)
        dataset = await get_dataset_with_permission(dataset_id, current_user.id)
        storage_path = dataset.get('storage_path', '')

        # 2. Construct full object key
        if storage_path.endswith('/'):
            object_key = f"{storage_path}{filename}"
        else:
            object_key = f"{storage_path}/{filename}"

        logger.info(f"[FILE] Fetching file: {object_key}")

        # 3. Get file content from R2
        try:
            response = dual_storage.external_client.get_object(
                Bucket=dual_storage.external_bucket_datasets,
                Key=object_key
            )
            content = response['Body'].read().decode('utf-8')

            # Try to parse as JSON
            try:
                json_content = json.loads(content)
                return json_content
            except json.JSONDecodeError:
                # Return as plain text if not JSON
                return {"content": content, "type": "text"}

        except dual_storage.external_client.exceptions.NoSuchKey:
            raise HTTPException(status_code=404, detail=f"File not found: {filename}")
        except Exception as e:
            logger.error(f"[FILE] Error reading file {object_key}: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to read file: {str(e)}")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[FILE] Error fetching file: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to fetch file: {str(e)}")
