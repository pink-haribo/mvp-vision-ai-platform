# ================================
# Backend Environment Variables
# ================================
#
# Setup Instructions:
#   1. Copy this file: cp .env.example .env
#   2. Update API keys and credentials
#   3. Choose your infrastructure tier (see Resource Comparison below)
#   4. Start infrastructure: cd ../infrastructure && docker-compose up -d
#   5. Start backend: poetry run uvicorn app.main:app --reload --port 8000

# ==========================================
# LLM API (REQUIRED) - OpenAI Compatible
# ==========================================
# Supports: OpenAI, Azure OpenAI, LocalAI, Ollama, vLLM, LiteLLM, etc.
#
# OpenAI Example:
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini
#
# Azure OpenAI Example:
# OPENAI_API_KEY=your-azure-api-key
# OPENAI_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment
# LLM_MODEL=gpt-4o
#
# Local Ollama Example:
# OPENAI_API_KEY=ollama
# OPENAI_BASE_URL=http://localhost:11434/v1
# LLM_MODEL=llama3.2
#
# LocalAI Example:
# OPENAI_API_KEY=localai
# OPENAI_BASE_URL=http://localhost:8080/v1
# LLM_MODEL=gpt-4
#
# Legacy (deprecated - kept for backward compatibility):
# GOOGLE_API_KEY=your-gemini-api-key-here

# ==========================================
# Database
# ==========================================
# Platform DB: Projects, training jobs, datasets, metrics, etc.
# Local Dev: PostgreSQL on port 5432
# Production: Managed PostgreSQL (Railway, AWS RDS, etc.)
DATABASE_URL=postgresql://admin:devpass@localhost:5432/platform

# ==========================================
# Shared User Database (Phase 11: Microservice Separation)
# ==========================================
# Shared between Platform and Labeler for user authentication
# Local Dev: PostgreSQL on port 5433 (separate from platform DB)
# Production: Managed PostgreSQL with separate credentials
USER_DATABASE_URL=postgresql://admin:devpass@localhost:5433/users

# ==========================================
# Redis (Multi-backend State Management)
# ==========================================
# Redis for session store, pub/sub, WebSocket state, and caching
# Local Dev: localhost:6379 with DB 0 (tests use DB 15)
# Production: Managed Redis (Railway, AWS ElastiCache, etc.)
REDIS_URL=redis://localhost:6379/0

# ==========================================
# Temporal Workflow Orchestration
# ==========================================
# Temporal server for workflow orchestration (training lifecycle)
# Local Dev: localhost:7233 (start with docker-compose)
# Production: Temporal Cloud or self-hosted cluster
TEMPORAL_HOST=localhost:7233
TEMPORAL_NAMESPACE=default

# ==========================================
# Paths (relative to platform/backend directory)
# ==========================================
UPLOAD_DIR=../data/uploads
OUTPUT_DIR=../data/outputs
MODEL_DIR=../data/models
LOG_DIR=../data/logs

# ==========================================
# Backend API Configuration
# ==========================================
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
FRONTEND_PORT=3000

# API Base URL (used by Temporal workers for callbacks)
# Local Dev: http://localhost:8000
# Production: https://your-api-domain.com
API_BASE_URL=http://localhost:8000

# ==========================================
# Training Defaults
# ==========================================
DEFAULT_EPOCHS=50
DEFAULT_BATCH_SIZE=32
DEFAULT_LEARNING_RATE=0.001

# ==========================================
# Training Services (HTTP API - Microservice Architecture)
# ==========================================
# Framework-specific service URLs
# Local Dev: Different ports for each service
# Production: Separate Railway services or K8s deployments
TIMM_SERVICE_URL=http://localhost:8001
ULTRALYTICS_SERVICE_URL=http://localhost:8002
HUGGINGFACE_SERVICE_URL=http://localhost:8003

# Fallback URL (if framework-specific URL not set)
TRAINING_SERVICE_URL=http://localhost:8001

# ==========================================
# Labeler Service (Phase 11.5: Dataset Service Integration)
# ==========================================
# Labeler Backend API for dataset metadata (Single Source of Truth)
# Local Dev: localhost:8011
# Production: Separate Railway service or K8s deployment
LABELER_API_URL=http://localhost:8011

# Service-to-Service authentication key (Platform â†’ Labeler)
# Generate with: openssl rand -base64 32
# IMPORTANT: Use different keys for dev/prod environments
LABELER_SERVICE_KEY=dev-labeler-service-key-change-in-production

# ==========================================
# Dual Storage Architecture (Internal + External)
# ==========================================
# Internal Storage: Pretrained weights, checkpoints, config schemas (MinIO)
# External Storage: Training datasets, user uploads (S3/R2/MinIO)

# Internal Storage (MinIO-Results on port 9002)
INTERNAL_STORAGE_ENDPOINT=http://localhost:9002
INTERNAL_STORAGE_ACCESS_KEY=minioadmin
INTERNAL_STORAGE_SECRET_KEY=minioadmin

# External Storage (MinIO-Datasets on port 9000 OR Cloudflare R2)
# Local Dev Example (MinIO):
EXTERNAL_STORAGE_ENDPOINT=http://localhost:9000
EXTERNAL_STORAGE_ACCESS_KEY=minioadmin
EXTERNAL_STORAGE_SECRET_KEY=minioadmin

# Production Example (Cloudflare R2):
# EXTERNAL_STORAGE_ENDPOINT=https://your-account-id.r2.cloudflarestorage.com
# EXTERNAL_STORAGE_ACCESS_KEY=your_r2_access_key
# EXTERNAL_STORAGE_SECRET_KEY=your_r2_secret_key
# EXTERNAL_STORAGE_REGION=auto
# EXTERNAL_STORAGE_USE_SSL=true

# R2 Public URL (optional - for public image access)
# R2_PUBLIC_URL=https://pub-xxxxx.r2.dev

# Internal Storage Buckets (MinIO)
INTERNAL_BUCKET_WEIGHTS=model-weights
INTERNAL_BUCKET_CHECKPOINTS=training-checkpoints
INTERNAL_BUCKET_SCHEMAS=config-schemas
INTERNAL_BUCKET_RESULTS=training-results

# External Storage Buckets (S3/R2/MinIO)
EXTERNAL_BUCKET_DATASETS=training-datasets

# ==========================================
# Legacy Storage Configuration (backward compatibility)
# ==========================================
# Deprecated in favor of INTERNAL_/EXTERNAL_ settings above
# Keep for compatibility with older code
AWS_S3_ENDPOINT_URL=http://localhost:9000
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin
S3_BUCKET_DATASETS=training-datasets
S3_BUCKET_CHECKPOINTS=training-checkpoints
S3_BUCKET_RESULTS=training-results

# ==========================================
# Observability Backends (Phase 13: Multi-Backend Support)
# ==========================================
# Comma-separated list of observability tools to use
# Options: clearml, mlflow, wandb, database
# Examples:
#   - "database" = Database only (no external dependencies)
#   - "clearml,database" = ClearML + Database backup
#   - "mlflow,wandb,database" = MLflow + W&B + Database
OBSERVABILITY_BACKENDS=database

# ==========================================
# ClearML Configuration (Phase 12.2)
# ==========================================
# ClearML for experiment tracking and model registry
# Local Dev: Open-source server (no credentials required)
# Production: ClearML Cloud or self-hosted enterprise
CLEARML_API_HOST=http://localhost:8008
CLEARML_WEB_HOST=http://localhost:8080
CLEARML_FILES_HOST=http://localhost:8081

# Credentials (empty for open-source server)
CLEARML_API_ACCESS_KEY=
CLEARML_API_SECRET_KEY=

# ==========================================
# MLflow Configuration (Optional)
# ==========================================
# MLflow for experiment tracking (if using mlflow backend)
# Local Dev: localhost:5000 (start with docker-compose)
# Production: Managed MLflow or self-hosted
MLFLOW_TRACKING_URI=http://localhost:5000

# ==========================================
# Weights & Biases Configuration (Optional)
# ==========================================
# W&B for experiment tracking (if using wandb backend)
# Get API key from: https://wandb.ai/authorize
# WANDB_API_KEY=your-wandb-api-key-here
# WANDB_PROJECT=vision-ai-platform
# WANDB_ENTITY=your-wandb-username-or-team

# ==========================================
# Loki Log Aggregation (Optional)
# ==========================================
# Loki endpoint for log queries and aggregation
# Local Dev: localhost:3100 (start with docker-compose)
# DISABLED on Windows due to query issues (re-enable in production)
LOKI_URL=http://localhost:3100
LOKI_ENABLED=false
LOKI_HTTP_PORT=3100
LOKI_GRPC_PORT=9096

# ==========================================
# Security & Authentication
# ==========================================
# JWT secret for user authentication tokens
# Generate with: openssl rand -hex 32
JWT_SECRET=your-super-secret-key-change-this-in-production
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=60
REFRESH_TOKEN_EXPIRE_DAYS=7

# Service-to-Service JWT (Phase 11.5.6: Hybrid JWT)
# Separate secret for inter-service communication
# Generate with: openssl rand -hex 32
SERVICE_JWT_SECRET=your-service-jwt-secret-change-this-in-production

# ==========================================
# CORS Configuration
# ==========================================
# Comma-separated list of allowed origins
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000,http://localhost:3001,http://127.0.0.1:3001

# ==========================================
# Logging
# ==========================================
LOG_LEVEL=INFO
LOG_FORMAT=json

# ==========================================
# Development Settings
# ==========================================
DEBUG=True
ENVIRONMENT=development

# ==========================================
# Training Execution Mode
# ==========================================
# Options:
#   - "subprocess" = Local subprocess (development/testing)
#   - "kubernetes" = Kubernetes Job (production)
# Temporal workflow orchestration is used regardless of mode
TRAINING_MODE=subprocess

# ==========================================
# GPU Settings (for subprocess mode)
# ==========================================
# USE_GPU: Enable/disable GPU usage (true/false)
# CUDA_VISIBLE_DEVICES: GPU device indices
#   - "0" = use GPU 0
#   - "0,1" = use GPU 0 and 1
#   - "-1" = use CPU only
USE_GPU=false
CUDA_VISIBLE_DEVICES=-1

# ==========================================
# Kubernetes Job Resources (for kubernetes mode)
# ==========================================
# K8S_GPU_COUNT: Number of GPUs to request
#   - 0 = CPU only (for local kind cluster testing)
#   - 1 = Single GPU (default production)
#   - 2+ = Multi-GPU training (AWS/GCP with GPU nodes)
K8S_GPU_COUNT=0
K8S_MEMORY_LIMIT=16Gi
K8S_MEMORY_REQUEST=8Gi
K8S_CPU_LIMIT=4
K8S_CPU_REQUEST=2

# ==========================================
# Resource Comparison (Infrastructure Tiers)
# ==========================================
# Tier 0 - Docker Compose (Recommended for local dev):
#   RAM: ~500MB-1GB
#   CPU: 5-10%
#   Startup: ~20 seconds
#   Services: PostgreSQL, Redis, Temporal, MinIO, ClearML
#
# Tier 1 - Kind (Local Kubernetes):
#   RAM: ~2-3GB
#   CPU: 20-30%
#   Startup: ~2 minutes
#   Services: Same as Tier 0 but in K8s pods
#
# Tier 2 - Railway (Cloud):
#   RAM: Managed by Railway
#   CPU: Managed by Railway
#   Services: Separate Railway services
#
# Tier 3 - Production Kubernetes:
#   RAM: Depends on node configuration
#   CPU: Depends on node configuration
#   Services: Full K8s deployment with autoscaling

# ==========================================
# Local Dataset Override (for testing)
# ==========================================
# Set this to test with known-good local YOLO dataset
# Useful for debugging dataset conversion issues
# LOCAL_DATASET_PATH=C:/datasets/det-coco128
