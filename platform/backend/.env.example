# ================================
# Backend Environment Variables
# ================================
#
# Setup Instructions:
#   1. Copy this file: cp .env.example .env
#   2. Update API keys and credentials
#   3. Choose your infrastructure tier (see Resource Comparison below)
#   4. Start infrastructure: cd ../infrastructure && docker-compose up -d
#   5. Start backend: poetry run uvicorn app.main:app --reload --port 8000

# ==========================================
# LLM API (REQUIRED) - Dual Provider Support
# ==========================================
# Provider Options: "openai" or "gemini"
LLM_PROVIDER=openai

# --- OpenAI Provider (LLM_PROVIDER=openai) ---
# Supports: OpenAI, Azure OpenAI, LocalAI, Ollama, vLLM, LiteLLM, etc.
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini

# --- Gemini Provider (LLM_PROVIDER=gemini) ---
# Get your API key from: https://aistudio.google.com/app/apikey
# GOOGLE_API_KEY=your-gemini-api-key-here
# LLM_MODEL=gemini-2.0-flash-exp

# ==========================================
# LLM Provider Examples
# ==========================================
#
# Example 1: OpenAI
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-xxx
# OPENAI_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4o-mini
#
# Example 2: Google Gemini
# LLM_PROVIDER=gemini
# GOOGLE_API_KEY=AIzaSy...
# LLM_MODEL=gemini-2.0-flash-exp
#
# Example 3: Local Ollama (OpenAI compatible)
# LLM_PROVIDER=openai
# OPENAI_API_KEY=ollama
# OPENAI_BASE_URL=http://localhost:11434/v1
# LLM_MODEL=llama3.2
#
# Example 4: Azure OpenAI
# LLM_PROVIDER=openai
# OPENAI_API_KEY=your-azure-api-key
# OPENAI_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment
# LLM_MODEL=gpt-4o

# ==========================================
# Database
# ==========================================
# Platform DB: Projects, training jobs, datasets, metrics, etc.
# Local Dev: PostgreSQL on port 5432
# Production: Managed PostgreSQL (Railway, AWS RDS, etc.)
DATABASE_URL=postgresql://admin:devpass@localhost:5432/platform

# ==========================================
# Shared User Database (Phase 11: Microservice Separation)
# ==========================================
# Shared between Platform and Labeler for user authentication
# Local Dev: Same PostgreSQL instance, different database (port 5432)
# Production: Same PostgreSQL instance or separate if needed
USER_DATABASE_URL=postgresql://admin:devpass@localhost:5432/users

# ==========================================
# Redis (Multi-backend State Management)
# ==========================================
# Redis for session store, pub/sub, WebSocket state, and caching
# Local Dev: localhost:6379 with DB 0 (tests use DB 15)
# Production: Managed Redis (Railway, AWS ElastiCache, etc.)
REDIS_URL=redis://localhost:6379/0

# ==========================================
# Temporal Workflow Orchestration
# ==========================================
# Temporal server for workflow orchestration (training lifecycle)
# Local Dev: localhost:7233 (start with docker-compose)
# Production: Temporal Cloud or self-hosted cluster
TEMPORAL_HOST=localhost:7233
TEMPORAL_NAMESPACE=default

# ==========================================
# Paths (relative to platform/backend directory)
# ==========================================
UPLOAD_DIR=../data/uploads
OUTPUT_DIR=../data/outputs
MODEL_DIR=../data/models
LOG_DIR=../data/logs

# ==========================================
# Backend API Configuration
# ==========================================
BACKEND_PORT=8000

# API Base URL (used by Temporal workers for callbacks)
# Local Dev: http://localhost:8000
# Production: https://your-api-domain.com
API_BASE_URL=http://localhost:8000

# ==========================================
# Training Defaults
# ==========================================
DEFAULT_EPOCHS=50
DEFAULT_BATCH_SIZE=32
DEFAULT_LEARNING_RATE=0.001

# ==========================================
# Labeler Service (Phase 11.5: Dataset Service Integration)
# ==========================================
# Labeler Backend API for dataset metadata (Single Source of Truth)
# Local Dev: localhost:8011
# Production: Separate Railway service or K8s deployment
LABELER_API_URL=http://localhost:8011

# Service-to-Service authentication key (Platform â†’ Labeler)
# Generate with: openssl rand -base64 32
# IMPORTANT: Use different keys for dev/prod environments
LABELER_SERVICE_KEY=dev-labeler-service-key-change-in-production

# ==========================================
# Dual Storage Architecture (Internal + External)
# ==========================================
# Internal Storage: Pretrained weights, checkpoints, config schemas (MinIO)
# External Storage: Training datasets, user uploads (S3/R2/MinIO)

# Internal Storage (MinIO-Results on port 9002)
INTERNAL_STORAGE_ENDPOINT=http://localhost:9002
INTERNAL_STORAGE_ACCESS_KEY=minioadmin
INTERNAL_STORAGE_SECRET_KEY=minioadmin

# External Storage (MinIO-Datasets on port 9000 OR Cloudflare R2)
# Local Dev Example (MinIO):
EXTERNAL_STORAGE_ENDPOINT=http://localhost:9000
EXTERNAL_STORAGE_ACCESS_KEY=minioadmin
EXTERNAL_STORAGE_SECRET_KEY=minioadmin

# Production Example (Cloudflare R2):
# EXTERNAL_STORAGE_ENDPOINT=https://your-account-id.r2.cloudflarestorage.com
# EXTERNAL_STORAGE_ACCESS_KEY=your_r2_access_key
# EXTERNAL_STORAGE_SECRET_KEY=your_r2_secret_key

# Internal Storage Buckets (MinIO)
INTERNAL_BUCKET_WEIGHTS=model-weights
INTERNAL_BUCKET_CHECKPOINTS=training-checkpoints
INTERNAL_BUCKET_SCHEMAS=config-schemas

# External Storage Buckets (S3/R2/MinIO)
EXTERNAL_BUCKET_DATASETS=training-datasets

# ==========================================
# Legacy Storage Configuration (backward compatibility)
# ==========================================
# Deprecated in favor of INTERNAL_/EXTERNAL_ settings above
# Keep for compatibility with older code
AWS_S3_ENDPOINT_URL=http://localhost:9000
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin
S3_BUCKET_DATASETS=training-datasets

# ==========================================
# ClearML Configuration (Phase 12.2)
# ==========================================
# ClearML for experiment tracking and model registry
# Local Dev: Open-source server (no credentials required)
# Production: ClearML Cloud or self-hosted enterprise
CLEARML_API_HOST=http://localhost:8008
CLEARML_WEB_HOST=http://localhost:8080
CLEARML_FILES_HOST=http://localhost:8081

# Credentials (empty for open-source server)
CLEARML_API_ACCESS_KEY=
CLEARML_API_SECRET_KEY=

# ==========================================
# MLflow Configuration (Optional)
# ==========================================
# MLflow for experiment tracking (if using mlflow backend)
# Local Dev: localhost:5000 (start with docker-compose)
# Production: Managed MLflow or self-hosted
MLFLOW_TRACKING_URI=http://localhost:5000

# ==========================================
# Loki Log Aggregation (Optional)
# ==========================================
# Loki endpoint for log queries and aggregation
# Local Dev: localhost:3100 (start with docker-compose)
# DISABLED on Windows due to query issues (re-enable in production)
LOKI_URL=http://localhost:3100
LOKI_ENABLED=false
LOKI_HTTP_PORT=3100
LOKI_GRPC_PORT=9096

# ==========================================
# Security & Authentication
# ==========================================
# JWT secret for user authentication tokens
# Generate with: openssl rand -hex 32
JWT_SECRET=your-super-secret-key-change-this-in-production
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=60
REFRESH_TOKEN_EXPIRE_DAYS=7

# Service-to-Service JWT (Phase 11.5.6: Hybrid JWT)
# Separate secret for inter-service communication
# Generate with: openssl rand -hex 32
SERVICE_JWT_SECRET=your-service-jwt-secret-change-this-in-production

# ==========================================
# CORS Configuration
# ==========================================
# Comma-separated list of allowed origins
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000,http://localhost:3001,http://127.0.0.1:3001

# Frontend URL (used for email links, redirects, etc.)
# Local Dev: http://localhost:3000
# Production: https://your-frontend-domain.com
FRONTEND_URL=http://localhost:3000

# ==========================================
# Logging
# ==========================================
LOG_LEVEL=INFO
LOG_FORMAT=json

# ==========================================
# Development Settings
# ==========================================
DEBUG=True
ENVIRONMENT=development

# ==========================================
# Training Execution Mode
# ==========================================
# Options:
#   - "subprocess" = Local subprocess (development/testing)
#   - "kubernetes" = Kubernetes Job (production)
# Temporal workflow orchestration is used regardless of mode
TRAINING_MODE=subprocess

# ==========================================
# Resource Comparison (Infrastructure Tiers)
# ==========================================
# Tier 0 - Docker Compose (Recommended for local dev):
#   RAM: ~500MB-1GB
#   CPU: 5-10%
#   Startup: ~20 seconds
#   Services: PostgreSQL, Redis, Temporal, MinIO, ClearML
#
# Tier 1 - Kind (Local Kubernetes):
#   RAM: ~2-3GB
#   CPU: 20-30%
#   Startup: ~2 minutes
#   Services: Same as Tier 0 but in K8s pods
#
# Tier 2 - Railway (Cloud):
#   RAM: Managed by Railway
#   CPU: Managed by Railway
#   Services: Separate Railway services
#
# Tier 3 - Production Kubernetes:
#   RAM: Depends on node configuration
#   CPU: Depends on node configuration
#   Services: Full K8s deployment with autoscaling
