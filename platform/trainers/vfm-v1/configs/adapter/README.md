# YOLO-World Adapter-based Fine-tuning

ì´ ë””ë ‰í† ë¦¬ëŠ” YOLO-World ëª¨ë¸ì— Adapterë¥¼ ì ìš©í•˜ì—¬ parameter-efficient fine-tuningì„ ìˆ˜í–‰í•˜ëŠ” ì„¤ì • íŒŒì¼ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

Adapter ê¸°ë°˜ í•™ìŠµì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ëŒ€ë¶€ë¶„ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •(freeze)í•˜ê³ , ì‘ì€ adapter ëª¨ë“ˆë§Œ í•™ìŠµí•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ fine-tuningí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

### ì¥ì 
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ 1-7%ë§Œ í•™ìŠµ
- **ë¹ ë¥¸ í•™ìŠµ**: ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë¹ ë¥¸ ìˆ˜ë ´
- **ê³¼ì í•© ë°©ì§€**: ì œí•œëœ íŒŒë¼ë¯¸í„°ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- **ëª¨ë“ˆì„±**: ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì— ëŒ€í•´ adapterë§Œ êµì²´ ê°€ëŠ¥

## ğŸ—ï¸ êµ¬ì¡°

### Phase 1 êµ¬í˜„

#### Option 1: BottleneckAdapter
```
Input â†’ Down-projection â†’ GELU â†’ Up-projection â†’ Output
  â†“                                                  â†‘
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Residual â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**íŠ¹ì§•:**
- ê°„ë‹¨í•œ bottleneck êµ¬ì¡°
- íŒŒë¼ë¯¸í„° ìˆ˜: ìµœì†Œ (~1-2%)
- ë¹ ë¥¸ í•™ìŠµ ì†ë„

#### Option 2: HierarchicalAdapter
```
Input â†’ Down â†’ GELU â†’ Up â†’ DoubleConv â†’ Attention â†’ MLP â†’ Output
  â†“                                                           â†‘
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Residual â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**íŠ¹ì§•:**
- Attention ë©”ì»¤ë‹ˆì¦˜ í¬í•¨
- íŒŒë¼ë¯¸í„° ìˆ˜: ì¤‘ê°„ (~5-7%)
- ë” ë†’ì€ í‘œí˜„ë ¥

### ì „ëµ

#### Strategy A: Neck Only
- **ì ìš© ìœ„ì¹˜**: Neck (YOLOWorldPAFPN)ë§Œ
- **Adapter ìœ„ì¹˜**: top_down, bottom_up layers
- **íŒŒë¼ë¯¸í„° ìˆ˜**: ìµœì†Œ
- **ê¶Œì¥ ì‚¬ìš©**: ë¹ ë¥¸ ì‹¤í—˜, ì œí•œëœ ë°ì´í„°

#### Strategy B: Multi-stage
- **ì ìš© ìœ„ì¹˜**: Backbone + Neck + Head
- **Adapter ìœ„ì¹˜**: 
  - Backbone: stage 2, 3, 4
  - Neck: top_down, bottom_up layers
  - Head: cls, reg branches
- **íŒŒë¼ë¯¸í„° ìˆ˜**: ì¤‘ê°„
- **ê¶Œì¥ ì‚¬ìš©**: ë” ë†’ì€ ì„±ëŠ¥ í•„ìš”ì‹œ

## ğŸ“ Config íŒŒì¼

| Config | Adapter Type | Strategy | ì„¤ëª… |
|--------|-------------|----------|------|
| `phase1_option1_strategy_a.py` | BottleneckAdapter | Neck only | ê°€ì¥ ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ì„¤ì • |
| `phase1_option2_strategy_a.py` | HierarchicalAdapter | Neck only | Attention í¬í•¨, Neckë§Œ |
| `phase1_option1_strategy_b.py` | BottleneckAdapter | Multi-stage | ì „ì²´ ë„¤íŠ¸ì›Œí¬ì— ê°„ë‹¨í•œ adapter |
| `phase1_option2_strategy_b.py` | HierarchicalAdapter | Multi-stage | ì „ì²´ ë„¤íŠ¸ì›Œí¬ì— ë³µì¡í•œ adapter |

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ í•™ìŠµ (ì´ë¯¸ ì™„ë£Œ)

ë¨¼ì € ê¸°ë³¸ YOLO-World ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤:

```bash
./tools/dist_train.sh ./configs/finetune_coco/vfm_v1_l_mvtec.py 1 --amp
```

ì´ í•™ìŠµì€ `work_dirs/vfm_v1_l_mvtec/epoch_100.pth`ì— ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

### 2. Adapter í•™ìŠµ

ê¸°ë³¸ í•™ìŠµì´ ì™„ë£Œëœ í›„, adapterë¥¼ ì¶”ê°€í•˜ì—¬ fine-tuningí•©ë‹ˆë‹¤:

#### Option 1 + Strategy A (ê¶Œì¥ ì‹œì‘ì )
```bash
./tools/dist_train.sh ./configs/adapter/phase1_option1_strategy_a.py 1 --amp
```

#### Option 2 + Strategy A
```bash
./tools/dist_train.sh ./configs/adapter/phase1_option2_strategy_a.py 1 --amp
```

#### Option 1 + Strategy B
```bash
./tools/dist_train.sh ./configs/adapter/phase1_option1_strategy_b.py 1 --amp
```

#### Option 2 + Strategy B
```bash
./tools/dist_train.sh ./configs/adapter/phase1_option2_strategy_b.py 1 --amp
```

### 3. í…ŒìŠ¤íŠ¸/ê²€ì¦

í•™ìŠµëœ adapter ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:

```bash
# Validation
python tools/test.py \
    configs/adapter/phase1_option1_strategy_a.py \
    work_dirs/phase1_option1_strategy_a/epoch_50.pth \
    --work-dir work_dirs/phase1_option1_strategy_a/test

# Inference on images
python demo/image_demo.py \
    path/to/image.jpg \
    configs/adapter/phase1_option1_strategy_a.py \
    work_dirs/phase1_option1_strategy_a/epoch_50.pth \
    --texts data/texts/mvtec.json
```

## âš™ï¸ Config ì»¤ìŠ¤í„°ë§ˆì´ì§•

### Adapter ì„¤ì • ë³€ê²½

```python
# Reduction ratio ì¡°ì • (íŒŒë¼ë¯¸í„° ìˆ˜ ì¡°ì ˆ)
adapter_reduction_ratio = 4  # 4, 8, 16 ë“±

# Adapter ìœ„ì¹˜ ë³€ê²½
adapter_positions = ['top_down', 'bottom_up', 'out']  # 'reduce', 'out' ì¶”ê°€ ê°€ëŠ¥

# Backbone adapter stages ë³€ê²½
backbone_adapter_stages = [2, 3, 4]  # 1, 2, 3, 4 ì¤‘ ì„ íƒ
```

### í•™ìŠµ ì„¤ì • ë³€ê²½

```python
# Epoch ìˆ˜ ì¡°ì •
max_epochs = 50  # ì›í•˜ëŠ” epoch ìˆ˜

# Learning rate ì¡°ì •
base_lr = 1e-4  # 1e-3, 1e-4, 1e-5 ë“±

# Batch size ì¡°ì •
train_batch_size_per_gpu = 4  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
```

### Resume ì„¤ì •

```python
# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œë§Œ (optimizer ìƒíƒœ ì œì™¸)
load_from = 'work_dirs/vfm_v1_l_mvtec/epoch_100.pth'
resume = False

# ì™„ì „í•œ resume (optimizer, scheduler í¬í•¨)
load_from = 'work_dirs/phase1_option1_strategy_a/epoch_20.pth'
resume = True
```

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ

| ì„¤ì • | ì „ì²´ íŒŒë¼ë¯¸í„° | í•™ìŠµ íŒŒë¼ë¯¸í„° | ë¹„ìœ¨ |
|------|--------------|--------------|------|
| Full fine-tuning | ~43M | ~43M | 100% |
| Option 1 + Strategy A | ~43M | ~0.5M | ~1.2% |
| Option 2 + Strategy A | ~43M | ~1.5M | ~3.5% |
| Option 1 + Strategy B | ~43M | ~1.5M | ~3.5% |
| Option 2 + Strategy B | ~43M | ~3.0M | ~7.0% |

### í•™ìŠµ ì†ë„

- **Option 1 + Strategy A**: ê°€ì¥ ë¹ ë¦„ (~1.2x faster than full)
- **Option 2 + Strategy B**: ì•½ê°„ ëŠë¦¼ (~1.05x faster than full)

## ğŸ” ëª¨ë‹ˆí„°ë§

### TensorBoard

```bash
tensorboard --logdir work_dirs/phase1_option1_strategy_a
```

### í•™ìŠµ ì¤‘ í™•ì¸ ì‚¬í•­

1. **Loss ê°ì†Œ**: Adapter í•™ìŠµë„ lossê°€ ê°ì†Œí•´ì•¼ í•¨
2. **mAP í–¥ìƒ**: Validation mAPê°€ ê¸°ë³¸ í•™ìŠµë³´ë‹¤ í–¥ìƒë˜ëŠ”ì§€ í™•ì¸
3. **Overfitting**: Validation lossê°€ ì¦ê°€í•˜ë©´ early stopping ê³ ë ¤

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### 1. Out of Memory

```python
# Configì—ì„œ batch size ì¤„ì´ê¸°
train_batch_size_per_gpu = 2  # 4 â†’ 2

# ë˜ëŠ” gradient accumulation ì‚¬ìš©
optim_wrapper = dict(
    accumulative_counts=2,  # 2 stepë§ˆë‹¤ ì—…ë°ì´íŠ¸
    ...
)
```

### 2. Adapterê°€ í•™ìŠµë˜ì§€ ì•ŠìŒ

í•™ìŠµ ë¡œê·¸ì—ì„œ í™•ì¸:
```bash
grep "lr_mult" work_dirs/phase1_option1_strategy_a/*.log
```

Adapter íŒŒë¼ë¯¸í„°ê°€ `lr_mult=1.0`ì¸ì§€ í™•ì¸

### 3. ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ ì•ŠìŒ

- Learning rate ì¡°ì •: `1e-3`, `1e-4`, `1e-5` ì‹œë„
- Adapter reduction ratio ì¡°ì •: `2`, `4`, `8` ì‹œë„
- ë” ë§ì€ epoch í•™ìŠµ
- Strategy B (Multi-stage) ì‹œë„

## ğŸ“š ì°¸ê³  ìë£Œ

- [Parameter-Efficient Transfer Learning](https://arxiv.org/abs/1902.00751)
- [Adapter-based Fine-tuning](https://arxiv.org/abs/1902.00751)
- [YOLO-World Paper](https://arxiv.org/abs/2401.17270)

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **Phase 1 ì™„ë£Œ í›„**: 4ê°€ì§€ ì„¤ì • ì¤‘ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒ ì„ íƒ
2. **Hyperparameter Tuning**: Learning rate, reduction ratio ë“± ìµœì í™”
3. **Ensemble**: ì—¬ëŸ¬ adapter ëª¨ë¸ ì•™ìƒë¸”
4. **Deployment**: ìµœì¢… ëª¨ë¸ ë°°í¬

---

## ğŸš€ Phase 2: ì£¼ìš” ì‹¤í—˜ Config (Step 1~4)

### **Step 1: Dense LoRA**
**íŒŒì¼:** `phase2_lora_v1.py`
- LoRA adapterë§Œ ì‚¬ìš© (unfreezing ì—†ìŒ)
- Trainable: 0.2% (ê°€ì¥ íš¨ìœ¨ì )

### **Step 2: Hybrid Moderate** â­ ê°€ì¥ ì•ˆì •ì 
**íŒŒì¼:** `phase2_hybrid_v1.py`
- Dense LoRA + Selective Unfreezing
- Trainable: 2.3% (2.49M params)

### **Step 3: Hybrid Aggressive**
**íŒŒì¼:** `phase2_hybrid_aggressive_v1.py`
- Dense LoRA + Aggressive Unfreezing
- Trainable: 18.8% (20.78M params)
- âš ï¸ Gradient explosion ë°œìƒ (Epoch 109)

### **Step 4-1: Rep-MoNA Conservative** â­ ê³µê°„ ë¬¸ë§¥ ì¸ì‹
**íŒŒì¼:** `phase2_step4_1_rep_mona.py`
- **Rep-MoNA LoRA** (Neckì—ë§Œ ì ìš©)
- Multi-scale spatial context [3Ã—3, 5Ã—5, 7Ã—7]
- Trainable: 2.3% (2.51M params)
- âœ… Scheduler ìˆ˜ì •ë¨ (negative LR ë°©ì§€)

### **Step 4-2: Rep-MoNA Moderate** â­ ê³µê°„ ë¬¸ë§¥ + ì„±ëŠ¥
**íŒŒì¼:** `phase2_step4_2_rep_mona.py`
- **Rep-MoNA LoRA** (Neckì—ë§Œ ì ìš©)
- Step 3 ê°œì„  ë²„ì „
- Trainable: ~5% (ì˜ˆìƒ)
- âœ… Scheduler ìˆ˜ì •ë¨ (negative LR ë°©ì§€)

### **Step 5: MoE-Enhanced RepMoNA** ğŸ†• ìµœì‹ 
**íŒŒì¼:** `phase2_step5_moe_mona.py`, `phase2_step5_moe_mona_v2.py`
- **MoE + RepMoNA** ê²°í•©
- ë™ì  Expert ì„ íƒ (Top-k Soft Gating)
- Multi-scale Experts [3Ã—3, 5Ã—5, 7Ã—7]
- SE Block (Channel Attention)
- V2: Spatial Attention + Load Balancing Loss

**ì°¸ê³  ë…¼ë¬¸:**
- Conv-LoRA (ICLR 2024): Convolution Meets LoRA
- MoE-Adapters (CVPR 2024): Mixture-of-Experts for Continual Learning
- Self-Expansion MoE (CVPR 2025): Pre-trained Models with MoE Adapters

### ë¹„êµí‘œ

| Step | Config | Adapter | Trainable % | íŠ¹ì§• |
|------|--------|---------|-------------|------|
| **Step 1** | `phase2_lora_v1.py` | LoRA | 0.2% | ê°€ì¥ íš¨ìœ¨ì  |
| **Step 2** | `phase2_hybrid_v1.py` | Dense LoRA | 2.3% | **ê°€ì¥ ì•ˆì •ì ** âœ… |
| **Step 3** | `phase2_hybrid_aggressive_v1.py` | Dense LoRA | 18.8% | Gradient explosion âŒ |
| **Step 4-1** | `phase2_step4_1_rep_mona.py` | **Rep-MoNA** | 2.3% | ê³µê°„ ë¬¸ë§¥ + ì•ˆì •ì„± â­ |
| **Step 4-2** | `phase2_step4_2_rep_mona.py` | **Rep-MoNA** | ~5% | ê³µê°„ ë¬¸ë§¥ + ì„±ëŠ¥ â­ |
| **Step 5** | `phase2_step5_moe_mona.py` | **MoE-RepMoNA** | ~6% | ë™ì  Expert ì„ íƒ ğŸ†• |
| **Step 5-V2** | `phase2_step5_moe_mona_v2.py` | **MoE-RepMoNA-V2** | ~7% | + Spatial Attn ğŸ†• |

### í•™ìŠµ ëª…ë ¹ì–´

```bash
# Step 3
python tools/train.py configs/adapter/phase2_hybrid_aggressive_v1.py \
    --work-dir work_dirs/step3_aggressive

# Step 4-1 (ê¶Œì¥)
python tools/train.py configs/adapter/phase2_step4_1_rep_mona.py \
    --work-dir work_dirs/step4_1_rep_mona_fixed

# Step 4-2 (ê¶Œì¥)
python tools/train.py configs/adapter/phase2_step4_2_rep_mona.py \
    --work-dir work_dirs/step4_2_rep_mona_fixed

# Step 5 (MoE-Enhanced) ğŸ†•
python tools/train.py configs/adapter/phase2_step5_moe_mona.py \
    --work-dir work_dirs/step5_moe_mona

# Step 5 V2 (+ Spatial Attention) ğŸ†•
python tools/train.py configs/adapter/phase2_step5_moe_mona_v2.py \
    --work-dir work_dirs/step5_moe_mona_v2
```

### Step 5 MoE-RepMoNA êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MoE-RepMoNA Adapter                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Input (B, C, H, W)                                        â”‚
â”‚         â”‚                                                   â”‚
â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚         â”‚                                      â”‚            â”‚
â”‚         â–¼                                      â”‚            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚            â”‚
â”‚   â”‚   Router    â”‚ â† Soft Gating (í•™ìŠµ ê°€ëŠ¥)     â”‚            â”‚
â”‚   â”‚  (GAP+FC)   â”‚                              â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚            â”‚
â”‚         â”‚                                      â”‚            â”‚
â”‚    [gâ‚, gâ‚‚, gâ‚ƒ] (gate weights)                 â”‚            â”‚
â”‚         â”‚                                      â”‚            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚            â”‚
â”‚   â–¼           â–¼         â–¼                      â”‚            â”‚
â”‚ Expertâ‚    Expertâ‚‚   Expertâ‚ƒ                   â”‚            â”‚
â”‚ (3Ã—3 DW)   (5Ã—5 DW)  (7Ã—7 DW)                  â”‚            â”‚
â”‚   â”‚           â”‚         â”‚                      â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚            â”‚
â”‚         â”‚ Top-k Selection                      â”‚            â”‚
â”‚         â–¼                                      â”‚            â”‚
â”‚   Weighted Sum: Î£(gáµ¢ Ã— Expertáµ¢)                â”‚            â”‚
â”‚         â”‚                                      â”‚            â”‚
â”‚         â–¼                                      â”‚            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚            â”‚
â”‚   â”‚  SE Block   â”‚ â† Channel Attention          â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚            â”‚
â”‚         â”‚                                      â”‚            â”‚
â”‚         â–¼                                      â”‚            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚            â”‚
â”‚   â”‚ Up Project  â”‚                              â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚            â”‚
â”‚         â”‚                                      â”‚            â”‚
â”‚         â–¼                                      â”‚            â”‚
â”‚      Output â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (Residual) â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì°¸ê³  ë¬¸ì„œ
- `docs/adapter_finetuning_report.md` - ì „ì²´ ì‹¤í—˜ ê²°ê³¼
- `docs/step4_rep_mona_implementation.md` - Rep-MoNA êµ¬í˜„ ìƒì„¸

