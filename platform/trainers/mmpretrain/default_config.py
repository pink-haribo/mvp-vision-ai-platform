"""
MMPreTrain Default Config Template

This module provides the default MMEngine config template for MMPreTrain training.
The template is used by train.py to generate runtime config files.

Usage:
    from default_config import get_config_template
    config_str = get_config_template(
        model_name='resnet50',
        dataset_dir='/path/to/dataset',
        work_dir='/path/to/work_dir',
        ...
    )
"""


def get_config_template(
    model_name: str,
    dataset_dir: str,
    work_dir: str,
    num_classes: int,
    base_config: str,
    pretrained_url: str = None,
    # Training parameters
    epochs: int = 100,
    batch_size: int = 32,
    learning_rate: float = 0.1,
    weight_decay: float = 0.0001,
    warmup_epochs: int = 5,
    img_size: int = 224,
    label_smoothing: float = 0.0,
    mixup_alpha: float = 0.0,
    cutmix_alpha: float = 0.0,
) -> str:
    """
    Generate MMPreTrain config content.

    Args:
        model_name: Model name (e.g., 'resnet50')
        dataset_dir: Path to dataset directory
        work_dir: Working directory for outputs
        num_classes: Number of classes
        base_config: Base config file path
        pretrained_url: URL to pretrained weights
        epochs: Number of training epochs
        batch_size: Batch size per GPU
        learning_rate: Base learning rate
        weight_decay: Weight decay for optimizer
        warmup_epochs: Number of warmup epochs
        img_size: Input image size
        label_smoothing: Label smoothing value
        mixup_alpha: MixUp alpha (0 to disable)
        cutmix_alpha: CutMix alpha (0 to disable)

    Returns:
        Config file content as string
    """
    # Load pretrained weights line
    load_from_line = f"load_from = '{pretrained_url}'" if pretrained_url else ""

    # MixUp/CutMix augmentation
    mixup_cutmix = ""
    if mixup_alpha > 0 or cutmix_alpha > 0:
        mixup_cutmix = f"""
# MixUp/CutMix augmentation
train_cfg = dict(
    augments=[
        dict(type='BatchMixup', alpha={mixup_alpha}, num_classes={num_classes}),
        dict(type='BatchCutMix', alpha={cutmix_alpha}, num_classes={num_classes}),
    ]
)
"""

    return f'''# Auto-generated MMPreTrain config for {model_name}
# Generated by Vision AI Platform

_base_ = 'mmpretrain::classification/{base_config}'

# Dataset settings
data_root = '{dataset_dir}'

# Override number of classes
model = dict(
    head=dict(
        num_classes={num_classes},
        loss=dict(type='LabelSmoothLoss', label_smooth_val={label_smoothing}, mode='original')
    )
)

train_dataloader = dict(
    batch_size={batch_size},
    num_workers=4,
    dataset=dict(
        type='CustomDataset',
        data_root=data_root,
        data_prefix='train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='RandomResizedCrop', scale={img_size}, backend='pillow'),
            dict(type='RandomFlip', prob=0.5, direction='horizontal'),
            dict(type='PackInputs')
        ]
    )
)

val_dataloader = dict(
    batch_size={batch_size * 2},
    num_workers=4,
    dataset=dict(
        type='CustomDataset',
        data_root=data_root,
        data_prefix='val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='ResizeEdge', scale={img_size}, edge='short', backend='pillow'),
            dict(type='CenterCrop', crop_size={img_size}),
            dict(type='PackInputs')
        ]
    )
)

test_dataloader = val_dataloader

# Evaluator
val_evaluator = dict(type='Accuracy', topk=(1, 5))
test_evaluator = val_evaluator

# Training schedule
train_cfg = dict(by_epoch=True, max_epochs={epochs}, val_interval=1)
val_cfg = dict()
test_cfg = dict()

# Optimizer
optim_wrapper = dict(
    optimizer=dict(type='SGD', lr={learning_rate}, momentum=0.9, weight_decay={weight_decay})
)

# Learning rate scheduler
param_scheduler = [
    dict(type='LinearLR', start_factor=0.001, by_epoch=True, begin=0, end={warmup_epochs}),
    dict(type='CosineAnnealingLR', T_max={epochs - warmup_epochs}, by_epoch=True, begin={warmup_epochs}, end={epochs})
]

# Runtime settings
default_hooks = dict(
    timer=dict(type='IterTimerHook'),
    logger=dict(type='LoggerHook', interval=100),
    param_scheduler=dict(type='ParamSchedulerHook'),
    checkpoint=dict(type='CheckpointHook', interval=1, save_best='accuracy/top1', rule='greater'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    visualization=dict(type='VisualizationHook', enable=False)
)

{mixup_cutmix}

# Work directory
work_dir = '{work_dir}'

# Randomness
randomness = dict(seed=42, deterministic=False)

{load_from_line}
'''
