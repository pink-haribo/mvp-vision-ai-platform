"""
Action handlers for conversation actions

Each action type has a corresponding handler that executes the actual logic.
"""

import logging
from typing import Dict, Any, Optional
from sqlalchemy.orm import Session

from app.models.conversation import (
    ActionType,
    GeminiActionResponse,
    ConversationState,
)
from app.db.models import (
    Session as SessionModel,
    Message as MessageModel,
    Project,
    TrainingJob,
)

logger = logging.getLogger(__name__)


class ActionHandlers:
    """
    Handles all conversation actions

    Each handler returns:
    - new_state: New conversation state
    - message: Message to show user
    - temp_data: Updated temporary data
    """

    def __init__(self, db: Session):
        self.db = db
        self.current_user_id = None  # Will be set during handle_action

    async def handle_action(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str,
        user_id: int  # Required: Current logged-in user ID
    ) -> Dict[str, Any]:
        """
        Route action to appropriate handler

        Args:
            action_response: LLM's action response
            session: Current session
            user_message: Original user message
            user_id: Current user ID for project ownership - REQUIRED

        Returns:
            dict: {
                "new_state": ConversationState,
                "message": str,
                "temp_data": dict,
                "training_job_id": int (optional)
            }
        """
        # Store current user ID for handlers to use
        self.current_user_id = user_id

        # CRITICAL: Apply fallback extraction BEFORE routing to handler
        # This ensures config data is extracted even if LLM fails
        temp_data = session.temp_data or {}
        existing_config = temp_data.get("config", {})

        # TRACE: Step 4 - Before merging
        print(f"\n[TRACE-4-MERGE] Action handler:")
        print(f"  existing_config (from session): {existing_config}")
        print(f"  action_response.current_config: {action_response.current_config}")

        # Merge LLM's config first
        if action_response.current_config:
            existing_config.update(action_response.current_config)
            print(f"  MERGED config: {existing_config}")
        else:
            print(f"  NO MERGE - action_response.current_config is None/empty")

        # Then apply fallback extraction from user message
        # CRITICAL DEBUG: Write to file
        try:
            import os
            import datetime
            log_path = "C:\\Users\\flyto\\Project\\Github\\mvp-vision-ai-platform\\mvp\\data\\logs\\fallback_debug.log"
            os.makedirs(os.path.dirname(log_path), exist_ok=True)

            with open(log_path, "a", encoding="utf-8") as f:
                timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"\n[{timestamp}] Action: {action_response.action}\n")
                f.write(f"Before: {existing_config}\n")
                f.write(f"User message: {user_message}\n")
        except Exception as e:
            print(f"LOG ERROR: {e}")

        existing_config = self._extract_from_user_message(user_message, existing_config)

        try:
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(f"After: {existing_config}\n")
        except:
            pass

        logger.warning(f"[DEBUG] Before extraction: {existing_config}")
        logger.warning(f"[DEBUG] After extraction: {existing_config}")

        # Update session temp_data with extracted config
        temp_data["config"] = existing_config
        session.temp_data = temp_data

        logger.warning(f"[FALLBACK] Config after extraction: {existing_config}")

        action = action_response.action

        handlers = {
            # ê¸°ì¡´ í•¸ë“¤ëŸ¬
            ActionType.ASK_CLARIFICATION: self._handle_ask_clarification,
            ActionType.SHOW_PROJECT_OPTIONS: self._handle_show_project_options,
            ActionType.SHOW_PROJECT_LIST: self._handle_show_project_list,
            ActionType.CREATE_PROJECT: self._handle_create_project,
            ActionType.SELECT_PROJECT: self._handle_select_project,
            ActionType.SKIP_PROJECT: self._handle_skip_project,
            ActionType.CONFIRM_TRAINING: self._handle_confirm_training,
            ActionType.START_TRAINING: self._handle_start_training,
            ActionType.ERROR: self._handle_error,

            # Phase 1 ì¶”ê°€ í•¸ë“¤ëŸ¬ - Dataset
            ActionType.ANALYZE_DATASET: self._handle_analyze_dataset,
            ActionType.SHOW_DATASET_ANALYSIS: self._handle_show_dataset_analysis,
            ActionType.LIST_DATASETS: self._handle_list_datasets,

            # Phase 1 ì¶”ê°€ í•¸ë“¤ëŸ¬ - Model
            ActionType.SEARCH_MODELS: self._handle_search_models,
            ActionType.SHOW_MODEL_INFO: self._handle_show_model_info,
            ActionType.RECOMMEND_MODELS: self._handle_recommend_models,
            ActionType.COMPARE_MODELS: self._handle_compare_models,

            # Phase 1 ì¶”ê°€ í•¸ë“¤ëŸ¬ - Training Control
            ActionType.SHOW_TRAINING_STATUS: self._handle_show_training_status,
            ActionType.STOP_TRAINING: self._handle_stop_training,
            ActionType.LIST_TRAINING_JOBS: self._handle_list_training_jobs,
            ActionType.RESUME_TRAINING: self._handle_resume_training,

            # Phase 1 ì¶”ê°€ í•¸ë“¤ëŸ¬ - Inference
            ActionType.START_QUICK_INFERENCE: self._handle_start_quick_inference,
            ActionType.START_BATCH_INFERENCE: self._handle_start_batch_inference,
            ActionType.SHOW_INFERENCE_RESULTS: self._handle_show_inference_results,

            # Phase 1 ì¶”ê°€ í•¸ë“¤ëŸ¬ - Results
            ActionType.SHOW_VALIDATION_RESULTS: self._handle_show_validation_results,
            ActionType.SHOW_CONFUSION_MATRIX: self._handle_show_confusion_matrix,

            # Phase 1 ì¶”ê°€ í•¸ë“¤ëŸ¬ - Utility
            ActionType.SHOW_HELP: self._handle_show_help,
            ActionType.RESET_CONVERSATION: self._handle_reset_conversation,
        }

        handler = handlers.get(action)
        if not handler:
            logger.error(f"Unknown action: {action}")
            return self._handle_error(action_response, session, user_message)

        # Call handler
        result = await handler(action_response, session, user_message)

        # CRITICAL: Merge our extracted config with handler's temp_data
        # This ensures extracted data isn't lost when handler returns
        # EXCEPTION: Skip merge for actions that intentionally clear state
        if action == ActionType.RESET_CONVERSATION:
            # Don't merge config back - handler wants to clear everything
            logger.info(f"[RESET] Skipping config merge for {action}")
            return result

        handler_temp_data = result.get("temp_data", {})
        handler_config = handler_temp_data.get("config", {})

        # Merge: extracted config (priority) + handler config
        final_config = {**handler_config, **existing_config}

        handler_temp_data["config"] = final_config
        result["temp_data"] = handler_temp_data

        logger.info(f"[MERGE] Final config: {final_config}")

        return result

    async def _handle_ask_clarification(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle ask_clarification action"""
        temp_data = session.temp_data or {}
        current_state = ConversationState(session.state)

        # Config is already extracted in handle_action, just retrieve it
        existing_config = temp_data.get("config", {})

        # Determine next state based on missing fields
        missing_fields = action_response.missing_fields or []

        # If asking for project_name, transition to CREATING_PROJECT
        if "project_name" in missing_fields:
            new_state = ConversationState.CREATING_PROJECT
        # If asking for config fields, stay in or go to GATHERING_CONFIG
        else:
            new_state = ConversationState.GATHERING_CONFIG

        logger.debug(f"After ask_clarification: config = {existing_config}")

        return {
            "new_state": new_state,
            "message": action_response.message,
            "temp_data": temp_data,
        }

    async def _handle_show_project_options(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle show_project_options action"""
        temp_data = session.temp_data or {}

        # Config is already extracted in handle_action, just retrieve it
        existing_config = temp_data.get("config", {})

        # Save experiment metadata
        if action_response.experiment:
            temp_data["experiment"] = action_response.experiment

        logger.debug(f"After show_project_options: config = {existing_config}")

        # Build project options message with config summary
        message = self._format_config_summary(existing_config)
        message += "\n\ní•™ìŠµ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\n\n"
        message += "1ï¸âƒ£ ì‹ ê·œ í”„ë¡œì íŠ¸ ìƒì„±\n"
        message += "2ï¸âƒ£ ê¸°ì¡´ í”„ë¡œì íŠ¸ ì„ íƒ\n"
        message += "3ï¸âƒ£ í”„ë¡œì íŠ¸ ì—†ì´ ì‹¤í—˜ë§Œ ì§„í–‰\n\n"
        message += "ì›í•˜ì‹œëŠ” ë°©ì‹ì˜ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

        return {
            "new_state": ConversationState.SELECTING_PROJECT,
            "message": message,
            "temp_data": temp_data,
        }

    async def _handle_show_project_list(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle show_project_list action"""
        # Fetch projects (excluding Uncategorized)
        projects = self.db.query(Project).filter(
            Project.name != "Uncategorized"
        ).order_by(Project.updated_at.desc()).all()

        temp_data = session.temp_data or {}

        if not projects:
            message = "ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
            message += "ë‹¤ë¥¸ ì˜µì…˜ì„ ì„ íƒí•˜ì‹œê² ìŠµë‹ˆê¹Œ?\n"
            message += "1ï¸âƒ£ ì‹ ê·œ í”„ë¡œì íŠ¸ ìƒì„±\n"
            message += "3ï¸âƒ£ í”„ë¡œì íŠ¸ ì—†ì´ ì‹¤í—˜ë§Œ ì§„í–‰"

            return {
                "new_state": ConversationState.SELECTING_PROJECT,
                "message": message,
                "temp_data": temp_data,
            }

        # Build project list
        message = "ë‹¤ìŒ í”„ë¡œì íŠ¸ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:\n\n"
        available_projects = []

        for idx, project in enumerate(projects, start=1):
            desc = f" - {project.description}" if project.description else ""
            task = f" ({project.task_type})" if project.task_type else ""

            # Count experiments
            exp_count = self.db.query(TrainingJob).filter(
                TrainingJob.project_id == project.id
            ).count()

            message += f"{idx}. **{project.name}**{task}{desc} (ì‹¤í—˜ {exp_count}ê°œ)\n"

            available_projects.append({
                "id": project.id,
                "name": project.name,
            })

        message += "\ní”„ë¡œì íŠ¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

        # Save available projects to temp_data
        temp_data["available_projects"] = available_projects

        return {
            "new_state": ConversationState.SELECTING_PROJECT,
            "message": message,
            "temp_data": temp_data,
        }

    async def _handle_create_project(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle create_project action"""
        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        # DEBUG: Log what we received
        logger.info(f"[CREATE_PROJECT] action_response.project_name = {action_response.project_name}")
        logger.info(f"[CREATE_PROJECT] action_response.project_description = {action_response.project_description}")
        logger.info(f"[CREATE_PROJECT] Full action_response: {action_response}")

        # Validate project name (required)
        project_name = action_response.project_name
        if not project_name or str(project_name).strip() == "":
            logger.warning("create_project called without project_name")
            return {
                "new_state": ConversationState.SELECTING_PROJECT,
                "message": "í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ: ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡œì íŠ¸",
                "temp_data": temp_data
            }

        # Create new project
        try:
            new_project = Project(
                name=project_name.strip(),
                description=action_response.project_description or "",
                task_type=config.get("task_type"),
                user_id=self.current_user_id  # Set owner to current logged-in user
            )
            self.db.add(new_project)
            self.db.commit()
            self.db.refresh(new_project)

            logger.info(f"Created project: {new_project.name} (ID: {new_project.id})")
        except Exception as e:
            logger.error(f"Failed to create project: {e}", exc_info=True)
            self.db.rollback()
            return {
                "new_state": ConversationState.ERROR,
                "message": f"í”„ë¡œì íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "temp_data": temp_data
            }

        # Save project ID to temp_data
        temp_data["selected_project_id"] = new_project.id

        # Build confirmation message
        message = f"í”„ë¡œì íŠ¸ '{new_project.name}'ì´(ê°€) ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
        message += self._format_config_summary(config)
        message += "\n\ní•™ìŠµì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì˜ˆ/ì•„ë‹ˆì˜¤)"

        return {
            "new_state": ConversationState.CONFIRMING,
            "message": message,
            "temp_data": temp_data,
        }

    async def _handle_select_project(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle select_project action"""
        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        project_identifier = action_response.project_identifier

        # Try to find project
        project = None

        # Check if identifier is a number (project index)
        if project_identifier.isdigit():
            available_projects = temp_data.get("available_projects", [])
            project_idx = int(project_identifier) - 1

            if 0 <= project_idx < len(available_projects):
                project_id = available_projects[project_idx]["id"]
                project = self.db.query(Project).filter(Project.id == project_id).first()

        # If not found, try to search by name
        if not project:
            project = self.db.query(Project).filter(
                Project.name.ilike(f"%{project_identifier}%")
            ).first()

        if not project:
            return {
                "new_state": ConversationState.SELECTING_PROJECT,
                "message": f"'{project_identifier}' í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.",
                "temp_data": temp_data,
            }

        # Save selected project
        temp_data["selected_project_id"] = project.id

        # Build confirmation message
        message = f"í”„ë¡œì íŠ¸ '{project.name}'ì„(ë¥¼) ì„ íƒí–ˆìŠµë‹ˆë‹¤.\n\n"
        message += self._format_config_summary(config)
        message += "\n\ní•™ìŠµì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì˜ˆ/ì•„ë‹ˆì˜¤)"

        return {
            "new_state": ConversationState.CONFIRMING,
            "message": message,
            "temp_data": temp_data,
            "selected_project_id": project.id,  # For frontend to show project detail
        }

    async def _handle_skip_project(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle skip_project action"""
        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        # Get or create Uncategorized project
        uncategorized = self.db.query(Project).filter(
            Project.name == "Uncategorized"
        ).first()

        if not uncategorized:
            uncategorized = Project(
                name="Uncategorized",
                description="í”„ë¡œì íŠ¸ ì—†ì´ ì§„í–‰í•œ ì‹¤í—˜ë“¤",
            )
            self.db.add(uncategorized)
            self.db.commit()
            self.db.refresh(uncategorized)

        temp_data["selected_project_id"] = uncategorized.id

        # Build confirmation message
        message = "í”„ë¡œì íŠ¸ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.\n\n"
        message += self._format_config_summary(config)
        message += "\n\ní•™ìŠµì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì˜ˆ/ì•„ë‹ˆì˜¤)"

        return {
            "new_state": ConversationState.CONFIRMING,
            "message": message,
            "temp_data": temp_data,
        }

    async def _handle_confirm_training(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle confirm_training action"""
        # This is just a confirmation display, wait for user response
        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        message = self._format_config_summary(config)
        message += "\n\ní•™ìŠµì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì˜ˆ/ì•„ë‹ˆì˜¤)"

        return {
            "new_state": ConversationState.CONFIRMING,
            "message": message,
            "temp_data": temp_data,
        }

    async def _handle_start_training(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle start_training action"""
        from app.db.models import Dataset

        temp_data = session.temp_data or {}
        config = action_response.config or temp_data.get("config", {})
        experiment = action_response.experiment or temp_data.get("experiment", {})
        project_id = action_response.project_id or temp_data.get("selected_project_id")

        # Resolve dataset: prefer dataset_id over dataset_path
        dataset_id = config.get("dataset_id")
        dataset_path = config.get("dataset_path")
        dataset_format = config.get("dataset_format", "imagefolder")

        if dataset_id:
            # Look up dataset in DB
            dataset = self.db.query(Dataset).filter(Dataset.id == dataset_id).first()
            if dataset:
                # Use dataset info from DB
                dataset_path = dataset_id  # Training Service will resolve this
                dataset_format = dataset.format
                logger.info(f"[START_TRAINING] Using dataset from DB: {dataset_id}")
            else:
                logger.warning(f"[START_TRAINING] Dataset ID '{dataset_id}' not found in DB, using path")

        # Create training job
        training_job = TrainingJob(
            session_id=session.id,
            project_id=project_id,
            framework=config.get("framework"),
            model_name=config.get("model_name"),
            task_type=config.get("task_type"),
            dataset_id=dataset_id,  # Store dataset ID
            dataset_path=dataset_path,  # Use resolved path or ID
            dataset_format=dataset_format,  # Use format from DB or config
            num_classes=config.get("num_classes"),
            epochs=config.get("epochs"),
            batch_size=config.get("batch_size"),
            learning_rate=config.get("learning_rate"),
            output_dir=f"./outputs/{session.id}",
            experiment_name=experiment.get("name"),
            tags=experiment.get("tags"),
            notes=experiment.get("notes"),
            status="pending",
        )
        self.db.add(training_job)
        self.db.commit()
        self.db.refresh(training_job)

        logger.info(f"Created training job: ID={training_job.id}, dataset_id={dataset_id}, dataset_path={dataset_path}")

        message = f"í•™ìŠµ ì‘ì—…ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! (Job ID: {training_job.id})\n\n"
        message += "í•™ìŠµì´ ì‹œì‘ë©ë‹ˆë‹¤. ìš°ì¸¡ íŒ¨ë„ì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        return {
            "new_state": ConversationState.COMPLETE,
            "message": message,
            "temp_data": {},  # Clear temp data
            "training_job_id": training_job.id,
        }

    async def _handle_error(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle error action"""
        error_msg = action_response.error_message or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        logger.error(f"Action error: {error_msg}")

        return {
            "new_state": ConversationState.ERROR,
            "message": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}\n\nì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•´ì£¼ì„¸ìš”.",
            "temp_data": {},
        }

    def _extract_from_user_message(
        self, user_message: str, existing_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Extract config values from user message (fallback for LLM limitations)

        This handles cases where LLM doesn't properly extract structured data.
        """
        import re
        import os

        msg_lower = user_message.lower().strip()

        # Extract dataset path (Windows/Unix paths)
        # Match patterns like: C:\path\to\dataset or /path/to/dataset
        path_pattern = r'[A-Za-z]:\\[\w\\\-\.]+|/[\w/\-\.]+'
        path_matches = re.findall(path_pattern, user_message)
        if path_matches:
            # Take the longest match (most likely to be the full path)
            dataset_path = max(path_matches, key=len)
            if 'dataset' in dataset_path.lower() or os.path.exists(dataset_path):
                existing_config["dataset_path"] = dataset_path
                logger.info(f"Extracted dataset_path from user message: {dataset_path}")

        # Extract default values (Korean & English)
        if any(keyword in msg_lower for keyword in ["ê¸°ë³¸", "default", "ê¸°ë³¸ê°’"]):
            if "epochs" not in existing_config or existing_config.get("epochs") is None:
                existing_config["epochs"] = 50
                logger.info("Applied default epochs: 50")
            if "batch_size" not in existing_config or existing_config.get("batch_size") is None:
                existing_config["batch_size"] = 32
                logger.info("Applied default batch_size: 32")
            if "learning_rate" not in existing_config or existing_config.get("learning_rate") is None:
                existing_config["learning_rate"] = 0.001
                logger.info("Applied default learning_rate: 0.001")
            if "dataset_format" not in existing_config or existing_config.get("dataset_format") is None:
                existing_config["dataset_format"] = "imagefolder"

        # Extract epochs (ìˆ«ì + "epoch" or "ì—í¬í¬")
        epoch_match = re.search(r'(\d+)\s*(?:epoch|ì—í¬í¬)', msg_lower)
        if epoch_match:
            existing_config["epochs"] = int(epoch_match.group(1))
            logger.info(f"Extracted epochs: {existing_config['epochs']}")

        # Extract batch size (ìˆ«ì + "batch" or "ë°°ì¹˜")
        batch_match = re.search(r'(?:batch|ë°°ì¹˜)[\s:]*(\d+)', msg_lower)
        if batch_match:
            existing_config["batch_size"] = int(batch_match.group(1))
            logger.info(f"Extracted batch_size: {existing_config['batch_size']}")

        # Extract learning rate
        lr_match = re.search(r'(?:lr|learning.?rate|í•™ìŠµë¥ )[\s:=]*(0?\.\d+)', msg_lower)
        if lr_match:
            existing_config["learning_rate"] = float(lr_match.group(1))
            logger.info(f"Extracted learning_rate: {existing_config['learning_rate']}")

        return existing_config

    def _format_config_summary(self, config: Dict[str, Any]) -> str:
        """Format config as readable summary"""
        lines = [
            "**í•™ìŠµ ì„¤ì •:**",
            f"- í”„ë ˆì„ì›Œí¬: {config.get('framework', 'N/A')}",
            f"- ëª¨ë¸: {config.get('model_name', 'N/A')}",
            f"- ì‘ì—… ìœ í˜•: {config.get('task_type', 'N/A')}",
            f"- ë°ì´í„°ì…‹: {config.get('dataset_path', 'N/A')}",
            f"- ì—í¬í¬: {config.get('epochs', 'N/A')}",
            f"- ë°°ì¹˜ í¬ê¸°: {config.get('batch_size', 'N/A')}",
            f"- í•™ìŠµë¥ : {config.get('learning_rate', 'N/A')}",
        ]

        # Add advanced config preset if specified
        advanced_config = config.get('advanced_config')
        if advanced_config:
            preset_descriptions = {
                "basic": "ê°„ë‹¨í•œ í•™ìŠµ (minimal augmentation, Adam optimizer)",
                "standard": "ê· í˜•ì¡íŒ ì„¤ì • (AdamW, cosine scheduler, moderate augmentation)",
                "aggressive": "ê°•ë ¥í•œ augmentation (ì‘ì€ ë°ì´í„°ì…‹ì— ì í•©)",
                "fine_tuning": "ì‚¬ì „í•™ìŠµ ëª¨ë¸ fine-tuning ìµœì í™”"
            }
            preset_desc = preset_descriptions.get(advanced_config, advanced_config)
            lines.append(f"- Advanced Config: {advanced_config} ({preset_desc})")

        return "\n".join(lines)

    # ========== Phase 1 Dataset Handlers ==========

    async def _handle_analyze_dataset(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Handle analyze_dataset action

        Analyzes a dataset's structure, format, and quality using Tool Registry.
        """
        from app.utils.tool_registry import tool_registry

        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        # Get dataset path from config or action response
        dataset_path = config.get("dataset_path")

        if not dataset_path:
            logger.warning("analyze_dataset called without dataset_path")
            return {
                "new_state": ConversationState.INITIAL,
                "message": "ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: C:/datasets/my_dataset",
                "temp_data": temp_data
            }

        # Call tool registry to analyze dataset
        try:
            logger.info(f"Analyzing dataset at: {dataset_path}")
            result = await tool_registry.call_tool(
                "analyze_dataset",
                {"dataset_path": dataset_path},
                self.db,
                user_id=None  # Phase 1: Skip auth
            )

            # Save analysis results to temp_data
            temp_data["dataset_analysis"] = result

            # Format analysis results for user
            message = self._format_dataset_analysis(result)

            return {
                "new_state": ConversationState.ANALYZING_DATASET,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to analyze dataset: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.ERROR,
                "message": f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nê²½ë¡œë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "temp_data": temp_data
            }

    async def _handle_show_dataset_analysis(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Handle show_dataset_analysis action

        Displays previously analyzed dataset information from temp_data.
        """
        temp_data = session.temp_data or {}
        analysis = temp_data.get("dataset_analysis")

        if not analysis:
            logger.warning("show_dataset_analysis called without prior analysis")
            return {
                "new_state": ConversationState.INITIAL,
                "message": "ë¨¼ì € ë°ì´í„°ì…‹ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "temp_data": temp_data
            }

        # Format and display analysis
        message = self._format_dataset_analysis(analysis)
        message += "\n\nì´ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"

        return {
            "new_state": ConversationState.ANALYZING_DATASET,
            "message": message,
            "temp_data": temp_data
        }

    async def _handle_list_datasets(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Handle list_datasets action

        Lists available datasets in default or specified directory.
        """
        from app.utils.tool_registry import tool_registry

        temp_data = session.temp_data or {}

        # Get base path from action response or use default
        base_path = "C:/datasets"  # Default path

        try:
            logger.info(f"Listing datasets in: {base_path}")
            datasets = await tool_registry.call_tool(
                "list_datasets",
                {"base_path": base_path},
                self.db,
                user_id=None  # Phase 1: Skip auth
            )

            if not datasets:
                message = f"{base_path}ì— ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.\n\n"
                message += "ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”."

                return {
                    "new_state": ConversationState.INITIAL,
                    "message": message,
                    "temp_data": temp_data
                }

            # Format dataset list
            message = f"**ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹** ({base_path}):\n\n"
            for idx, dataset in enumerate(datasets, start=1):
                message += f"{idx}. {dataset['name']}\n"
                message += f"   ê²½ë¡œ: {dataset['path']}\n\n"

            message += "ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì´ë¦„ ë˜ëŠ” ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

            # Save dataset list to temp_data for reference
            temp_data["available_datasets"] = datasets

            return {
                "new_state": ConversationState.INITIAL,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to list datasets: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.INITIAL,
                "message": f"ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\në°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”.",
                "temp_data": temp_data
            }

    def _format_dataset_analysis(self, analysis: Dict[str, Any]) -> str:
        """
        Format dataset analysis results for user display

        Args:
            analysis: Analysis results from tool_registry.analyze_dataset

        Returns:
            Formatted message string
        """
        lines = ["**ë°ì´í„°ì…‹ ë¶„ì„ ê²°ê³¼:**\n"]

        # Basic info
        lines.append(f"ğŸ“ ê²½ë¡œ: {analysis.get('path', 'N/A')}")
        lines.append(f"ğŸ“‹ í¬ë§·: {analysis.get('format', 'unknown')}")
        lines.append(f"ğŸ“Š ì´ ì´ë¯¸ì§€ ìˆ˜: {analysis.get('total_images', 0):,}ê°œ")
        lines.append(f"ğŸ·ï¸ í´ë˜ìŠ¤ ìˆ˜: {analysis.get('num_classes', 0)}ê°œ")

        # Class distribution
        classes = analysis.get('classes', [])
        if classes:
            lines.append(f"\n**í´ë˜ìŠ¤ ëª©ë¡:**")
            class_dist = analysis.get('class_distribution', {})
            for cls in classes[:10]:  # Show first 10 classes
                count = class_dist.get(cls, 0)
                lines.append(f"  - {cls}: {count:,}ê°œ")

            if len(classes) > 10:
                lines.append(f"  ... ì™¸ {len(classes) - 10}ê°œ í´ë˜ìŠ¤")

        # Dataset info/warnings
        dataset_info = analysis.get('dataset_info', {})
        if dataset_info:
            lines.append(f"\n**ë°ì´í„°ì…‹ ì •ë³´:**")
            for key, value in dataset_info.items():
                lines.append(f"  - {key}: {value}")

        # Suggestions
        suggestions = analysis.get('suggestions', [])
        if suggestions:
            lines.append(f"\n**ğŸ’¡ ê¶Œì¥ì‚¬í•­:**")
            for suggestion in suggestions:
                lines.append(f"  - {suggestion}")

        return "\n".join(lines)

    # ========== Phase 1 Model Handlers ==========

    async def _handle_search_models(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Handle search_models action

        Searches for models based on task type, framework, or tags.
        """
        from app.utils.tool_registry import tool_registry

        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        # Extract search parameters from config
        search_params = {}
        if config.get("task_type"):
            search_params["task_type"] = config["task_type"]
        if config.get("framework"):
            search_params["framework"] = config["framework"]

        try:
            logger.info(f"Searching models with params: {search_params}")
            models = await tool_registry.call_tool(
                "search_models",
                search_params,
                self.db,
                user_id=None  # Phase 1: Skip auth
            )

            if not models:
                message = "ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                message += "ë‹¤ë¥¸ ì¡°ê±´ìœ¼ë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”."

                return {
                    "new_state": ConversationState.SELECTING_MODEL,
                    "message": message,
                    "temp_data": temp_data
                }

            # Save search results to temp_data
            temp_data["model_search_results"] = models

            # Format model list
            message = self._format_model_list(models, search_params)
            message += "\n\nì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”."

            return {
                "new_state": ConversationState.SELECTING_MODEL,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to search models: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.ERROR,
                "message": f"ëª¨ë¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "temp_data": temp_data
            }

    async def _handle_show_model_info(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Handle show_model_info action

        Shows detailed information about a specific model.
        """
        from app.utils.tool_registry import tool_registry

        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        # Get model info from config
        framework = config.get("framework")
        model_name = config.get("model_name")

        if not framework or not model_name:
            logger.warning("show_model_info called without framework/model_name")
            return {
                "new_state": ConversationState.SELECTING_MODEL,
                "message": "ëª¨ë¸ ì •ë³´ë¥¼ í™•ì¸í•˜ë ¤ë©´ í”„ë ˆì„ì›Œí¬ì™€ ëª¨ë¸ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\nì˜ˆ: timmì˜ resnet50 ì •ë³´ë¥¼ ì•Œë ¤ì¤˜",
                "temp_data": temp_data
            }

        try:
            logger.info(f"Getting model guide for: {framework}/{model_name}")
            model_guide = await tool_registry.call_tool(
                "get_model_guide",
                {"framework": framework, "model_name": model_name},
                self.db,
                user_id=None  # Phase 1: Skip auth
            )

            # Format model info
            message = self._format_model_info(model_guide)
            message += "\n\nì´ ëª¨ë¸ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"

            return {
                "new_state": ConversationState.SELECTING_MODEL,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to get model info: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.ERROR,
                "message": f"ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "temp_data": temp_data
            }

    async def _handle_recommend_models(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Handle recommend_models action

        Recommends models based on dataset analysis and task type.
        """
        from app.utils.tool_registry import tool_registry

        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})
        dataset_analysis = temp_data.get("dataset_analysis", {})

        # Determine task type from config or dataset
        task_type = config.get("task_type")

        # If no task type specified, try to infer from dataset
        if not task_type:
            # Default to classification if we have class information
            if dataset_analysis.get("num_classes", 0) > 0:
                task_type = "classification"
                config["task_type"] = task_type

        if not task_type:
            logger.warning("recommend_models called without task_type")
            return {
                "new_state": ConversationState.SELECTING_MODEL,
                "message": "ëª¨ë¸ì„ ì¶”ì²œí•˜ë ¤ë©´ ì‘ì—… ìœ í˜•ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\nì–´ë–¤ ì‘ì—…ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì˜ˆ: ë¶„ë¥˜, ê°ì²´ ê²€ì¶œ, ì„¸ê·¸ë©˜í…Œì´ì…˜)",
                "temp_data": temp_data
            }

        try:
            # Search models for the task type
            logger.info(f"Recommending models for task: {task_type}")
            models = await tool_registry.call_tool(
                "search_models",
                {"task_type": task_type},
                self.db,
                user_id=None  # Phase 1: Skip auth
            )

            if not models:
                message = f"{task_type} ì‘ì—…ì— ì í•©í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return {
                    "new_state": ConversationState.SELECTING_MODEL,
                    "message": message,
                    "temp_data": temp_data
                }

            # Sort by recommendation (for now, just take first 3)
            recommended = models[:3]
            temp_data["recommended_models"] = recommended

            # Format recommendations
            message = f"**{task_type} ì‘ì—…ì— ì¶”ì²œí•˜ëŠ” ëª¨ë¸:**\n\n"

            num_classes = dataset_analysis.get("num_classes", 0)
            if num_classes > 0:
                message += f"ë°ì´í„°ì…‹ ë¶„ì„ ê²°ê³¼ {num_classes}ê°œ í´ë˜ìŠ¤ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"

            for idx, model in enumerate(recommended, start=1):
                message += f"{idx}. **{model['name']}** ({model['framework']})\n"
                message += f"   {model.get('description', 'No description')}\n\n"

            message += "ì‚¬ìš©í•  ëª¨ë¸ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ëª¨ë¸ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

            return {
                "new_state": ConversationState.SELECTING_MODEL,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to recommend models: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.ERROR,
                "message": f"ëª¨ë¸ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "temp_data": temp_data
            }

    def _format_model_list(
        self, models: list, search_params: Dict[str, Any]
    ) -> str:
        """
        Format model search results for user display

        Args:
            models: List of model dictionaries
            search_params: Search parameters used

        Returns:
            Formatted message string
        """
        lines = ["**ëª¨ë¸ ê²€ìƒ‰ ê²°ê³¼:**\n"]

        # Show search criteria
        if search_params:
            lines.append("ê²€ìƒ‰ ì¡°ê±´:")
            for key, value in search_params.items():
                lines.append(f"  - {key}: {value}")
            lines.append("")

        # List models
        lines.append(f"ì´ {len(models)}ê°œì˜ ëª¨ë¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n")

        for idx, model in enumerate(models, start=1):
            lines.append(f"{idx}. **{model['name']}** ({model['framework']})")
            lines.append(f"   ì‘ì—… ìœ í˜•: {', '.join(model.get('task_types', []))}")
            if model.get('description'):
                lines.append(f"   ì„¤ëª…: {model['description']}")
            lines.append("")

        return "\n".join(lines)

    def _format_model_info(self, model_guide: Dict[str, Any]) -> str:
        """
        Format model guide information for user display

        Args:
            model_guide: Model guide from tool_registry

        Returns:
            Formatted message string
        """
        lines = ["**ëª¨ë¸ ìƒì„¸ ì •ë³´:**\n"]

        lines.append(f"ğŸ“¦ í”„ë ˆì„ì›Œí¬: {model_guide.get('framework', 'N/A')}")
        lines.append(f"ğŸ·ï¸ ëª¨ë¸ëª…: {model_guide.get('model_name', 'N/A')}")
        lines.append(f"ğŸ“ ì„¤ëª…: {model_guide.get('description', 'N/A')}")
        lines.append(f"âœ… ì‚¬ìš© ê°€ëŠ¥: {'ì˜ˆ' if model_guide.get('available') else 'ì•„ë‹ˆì˜¤'}")

        # Additional details if available
        if model_guide.get('parameters'):
            lines.append(f"\n**íŒŒë¼ë¯¸í„°:**")
            for key, value in model_guide['parameters'].items():
                lines.append(f"  - {key}: {value}")

        if model_guide.get('performance'):
            lines.append(f"\n**ì„±ëŠ¥:**")
            for key, value in model_guide['performance'].items():
                lines.append(f"  - {key}: {value}")

        return "\n".join(lines)

    # ========== Phase 1 Training Control Handlers ==========

    async def _handle_show_training_status(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Handle show_training_status action

        Shows current status and progress of a training job.
        """
        from app.utils.tool_registry import tool_registry

        temp_data = session.temp_data or {}

        # Try to get job_id from session's most recent training job
        job_id = None

        # Check if user specified a job_id in the message
        import re
        job_match = re.search(r'(?:job|ì‘ì—…)[\s#]*(\d+)', user_message.lower())
        if job_match:
            job_id = int(job_match.group(1))
        else:
            # Get most recent training job from this session
            recent_job = self.db.query(TrainingJob).filter(
                TrainingJob.session_id == session.id
            ).order_by(TrainingJob.created_at.desc()).first()

            if recent_job:
                job_id = recent_job.id

        if not job_id:
            logger.warning("show_training_status called without job_id")
            return {
                "new_state": ConversationState.MONITORING_TRAINING,
                "message": "í•™ìŠµ ì‘ì—… IDë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: job 123ì˜ ìƒíƒœë¥¼ ì•Œë ¤ì¤˜",
                "temp_data": temp_data
            }

        try:
            logger.info(f"Getting training status for job: {job_id}")
            status = await tool_registry.call_tool(
                "get_training_status",
                {"job_id": job_id},
                self.db,
                user_id=None  # Phase 1: Skip auth
            )

            # Format training status
            message = self._format_training_status(status)

            return {
                "new_state": ConversationState.MONITORING_TRAINING,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to get training status: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.ERROR,
                "message": f"í•™ìŠµ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "temp_data": temp_data
            }

    async def _handle_stop_training(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Handle stop_training action

        Stops a running training job.
        """
        from app.utils.tool_registry import tool_registry

        temp_data = session.temp_data or {}

        # Try to get job_id from user message
        import re
        job_id = None
        job_match = re.search(r'(?:job|ì‘ì—…)[\s#]*(\d+)', user_message.lower())
        if job_match:
            job_id = int(job_match.group(1))
        else:
            # Get most recent running job from this session
            running_job = self.db.query(TrainingJob).filter(
                TrainingJob.session_id == session.id,
                TrainingJob.status == "running"
            ).order_by(TrainingJob.created_at.desc()).first()

            if running_job:
                job_id = running_job.id

        if not job_id:
            logger.warning("stop_training called without job_id")
            return {
                "new_state": ConversationState.MONITORING_TRAINING,
                "message": "ì¤‘ì§€í•  í•™ìŠµ ì‘ì—… IDë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: job 123 ì¤‘ì§€í•´ì¤˜",
                "temp_data": temp_data
            }

        try:
            logger.info(f"Stopping training job: {job_id}")
            result = await tool_registry.call_tool(
                "stop_training",
                {"job_id": job_id, "save_checkpoint": True},
                self.db,
                user_id=None  # Phase 1: Skip auth
            )

            message = f"**í•™ìŠµ ì¤‘ì§€ ê²°ê³¼:**\n\n"
            message += f"Job ID: {result.get('job_id')}\n"
            message += f"ìƒíƒœ: {result.get('status')}\n"
            message += f"{result.get('message')}"

            return {
                "new_state": ConversationState.MONITORING_TRAINING,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to stop training: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.ERROR,
                "message": f"í•™ìŠµ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "temp_data": temp_data
            }

    async def _handle_list_training_jobs(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Handle list_training_jobs action

        Lists training jobs with optional filters.
        """
        from app.utils.tool_registry import tool_registry

        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        # Extract filters from user message
        filters = {"limit": 10}

        # Check for status filter
        if "ì‹¤í–‰ì¤‘" in user_message or "running" in user_message.lower():
            filters["status"] = "running"
        elif "ì™„ë£Œ" in user_message or "complete" in user_message.lower():
            filters["status"] = "completed"
        elif "ì‹¤íŒ¨" in user_message or "failed" in user_message.lower():
            filters["status"] = "failed"

        try:
            logger.info(f"Listing training jobs with filters: {filters}")
            jobs = await tool_registry.call_tool(
                "list_training_jobs",
                filters,
                self.db,
                user_id=None  # Phase 1: Skip auth
            )

            if not jobs:
                message = "ì¡°ê±´ì— ë§ëŠ” í•™ìŠµ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤."
                return {
                    "new_state": ConversationState.MONITORING_TRAINING,
                    "message": message,
                    "temp_data": temp_data
                }

            # Format job list
            message = f"**í•™ìŠµ ì‘ì—… ëª©ë¡** (ìµœê·¼ {len(jobs)}ê°œ):\n\n"

            for job in jobs:
                message += f"ğŸ“Š Job #{job['job_id']} - {job['model']}\n"
                message += f"   ìƒíƒœ: {job['status']}\n"
                message += f"   ì‘ì—… ìœ í˜•: {job.get('task_type', 'N/A')}\n"
                if job.get('final_metric'):
                    message += f"   ìµœì¢… ì •í™•ë„: {job['final_metric']:.2%}\n"
                message += f"   ìƒì„±: {job.get('created_at', 'N/A')}\n\n"

            message += "ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•˜ë ¤ë©´ 'job X ìƒíƒœ ì•Œë ¤ì¤˜'ë¼ê³  ì…ë ¥í•˜ì„¸ìš”."

            return {
                "new_state": ConversationState.MONITORING_TRAINING,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to list training jobs: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.ERROR,
                "message": f"í•™ìŠµ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "temp_data": temp_data
            }

    # ========== Phase 1 Inference Handlers ==========

    async def _handle_start_quick_inference(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Handle start_quick_inference action

        Runs quick inference on a single image.
        """
        from app.utils.tool_registry import tool_registry

        temp_data = session.temp_data or {}

        # Try to extract job_id and image_path from message
        import re
        job_id = None
        image_path = None

        # Extract job_id
        job_match = re.search(r'(?:job|ì‘ì—…)[\s#]*(\d+)', user_message.lower())
        if job_match:
            job_id = int(job_match.group(1))
        else:
            # Get most recent completed job
            completed_job = self.db.query(TrainingJob).filter(
                TrainingJob.session_id == session.id,
                TrainingJob.status.in_(["completed", "running"])
            ).order_by(TrainingJob.created_at.desc()).first()

            if completed_job:
                job_id = completed_job.id

        # Extract image path
        path_pattern = r'[A-Za-z]:\\[\w\\\-\.]+\.(jpg|jpeg|png|bmp)|/[\w/\-\.]+\.(jpg|jpeg|png|bmp)'
        path_match = re.search(path_pattern, user_message, re.IGNORECASE)
        if path_match:
            image_path = path_match.group(0)

        if not job_id:
            logger.warning("start_quick_inference called without job_id")
            return {
                "new_state": ConversationState.RUNNING_INFERENCE,
                "message": "ì¶”ë¡ ì„ ì‹¤í–‰í•  í•™ìŠµ ì‘ì—… IDë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: job 123ìœ¼ë¡œ ì´ë¯¸ì§€ ì¶”ë¡ í•´ì¤˜",
                "temp_data": temp_data
            }

        if not image_path:
            logger.warning("start_quick_inference called without image_path")
            return {
                "new_state": ConversationState.RUNNING_INFERENCE,
                "message": "ì¶”ë¡ í•  ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: C:/images/test.jpg",
                "temp_data": temp_data
            }

        try:
            logger.info(f"Running inference: job={job_id}, image={image_path}")
            result = await tool_registry.call_tool(
                "run_quick_inference",
                {"job_id": job_id, "image_path": image_path},
                self.db,
                user_id=None  # Phase 1: Skip auth
            )

            # Format inference results
            message = f"**ì¶”ë¡  ê²°ê³¼:**\n\n"
            message += f"Job ID: {result.get('job_id')}\n"
            message += f"ì´ë¯¸ì§€: {result.get('image_path')}\n\n"

            predictions = result.get('predictions', [])
            if predictions:
                message += "ì˜ˆì¸¡:\n"
                for pred in predictions[:5]:  # Top 5 predictions
                    message += f"  - {pred.get('class')}: {pred.get('confidence', 0):.2%}\n"
            else:
                message += result.get('message', 'ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')

            return {
                "new_state": ConversationState.RUNNING_INFERENCE,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to run inference: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.ERROR,
                "message": f"ì¶”ë¡  ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "temp_data": temp_data
            }

    def _format_training_status(self, status: Dict[str, Any]) -> str:
        """
        Format training status for user display

        Args:
            status: Training status from tool_registry

        Returns:
            Formatted message string
        """
        lines = ["**í•™ìŠµ ìƒíƒœ:**\n"]

        lines.append(f"ğŸ“Š Job ID: {status.get('job_id')}")
        lines.append(f"ğŸ”§ ëª¨ë¸: {status.get('model')}")
        lines.append(f"ğŸ“¦ í”„ë ˆì„ì›Œí¬: {status.get('framework')}")
        lines.append(f"ğŸ“ˆ ìƒíƒœ: {status.get('status')}")

        # Progress
        current_epoch = status.get('current_epoch', 0)
        total_epochs = status.get('total_epochs', 0)
        progress = status.get('progress_percent', 0)

        lines.append(f"â±ï¸ ì§„í–‰: {current_epoch}/{total_epochs} epochs ({progress:.1f}%)")

        # Latest metrics
        latest = status.get('latest_metrics', {})
        if latest:
            lines.append(f"\n**ìµœê·¼ ë©”íŠ¸ë¦­ (Epoch {latest.get('epoch', 0)}):**")
            if latest.get('loss') is not None:
                lines.append(f"  - Loss: {latest['loss']:.4f}")
            if latest.get('accuracy') is not None:
                lines.append(f"  - Accuracy: {latest['accuracy']:.2%}")
            if latest.get('val_loss') is not None:
                lines.append(f"  - Val Loss: {latest['val_loss']:.4f}")
            if latest.get('val_accuracy') is not None:
                lines.append(f"  - Val Accuracy: {latest['val_accuracy']:.2%}")

        # Timestamps
        if status.get('started_at'):
            lines.append(f"\nì‹œì‘: {status['started_at']}")

        return "\n".join(lines)

    # ========== ë¯¸êµ¬í˜„ í•¸ë“¤ëŸ¬ ì¶”ê°€ ==========

    async def _handle_compare_models(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle compare_models action"""
        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        models_to_compare = config.get("models_to_compare", [])

        if not models_to_compare or len(models_to_compare) < 2:
            return {
                "new_state": ConversationState.SELECTING_MODEL,
                "message": "ë¹„êµí•  ëª¨ë¸ì„ 2ê°œ ì´ìƒ ì§€ì •í•´ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ: resnet50ê³¼ efficientnet_b0 ë¹„êµí•´ì¤˜",
                "temp_data": temp_data
            }

        try:
            from app.utils.tool_registry import tool_registry

            # Call compare_models tool
            result = await tool_registry.call_tool(
                "compare_models",
                {"model_specs": models_to_compare},
                self.db,
                user_id=None
            )

            # Format comparison results
            message = f"**ëª¨ë¸ ë¹„êµ ê²°ê³¼:**\n\n"

            for i, model in enumerate(models_to_compare, 1):
                message += f"{i}. {model.get('framework')} / {model.get('name')}\n"

            message += f"\n{result.get('comparison', 'ëª¨ë¸ ë¹„êµ ê¸°ëŠ¥ì€ ê³§ ì œê³µë©ë‹ˆë‹¤.')}"

            temp_data["model_comparison"] = result

            return {
                "new_state": ConversationState.COMPARING_MODELS,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to compare models: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.ERROR,
                "message": f"ëª¨ë¸ ë¹„êµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "temp_data": temp_data
            }

    async def _handle_resume_training(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle resume_training action"""
        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        job_id = config.get("job_id")

        if not job_id:
            return {
                "new_state": ConversationState.GATHERING_CONFIG,
                "message": "ì¬ê°œí•  í•™ìŠµ ì‘ì—…ì˜ IDë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ: job 123 ì¬ê°œí•´ì¤˜",
                "temp_data": temp_data
            }

        try:
            # Check if job exists and is stopped
            job = self.db.query(TrainingJob).filter(TrainingJob.id == job_id).first()

            if not job:
                return {
                    "new_state": ConversationState.ERROR,
                    "message": f"í•™ìŠµ ì‘ì—… {job_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "temp_data": temp_data
                }

            if job.status != "stopped":
                return {
                    "new_state": ConversationState.MONITORING_TRAINING,
                    "message": f"í•™ìŠµ ì‘ì—… {job_id}ëŠ” í˜„ì¬ '{job.status}' ìƒíƒœì…ë‹ˆë‹¤. ì¤‘ì§€ëœ ì‘ì—…ë§Œ ì¬ê°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "temp_data": temp_data
                }

            # TODO: Implement actual resume logic
            # For now, just return a message indicating it's not implemented yet
            message = f"**í•™ìŠµ ì¬ê°œ:**\n\n"
            message += f"Job ID {job_id}ì˜ í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤.\n"
            message += f"ëª¨ë¸: {job.model_name}\n"
            message += f"í”„ë ˆì„ì›Œí¬: {job.framework}\n\n"
            message += "í•™ìŠµ ì¬ê°œ ê¸°ëŠ¥ì€ ê³§ êµ¬í˜„ë©ë‹ˆë‹¤."

            return {
                "new_state": ConversationState.MONITORING_TRAINING,
                "message": message,
                "temp_data": temp_data
            }

        except Exception as e:
            logger.error(f"Failed to resume training: {str(e)}", exc_info=True)
            return {
                "new_state": ConversationState.ERROR,
                "message": f"í•™ìŠµ ì¬ê°œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "temp_data": temp_data
            }

    async def _handle_start_batch_inference(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle start_batch_inference action"""
        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        job_id = config.get("job_id")
        image_dir = config.get("image_dir")

        if not job_id:
            return {
                "new_state": ConversationState.GATHERING_CONFIG,
                "message": "ì¶”ë¡ ì— ì‚¬ìš©í•  í•™ìŠµ ì‘ì—… IDë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ: job 123ìœ¼ë¡œ ì¶”ë¡ í•´ì¤˜",
                "temp_data": temp_data
            }

        if not image_dir:
            return {
                "new_state": ConversationState.GATHERING_CONFIG,
                "message": "ì¶”ë¡ í•  ì´ë¯¸ì§€ í´ë” ê²½ë¡œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ: C:/datasets/images í´ë” ì¶”ë¡ í•´ì¤˜",
                "temp_data": temp_data
            }

        # Check if job exists
        job = self.db.query(TrainingJob).filter(TrainingJob.id == job_id).first()

        if not job:
            return {
                "new_state": ConversationState.ERROR,
                "message": f"í•™ìŠµ ì‘ì—… {job_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "temp_data": temp_data
            }

        # TODO: Implement actual batch inference logic
        message = f"**ë°°ì¹˜ ì¶”ë¡  ì‹œì‘:**\n\n"
        message += f"Job ID: {job_id}\n"
        message += f"ì´ë¯¸ì§€ í´ë”: {image_dir}\n"
        message += f"ëª¨ë¸: {job.model_name}\n\n"
        message += "ë°°ì¹˜ ì¶”ë¡  ê¸°ëŠ¥ì€ ê³§ êµ¬í˜„ë©ë‹ˆë‹¤."

        return {
            "new_state": ConversationState.RUNNING_INFERENCE,
            "message": message,
            "temp_data": temp_data
        }

    async def _handle_show_inference_results(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle show_inference_results action"""
        temp_data = session.temp_data or {}

        # Check if inference results exist in temp_data
        inference_results = temp_data.get("inference_results")

        if not inference_results:
            return {
                "new_state": ConversationState.IDLE,
                "message": "í‘œì‹œí•  ì¶”ë¡  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¶”ë¡ ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.",
                "temp_data": temp_data
            }

        # Format inference results
        message = f"**ì¶”ë¡  ê²°ê³¼:**\n\n"

        job_id = inference_results.get("job_id")
        if job_id:
            message += f"Job ID: {job_id}\n\n"

        predictions = inference_results.get("predictions", [])
        if predictions:
            message += f"ì´ {len(predictions)}ê°œ ì´ë¯¸ì§€ ì¶”ë¡  ì™„ë£Œ:\n\n"
            for i, pred in enumerate(predictions[:10], 1):  # Show first 10
                image_name = pred.get("image", f"image_{i}")
                pred_class = pred.get("class", "Unknown")
                confidence = pred.get("confidence", 0)
                message += f"{i}. {image_name}: {pred_class} ({confidence:.2%})\n"

            if len(predictions) > 10:
                message += f"\n... ì™¸ {len(predictions) - 10}ê°œ"
        else:
            message += "ì¶”ë¡  ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."

        return {
            "new_state": ConversationState.VIEWING_RESULTS,
            "message": message,
            "temp_data": temp_data
        }

    async def _handle_show_validation_results(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle show_validation_results action"""
        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        job_id = config.get("job_id")

        if not job_id:
            return {
                "new_state": ConversationState.GATHERING_CONFIG,
                "message": "ê²€ì¦ ê²°ê³¼ë¥¼ ì¡°íšŒí•  í•™ìŠµ ì‘ì—… IDë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ: job 123 ê²€ì¦ ê²°ê³¼ ë³´ì—¬ì¤˜",
                "temp_data": temp_data
            }

        # Check if job exists
        job = self.db.query(TrainingJob).filter(TrainingJob.id == job_id).first()

        if not job:
            return {
                "new_state": ConversationState.ERROR,
                "message": f"í•™ìŠµ ì‘ì—… {job_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "temp_data": temp_data
            }

        # TODO: Fetch validation results from database or MLflow
        message = f"**ê²€ì¦ ê²°ê³¼:**\n\n"
        message += f"Job ID: {job_id}\n"
        message += f"ëª¨ë¸: {job.model_name}\n"
        message += f"ì‘ì—… ìœ í˜•: {job.task_type}\n\n"

        if job.final_accuracy:
            message += f"ìµœì¢… ì •í™•ë„: {job.final_accuracy:.2%}\n"

        message += "\nìƒì„¸í•œ ê²€ì¦ ë©”íŠ¸ë¦­ ì¡°íšŒ ê¸°ëŠ¥ì€ ê³§ êµ¬í˜„ë©ë‹ˆë‹¤."

        return {
            "new_state": ConversationState.VIEWING_RESULTS,
            "message": message,
            "temp_data": temp_data
        }

    async def _handle_show_confusion_matrix(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle show_confusion_matrix action"""
        temp_data = session.temp_data or {}
        config = temp_data.get("config", {})

        job_id = config.get("job_id")

        if not job_id:
            return {
                "new_state": ConversationState.GATHERING_CONFIG,
                "message": "Confusion Matrixë¥¼ ì¡°íšŒí•  í•™ìŠµ ì‘ì—… IDë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ: job 123 confusion matrix ë³´ì—¬ì¤˜",
                "temp_data": temp_data
            }

        # Check if job exists
        job = self.db.query(TrainingJob).filter(TrainingJob.id == job_id).first()

        if not job:
            return {
                "new_state": ConversationState.ERROR,
                "message": f"í•™ìŠµ ì‘ì—… {job_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "temp_data": temp_data
            }

        if job.task_type not in ["classification", "image_classification"]:
            return {
                "new_state": ConversationState.VIEWING_RESULTS,
                "message": f"Confusion MatrixëŠ” ë¶„ë¥˜ ì‘ì—…({job.task_type})ì—ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "temp_data": temp_data
            }

        # TODO: Generate and fetch confusion matrix from validation results
        message = f"**Confusion Matrix:**\n\n"
        message += f"Job ID: {job_id}\n"
        message += f"ëª¨ë¸: {job.model_name}\n\n"
        message += "Confusion Matrix ì‹œê°í™” ê¸°ëŠ¥ì€ ê³§ êµ¬í˜„ë©ë‹ˆë‹¤."

        return {
            "new_state": ConversationState.VIEWING_RESULTS,
            "message": message,
            "temp_data": temp_data
        }

    async def _handle_show_help(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle show_help action"""
        temp_data = session.temp_data or {}

        message = """**Vision AI Training Platform ë„ì›€ë§**

ğŸ“Š **ë°ì´í„°ì…‹ ê´€ë¦¬:**
- ë°ì´í„°ì…‹ ë¶„ì„: "C:/datasets/coco8 ë¶„ì„í•´ì¤˜"
- ë°ì´í„°ì…‹ ëª©ë¡: "ë°ì´í„°ì…‹ ëª©ë¡ ë³´ì—¬ì¤˜"

ğŸ¤– **ëª¨ë¸ ì„ íƒ:**
- ëª¨ë¸ ê²€ìƒ‰: "classification ëª¨ë¸ ì°¾ì•„ì¤˜"
- ëª¨ë¸ ì¶”ì²œ: "ì–´ë–¤ ëª¨ë¸ì´ ì¢‹ì„ê¹Œ?"
- ëª¨ë¸ ì •ë³´: "yolov8n ì •ë³´ ì•Œë ¤ì¤˜"
- ëª¨ë¸ ë¹„êµ: "resnet50ê³¼ efficientnet_b0 ë¹„êµí•´ì¤˜"

ğŸš€ **í•™ìŠµ ê´€ë¦¬:**
- í•™ìŠµ ì‹œì‘: "resnet50ìœ¼ë¡œ í•™ìŠµ ì‹œì‘"
- í•™ìŠµ ìƒíƒœ: "job 123 ìƒíƒœ ì•Œë ¤ì¤˜"
- í•™ìŠµ ì¤‘ì§€: "job 123 ì¤‘ì§€í•´ì¤˜"
- í•™ìŠµ ì¬ê°œ: "job 123 ì¬ê°œí•´ì¤˜"
- ì‘ì—… ëª©ë¡: "í•™ìŠµ ì‘ì—… ëª©ë¡ ë³´ì—¬ì¤˜"

ğŸ”® **ì¶”ë¡ :**
- ë¹ ë¥¸ ì¶”ë¡ : "job 123ìœ¼ë¡œ C:/test/cat.jpg ì¶”ë¡ í•´ì¤˜"
- ë°°ì¹˜ ì¶”ë¡ : "job 123ìœ¼ë¡œ C:/test/images í´ë” ì¶”ë¡ í•´ì¤˜"
- ê²°ê³¼ ì¡°íšŒ: "ì¶”ë¡  ê²°ê³¼ ë³´ì—¬ì¤˜"

ğŸ“ˆ **ê²°ê³¼ ì¡°íšŒ:**
- ê²€ì¦ ê²°ê³¼: "job 123 ê²€ì¦ ê²°ê³¼ ë³´ì—¬ì¤˜"
- Confusion Matrix: "job 123 confusion matrix ë³´ì—¬ì¤˜"

âš™ï¸ **ê¸°íƒ€:**
- ë„ì›€ë§: "ë„ì›€ë§" ë˜ëŠ” "help"
- ëŒ€í™” ì´ˆê¸°í™”: "ì´ˆê¸°í™”" ë˜ëŠ” "reset"

ğŸ’¡ **íŒ:** ìì—°ì–´ë¡œ í¸í•˜ê²Œ ë§ì”€í•˜ì„¸ìš”! ì˜ˆ: "ê³ ì–‘ì´ ê°•ì•„ì§€ ë¶„ë¥˜ ëª¨ë¸ ë§Œë“¤ê³  ì‹¶ì–´"
"""

        return {
            "new_state": ConversationState.IDLE,
            "message": message,
            "temp_data": temp_data
        }

    async def _handle_reset_conversation(
        self,
        action_response: GeminiActionResponse,
        session: SessionModel,
        user_message: str
    ) -> Dict[str, Any]:
        """Handle reset_conversation action"""
        # Clear all temp_data
        session.temp_data = {}

        message = "ëŒ€í™”ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì‘ì—…ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.\n\n"
        message += "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n"
        message += "- ë°ì´í„°ì…‹ ë¶„ì„\n"
        message += "- ëª¨ë¸ ê²€ìƒ‰/ì¶”ì²œ\n"
        message += "- í•™ìŠµ ì‹œì‘\n"
        message += "- ì¶”ë¡  ì‹¤í–‰\n\n"
        message += "ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ 'ë„ì›€ë§'ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

        return {
            "new_state": ConversationState.INITIAL,
            "message": message,
            "temp_data": {}
        }
