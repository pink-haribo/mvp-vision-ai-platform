# ================================
# Backend Environment Variables (Local Development)
# ================================

# ==========================================
# LLM Configuration
# ==========================================
# Provider: "gemini" or "openai"
LLM_PROVIDER=gemini
LLM_MODEL=gemini-2.5-flash-lite

# Gemini Configuration (required if LLM_PROVIDER=gemini)
GOOGLE_API_KEY=your-gemini-api-key-here

# OpenAI Configuration (required if LLM_PROVIDER=openai)
# Supports OpenAI API and OpenAI-compatible APIs (vLLM, Ollama, LocalAI, etc.)
OPENAI_API_KEY=your-openai-api-key-here
# OPENAI_API_BASE=https://api.openai.com/v1  # Default OpenAI endpoint
# OPENAI_API_BASE=http://localhost:11434/v1  # Ollama example
# OPENAI_API_BASE=http://localhost:8000/v1   # vLLM example

# Example configurations:
#
# 1. Using Gemini (default):
#    LLM_PROVIDER=gemini
#    LLM_MODEL=gemini-2.5-flash-lite
#    GOOGLE_API_KEY=your-gemini-api-key
#
# 2. Using OpenAI:
#    LLM_PROVIDER=openai
#    LLM_MODEL=gpt-4o-mini
#    OPENAI_API_KEY=sk-...
#
# 3. Using Ollama (OpenAI-compatible):
#    LLM_PROVIDER=openai
#    LLM_MODEL=llama3.2
#    OPENAI_API_KEY=ollama  # Any non-empty value works for Ollama
#    OPENAI_API_BASE=http://localhost:11434/v1
#
# 4. Using vLLM:
#    LLM_PROVIDER=openai
#    LLM_MODEL=your-model-name
#    OPENAI_API_KEY=token-abc123
#    OPENAI_API_BASE=http://localhost:8000/v1

# ==========================================
# Database
# ==========================================
DATABASE_URL=sqlite:///../../mvp/data/db/vision_platform.db

# ==========================================
# Paths (relative to mvp/backend directory)
# ==========================================
UPLOAD_DIR=../../mvp/data/uploads
OUTPUT_DIR=../../mvp/data/outputs
MODEL_DIR=../../mvp/data/models
LOG_DIR=../../mvp/data/logs

# ==========================================
# Backend API
# ==========================================
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
FRONTEND_PORT=3000

# ==========================================
# Training Defaults
# ==========================================
DEFAULT_EPOCHS=50
DEFAULT_BATCH_SIZE=32
DEFAULT_LEARNING_RATE=0.001

# ==========================================
# Training Services (HTTP API - Microservice Architecture)
# ==========================================
# Framework-specific service URLs (Railway production uses separate services)
TIMM_SERVICE_URL=http://localhost:8001
ULTRALYTICS_SERVICE_URL=http://localhost:8002
HUGGINGFACE_SERVICE_URL=http://localhost:8003

# Fallback URL (if framework-specific URL not set)
TRAINING_SERVICE_URL=http://localhost:8001

# ==========================================
# Dual Storage Architecture (Internal + External)
# ==========================================
# Internal Storage: Pretrained weights, checkpoints, config schemas (MinIO - Backend location)
# External Storage: Training datasets, user uploads (S3/R2 - Cloud storage)

# Local Development: Both use same MinIO instance
INTERNAL_STORAGE_ENDPOINT=http://localhost:30900
INTERNAL_STORAGE_ACCESS_KEY=minioadmin
INTERNAL_STORAGE_SECRET_KEY=minioadmin

EXTERNAL_STORAGE_ENDPOINT=http://localhost:30900
EXTERNAL_STORAGE_ACCESS_KEY=minioadmin
EXTERNAL_STORAGE_SECRET_KEY=minioadmin

# Production Example:
# Internal Storage (MinIO - same location as Backend):
# INTERNAL_STORAGE_ENDPOINT=http://minio.backend.svc.cluster.local:9000
# INTERNAL_STORAGE_ACCESS_KEY=your_minio_access_key
# INTERNAL_STORAGE_SECRET_KEY=your_minio_secret_key

# External Storage (Cloudflare R2):
# EXTERNAL_STORAGE_ENDPOINT=https://your-account-id.r2.cloudflarestorage.com
# EXTERNAL_STORAGE_ACCESS_KEY=your_r2_access_key
# EXTERNAL_STORAGE_SECRET_KEY=your_r2_secret_key

# Internal Storage Buckets (MinIO - Backend location)
INTERNAL_BUCKET_WEIGHTS=model-weights
INTERNAL_BUCKET_CHECKPOINTS=training-checkpoints
INTERNAL_BUCKET_SCHEMAS=config-schemas

# External Storage Buckets (S3/R2 - Cloud)
EXTERNAL_BUCKET_DATASETS=training-datasets

# ==========================================
# Legacy Storage Configuration (for backward compatibility)
# ==========================================
# These are deprecated in favor of INTERNAL_/EXTERNAL_ settings above
AWS_S3_ENDPOINT_URL=http://localhost:30900
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin
S3_BUCKET_DATASETS=training-datasets
S3_BUCKET_CHECKPOINTS=training-checkpoints
S3_BUCKET_RESULTS=training-results

# ==========================================
# MLflow Tracking
# ==========================================
# Local Dev (Kubernetes): Use NodePort
MLFLOW_TRACKING_URI=http://localhost:30500
# Production (Railway): Use internal service URL
# MLFLOW_TRACKING_URI=http://mlflow.monitoring.svc.cluster.local:5000

# ==========================================
# Loki Log Aggregation
# ==========================================
# Loki endpoint for log queries (from Backend)
# Docker Compose: http://localhost:3100
# Kind K8s: http://localhost:30100 (NodePort)
# Production K8s: http://loki.monitoring.svc.cluster.local:3100
LOKI_URL=http://localhost:30100

# Loki service configuration (for docker-compose.yml)
LOKI_HTTP_PORT=3100
LOKI_GRPC_PORT=9096
LOKI_RETENTION_PERIOD=720h  # 30 days (local), 2160h (90 days for production)
LOKI_STORAGE_TYPE=filesystem  # filesystem (local), s3/gcs (production)

# Promtail log collection configuration (for docker-compose.yml)
PROMTAIL_HTTP_PORT=9080
PROMTAIL_LOKI_URL=http://loki:3100  # Internal Docker/K8s network URL
PROMTAIL_LOG_PATH=/app/logs/*.log  # Training logs path inside container

# ==========================================
# Backend Internal API (for Training Service callbacks)
# ==========================================
BACKEND_INTERNAL_URL=http://localhost:8000/internal
BACKEND_INTERNAL_AUTH_TOKEN=dev-internal-token-123

# ==========================================
# CORS
# ==========================================
CORS_ORIGINS=http://localhost:3000

# ==========================================
# Logging
# ==========================================
LOG_LEVEL=INFO

# ==========================================
# Development Settings
# ==========================================
DEBUG=true
RELOAD=true
ENVIRONMENT=development

# ==========================================
# Training Execution Mode
# ==========================================
# Options: "subprocess" (local dev), "kubernetes" (production)
# - subprocess: Run training locally in a subprocess (for development/testing)
# - kubernetes: Run training as Kubernetes Job (requires Docker image and K8s cluster)
TRAINING_EXECUTOR=subprocess

# ==========================================
# GPU Settings (for subprocess mode)
# ==========================================
# USE_GPU: Enable/disable GPU usage (true/false)
# CUDA_VISIBLE_DEVICES: GPU device indices
#   - "0" = use GPU 0
#   - "0,1" = use GPU 0 and 1
#   - "-1" = use CPU only
USE_GPU=false
CUDA_VISIBLE_DEVICES=-1

# ==========================================
# Kubernetes Job Resources (for kubernetes mode)
# ==========================================
# K8S_GPU_COUNT: Number of GPUs to request
#   - 0 = CPU only (for local kind cluster testing)
#   - 1 = Single GPU (default production)
#   - 2+ = Multi-GPU training (AWS/GCP with GPU nodes)
# Memory/CPU limits and requests for K8s Pod
K8S_GPU_COUNT=0
K8S_MEMORY_LIMIT=16Gi
K8S_MEMORY_REQUEST=8Gi
K8S_CPU_LIMIT=4
K8S_CPU_REQUEST=2
