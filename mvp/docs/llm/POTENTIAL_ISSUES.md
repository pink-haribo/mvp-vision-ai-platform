# Phase 1 LLM Control - ì ì¬ì  ë¬¸ì œì  ë° ë””ë²„ê¹… ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-11-02
**ëª©ì **: ë¬¸ì„œëŒ€ë¡œ ì²˜ë¦¬ë˜ì§€ ì•Šì„ ë•Œ í™•ì¸í•´ì•¼ í•  ë¶€ë¶„ë“¤

## ê°œìš”

Phase 1 LLM ì»¨íŠ¸ë¡¤ì´ USER_UTTERANCE_EXAMPLES.md ë¬¸ì„œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì„ ë•Œ ë°œìƒ ê°€ëŠ¥í•œ ë¬¸ì œì ê³¼ í•´ê²° ë°©ë²•ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## 1. LLM íŒŒì‹± ì‹¤íŒ¨ (ê°€ì¥ í”í•œ ë¬¸ì œ)

### ë¬¸ì œ ì¦ìƒ

**ì‚¬ìš©ì ë°œí™”**:
```
"C:/datasets/coco8ë¡œ yolov8n í•™ìŠµí•´ì¤˜"
```

**ì˜ˆìƒ ë™ì‘**:
- LLMì´ dataset_pathì™€ model_name ì¶”ì¶œ
- ë°ì´í„°ì…‹ ë¶„ì„ ì‹¤í–‰
- í•™ìŠµ ì„¤ì •ìœ¼ë¡œ ì§„í–‰

**ì‹¤ì œ ë™ì‘**:
```
"í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´ ëª‡ ê°€ì§€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤:
1. ë°ì´í„°ì…‹ ê²½ë¡œëŠ”?
2. ì–´ë–¤ ëª¨ë¸ ì‚¬ìš©?"
```

### ì›ì¸ ë¶„ì„

#### ì›ì¸ 1: Geminiê°€ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•¨

**í™•ì¸ ë°©ë²•**:
```bash
# llm_debug.log í™•ì¸
cat mvp/backend/llm_debug.log

# ì¶œë ¥ ì˜ˆì‹œ:
[DEBUG] LLM Response for session 1:
Action: ASK_CLARIFICATION
Message: í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´...
Current Config: None  # â† ë¬¸ì œ!
Config: None  # â† ë¬¸ì œ!
```

**ê·¼ë³¸ ì›ì¸**:
1. **Gemini í”„ë¡¬í”„íŠ¸ê°€ ë¶ˆëª…í™•**
   - `llm_structured.py`ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ íŒŒë¼ë¯¸í„° ì¶”ì¶œì„ ëª…í™•íˆ ì§€ì‹œí•˜ì§€ ì•ŠìŒ
   - ì˜ˆì‹œê°€ ë¶€ì¡±í•¨

2. **Few-shot ì˜ˆì‹œ ë¶€ì¡±**
   - Geminiê°€ ì–´ë–¤ í˜•ì‹ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•´ì•¼ í•˜ëŠ”ì§€ ëª¨ë¦„

3. **Schema ì •ì˜ ë¶ˆëª…í™•**
   - `GeminiActionResponse`ì˜ `current_config` í•„ë“œ ì„¤ëª… ë¶€ì¡±

**í•´ê²° ë°©ë²•**:

**íŒŒì¼**: `mvp/backend/app/utils/llm_structured.py`

í˜„ì¬:
```python
system_prompt = """
You are an AI assistant for a Vision AI Training Platform.
Parse user intent and extract parameters.
"""
```

ê°œì„ :
```python
system_prompt = """
You are an AI assistant for a Vision AI Training Platform.

CRITICAL: ALWAYS extract parameters from user messages.

Example 1:
User: "C:/datasets/coco8ë¡œ yolov8n í•™ìŠµí•´ì¤˜"
Response:
{
  "action": "ANALYZE_DATASET",
  "current_config": {
    "dataset_path": "C:/datasets/coco8",
    "model_name": "yolov8n",
    "framework": "ultralytics"
  }
}

Example 2:
User: "resnet50 ì •ë³´ ì•Œë ¤ì¤˜"
Response:
{
  "action": "SHOW_MODEL_INFO",
  "current_config": {
    "model_name": "resnet50",
    "framework": "timm"
  }
}

ALWAYS populate current_config with extracted parameters!
"""
```

**ê²€ì¦ ë°©ë²•**:
```python
# í…ŒìŠ¤íŠ¸ ì¶”ê°€
async def test_llm_extracts_parameters():
    """LLMì´ íŒŒë¼ë¯¸í„°ë¥¼ ì œëŒ€ë¡œ ì¶”ì¶œí•˜ëŠ”ì§€ í™•ì¸"""
    response = await structured_intent_parser.parse_intent(
        user_message="C:/datasets/coco8ë¡œ yolov8n í•™ìŠµ",
        state=ConversationState.INITIAL,
        context="",
        temp_data={}
    )

    assert response.current_config is not None
    assert response.current_config.get("dataset_path") == "C:/datasets/coco8"
    assert response.current_config.get("model_name") == "yolov8n"
```

---

#### ì›ì¸ 2: Gemini API ì‘ë‹µì´ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ

**í™•ì¸ ë°©ë²•**:
```bash
# Gemini ì›ë³¸ ì‘ë‹µ í™•ì¸
tail -50 mvp/backend/gemini_responses.txt

# ì¶œë ¥ ì˜ˆì‹œ:
{
  "action": "start_training",  # â† ActionType enumì´ ì•„ë‹˜!
  "dataset": "C:/datasets/coco8",  # â† current_configì— ì—†ìŒ!
  "model": "yolov8n"
}
```

**ê·¼ë³¸ ì›ì¸**:
- Geminiê°€ `GeminiActionResponse` Pydantic ìŠ¤í‚¤ë§ˆë¥¼ ë¬´ì‹œ
- JSON í•„ë“œëª…ì„ ì„ì˜ë¡œ ë³€ê²½

**í•´ê²° ë°©ë²•**:

**íŒŒì¼**: `mvp/backend/app/utils/llm_structured.py`

```python
async def parse_intent(...) -> GeminiActionResponse:
    # ... Gemini API í˜¸ì¶œ ...

    raw_response = response.text

    # CRITICAL: Validate and fix Gemini response
    try:
        parsed = json.loads(raw_response)

        # Fix common mistakes
        if "action" in parsed:
            # Convert string to ActionType
            action_str = parsed["action"]
            if action_str.lower() == "start_training":
                parsed["action"] = "START_TRAINING"
            # ... ë‹¤ë¥¸ ë§¤í•‘ ...

        # Extract parameters from wrong fields
        if "dataset" in parsed and "current_config" not in parsed:
            parsed["current_config"] = {
                "dataset_path": parsed.pop("dataset"),
                "model_name": parsed.pop("model", None)
            }

        # Validate with Pydantic
        return GeminiActionResponse(**parsed)

    except Exception as e:
        logger.error(f"Failed to parse Gemini response: {e}")
        logger.error(f"Raw response: {raw_response}")
        # Fallback to ASK_CLARIFICATION
        return GeminiActionResponse(
            action=ActionType.ASK_CLARIFICATION,
            message="ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ í•œë²ˆ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
        )
```

---

### ì›ì¸ 3: Fallback ì¶”ì¶œë„ ì‹¤íŒ¨

**í™•ì¸ ë°©ë²•**:
```bash
# Fallback ë¡œê·¸ í™•ì¸
cat mvp/data/logs/fallback_debug.log

# ì¶œë ¥ ì˜ˆì‹œ:
[2025-11-02 10:30:00] Action: START_TRAINING
Before: {}
User message: C:/datasets/coco8ë¡œ yolov8n í•™ìŠµ
After: {}  # â† ì•„ë¬´ê²ƒë„ ì¶”ì¶œ ì•ˆë¨!
```

**ê·¼ë³¸ ì›ì¸**:
- `_extract_from_user_message`ì˜ ì •ê·œì‹ì´ ì‹¤íŒ¨
- ê²½ë¡œ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦„ (Windows vs Linux)

**í•´ê²° ë°©ë²•**:

**íŒŒì¼**: `mvp/backend/app/services/action_handlers.py`

í˜„ì¬ ì •ê·œì‹:
```python
def _extract_from_user_message(self, user_message: str, existing_config: dict) -> dict:
    # ê²½ë¡œ ì¶”ì¶œ (Windowsë§Œ ì§€ì›)
    path_pattern = r'([A-Z]:/[^\s]+)'

    # job_id ì¶”ì¶œ
    job_pattern = r'job\s+(\d+)'
```

ê°œì„ :
```python
def _extract_from_user_message(self, user_message: str, existing_config: dict) -> dict:
    """Extract parameters from user message with robust regex"""

    # ê²½ë¡œ ì¶”ì¶œ (Windows + Linux + í•œê¸€ ê²½ë¡œ)
    path_patterns = [
        r'([A-Z]:/[^\s]+)',           # Windows: C:/path
        r'([A-Z]:\\[^\s]+)',          # Windows backslash: C:\path
        r'(/[^\s]+)',                 # Linux: /path
        r'([ê°€-í£A-Za-z0-9_/\\:]+)',  # í•œê¸€ í¬í•¨ ê²½ë¡œ
    ]

    for pattern in path_patterns:
        match = re.search(pattern, user_message)
        if match:
            path = match.group(1)
            # Validate path exists
            from pathlib import Path
            if Path(path).exists():
                existing_config["dataset_path"] = path
                logger.info(f"[FALLBACK] Extracted dataset_path: {path}")
                break

    # job_id ì¶”ì¶œ (ë‹¤ì–‘í•œ í˜•ì‹)
    job_patterns = [
        r'job\s+(\d+)',          # "job 42"
        r'ì‘ì—…\s+(\d+)',         # "ì‘ì—… 42"
        r'(\d+)ë²ˆ',              # "42ë²ˆ"
        r'#(\d+)',               # "#42"
    ]

    for pattern in job_patterns:
        match = re.search(pattern, user_message)
        if match:
            job_id = int(match.group(1))
            existing_config["job_id"] = job_id
            logger.info(f"[FALLBACK] Extracted job_id: {job_id}")
            break

    # ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
    known_models = [
        "resnet50", "resnet18", "efficientnet_b0",
        "yolov8n", "yolov8s", "yolov8m", "yolov8l", "yolov8x"
    ]

    for model in known_models:
        if model.lower() in user_message.lower():
            existing_config["model_name"] = model
            logger.info(f"[FALLBACK] Extracted model_name: {model}")

            # Framework auto-detection
            if model.startswith("yolo"):
                existing_config["framework"] = "ultralytics"
            elif model.startswith("resnet") or model.startswith("efficient"):
                existing_config["framework"] = "timm"
            break

    # ìˆ«ì íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    epoch_match = re.search(r'(\d+)\s*ì—?í­', user_message)
    if epoch_match:
        existing_config["epochs"] = int(epoch_match.group(1))

    batch_match = re.search(r'ë°°ì¹˜\s*(\d+)', user_message)
    if batch_match:
        existing_config["batch_size"] = int(batch_match.group(1))

    return existing_config
```

**í…ŒìŠ¤íŠ¸ ì¶”ê°€**:
```python
def test_fallback_extraction():
    """Fallbackì´ ë‹¤ì–‘í•œ í˜•ì‹ì„ ì²˜ë¦¬í•˜ëŠ”ì§€ í™•ì¸"""
    handler = ActionHandlers(db)

    # Windows ê²½ë¡œ
    config = handler._extract_from_user_message(
        "C:/datasets/coco8ë¡œ í•™ìŠµ",
        {}
    )
    assert config["dataset_path"] == "C:/datasets/coco8"

    # Linux ê²½ë¡œ
    config = handler._extract_from_user_message(
        "/home/user/dataë¡œ í•™ìŠµ",
        {}
    )
    assert config["dataset_path"] == "/home/user/data"

    # job_id ë‹¤ì–‘í•œ í˜•ì‹
    assert handler._extract_from_user_message("job 42 ì¤‘ì§€", {})["job_id"] == 42
    assert handler._extract_from_user_message("42ë²ˆ ì¤‘ì§€", {})["job_id"] == 42
    assert handler._extract_from_user_message("#42 ì¤‘ì§€", {})["job_id"] == 42
```

---

## 2. Tool ì‹¤í–‰ ì˜¤ë¥˜

### ë¬¸ì œ ì¦ìƒ

**ì‚¬ìš©ì ë°œí™”**:
```
"C:/datasets/notexist ë¶„ì„í•´ì¤˜"
```

**ì˜ˆìƒ ë™ì‘**:
- "í•´ë‹¹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" ì—ëŸ¬ ë©”ì‹œì§€

**ì‹¤ì œ ë™ì‘**:
```
500 Internal Server Error
```

### ì›ì¸ ë¶„ì„

**í™•ì¸ ë°©ë²•**:
```bash
# ë°±ì—”ë“œ ë¡œê·¸ í™•ì¸
tail -100 mvp/backend/app.log

# ì¶œë ¥ ì˜ˆì‹œ:
ERROR: Tool analyze_dataset failed: [Errno 2] No such file or directory: 'C:/datasets/notexist'
Traceback (most recent call last):
  File "app/utils/tool_registry.py", line 287, in _analyze_dataset
    analysis = analyze_dataset(dataset_path)
  File "app/utils/dataset_analyzer.py", line 45, in analyze_dataset
    items = os.listdir(dataset_path)  # â† ì—ëŸ¬!
FileNotFoundError: [Errno 2] No such file or directory
```

**ê·¼ë³¸ ì›ì¸**:
- `tool_registry._analyze_dataset`ì—ì„œ ê²½ë¡œ ê²€ì¦ í›„ì—ë„ `dataset_analyzer`ê°€ ë‹¤ì‹œ ì—ëŸ¬
- ì˜ˆì™¸ ì²˜ë¦¬ ëˆ„ë½

**í•´ê²° ë°©ë²•**:

**íŒŒì¼**: `mvp/backend/app/utils/tool_registry.py`

í˜„ì¬:
```python
async def _analyze_dataset(self, params, db, user_id):
    dataset_path = params.get("dataset_path")

    if not Path(dataset_path).exists():
        raise ValueError(f"Dataset path does not exist: {dataset_path}")

    analysis = analyze_dataset(dataset_path)  # â† ì—¬ê¸°ì„œ ë˜ ì—ëŸ¬ ê°€ëŠ¥
    return {...}
```

ê°œì„ :
```python
async def _analyze_dataset(self, params, db, user_id):
    dataset_path = params.get("dataset_path")

    if not dataset_path:
        raise ValueError("dataset_path is required")

    path = Path(dataset_path)

    # ë” ìƒì„¸í•œ ê²€ì¦
    if not path.exists():
        raise ValueError(f"ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_path}")

    if not path.is_dir():
        raise ValueError(f"ê²½ë¡œê°€ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {dataset_path}")

    # ì½ê¸° ê¶Œí•œ í™•ì¸
    if not os.access(dataset_path, os.R_OK):
        raise ValueError(f"ê²½ë¡œì— ëŒ€í•œ ì½ê¸° ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")

    try:
        analysis = analyze_dataset(dataset_path)
    except Exception as e:
        logger.error(f"Dataset analysis failed: {e}", exc_info=True)
        raise ValueError(f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    return {...}
```

**ActionHandlerì—ì„œ ì—ëŸ¬ ì²˜ë¦¬**:

**íŒŒì¼**: `mvp/backend/app/services/action_handlers.py`

```python
async def _handle_analyze_dataset(...):
    try:
        result = await tool_registry.call_tool(
            "analyze_dataset",
            {"dataset_path": dataset_path},
            self.db,
            user_id=None
        )

        # Success path...

    except ValueError as e:
        # User-friendly error
        return {
            "new_state": ConversationState.ERROR,
            "message": f"âŒ {str(e)}\n\nì˜¬ë°”ë¥¸ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.",
            "temp_data": temp_data
        }
    except Exception as e:
        # Unexpected error
        logger.error(f"Unexpected error: {e}", exc_info=True)
        return {
            "new_state": ConversationState.ERROR,
            "message": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "temp_data": temp_data
        }
```

---

## 3. ìƒíƒœ ì „í™˜ ì˜¤ë¥˜

### ë¬¸ì œ ì¦ìƒ

**ì‹œë‚˜ë¦¬ì˜¤**:
1. ì‚¬ìš©ì: "C:/datasets/coco8 ë¶„ì„í•´ì¤˜"
2. ì‹œìŠ¤í…œ: ë¶„ì„ ê²°ê³¼ í‘œì‹œ
3. ì‚¬ìš©ì: "í•™ìŠµ ì‹œì‘"
4. ì‹œìŠ¤í…œ: "ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”" â† ì´ë¯¸ ë¶„ì„í–ˆëŠ”ë°!

**ì˜ˆìƒ ë™ì‘**:
- ì´ë¯¸ ë¶„ì„í•œ ë°ì´í„°ì…‹ ì •ë³´ë¥¼ ìœ ì§€
- ë°”ë¡œ ëª¨ë¸ ì„ íƒìœ¼ë¡œ ì§„í–‰

**ì‹¤ì œ ë™ì‘**:
- `temp_data`ì—ì„œ dataset_pathê°€ ì‚¬ë¼ì§
- ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë¬¼ì–´ë´„

### ì›ì¸ ë¶„ì„

**í™•ì¸ ë°©ë²•**:
```python
# conversation_manager.pyì˜ TRACE ë¡œê·¸ í™•ì¸
# ì¶œë ¥ ì˜ˆì‹œ:
[TRACE-5-SAVE] Saving to DB:
  new_state: ANALYZING_DATASET
  updated_temp_data: {"dataset_analysis": {...}}  # â† config ì—†ìŒ!
```

**ê·¼ë³¸ ì›ì¸**:
- `_handle_analyze_dataset`ê°€ `config`ì— dataset_pathë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ
- ë‹¤ìŒ ìš”ì²­ì—ì„œ dataset_pathë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ

**í•´ê²° ë°©ë²•**:

**íŒŒì¼**: `mvp/backend/app/services/action_handlers.py`

í˜„ì¬:
```python
async def _handle_analyze_dataset(...):
    # ... analysis ...

    temp_data["dataset_analysis"] = result

    return {
        "new_state": ConversationState.ANALYZING_DATASET,
        "message": message,
        "temp_data": temp_data
    }
```

ê°œì„ :
```python
async def _handle_analyze_dataset(...):
    # ... analysis ...

    # CRITICAL: ë¶„ì„í•œ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ configì— ì €ì¥
    config = temp_data.get("config", {})
    config["dataset_path"] = dataset_path
    config["task_type"] = result.get("task_type")  # YOLO â†’ object_detection
    config["num_classes"] = result.get("num_classes")

    temp_data["config"] = config
    temp_data["dataset_analysis"] = result

    return {
        "new_state": ConversationState.ANALYZING_DATASET,
        "message": message,
        "temp_data": temp_data
    }
```

**ê²€ì¦ í…ŒìŠ¤íŠ¸**:
```python
async def test_dataset_path_persists():
    """ë°ì´í„°ì…‹ ë¶„ì„ í›„ ê²½ë¡œê°€ ìœ ì§€ë˜ëŠ”ì§€ í™•ì¸"""
    session = create_test_session()

    # Step 1: Analyze dataset
    response1 = await conversation_manager.process_message(
        session.id,
        "C:/datasets/coco8 ë¶„ì„í•´ì¤˜"
    )

    # Verify dataset_path is in config
    session.refresh()
    assert session.temp_data["config"]["dataset_path"] == "C:/datasets/coco8"

    # Step 2: Start training (should use saved dataset_path)
    response2 = await conversation_manager.process_message(
        session.id,
        "yolov8nìœ¼ë¡œ í•™ìŠµ ì‹œì‘"
    )

    # Should NOT ask for dataset_path again
    assert "ë°ì´í„°ì…‹ ê²½ë¡œ" not in response2["message"]
```

---

## 4. í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ ë¬¸ì œ

### ë¬¸ì œ ì¦ìƒ

**ë°±ì—”ë“œ ì‘ë‹µ**:
```json
{
  "message": "ğŸ“Š **ë°ì´í„°ì…‹ ë¶„ì„ ê²°ê³¼**\n\nê²½ë¡œ: C:/datasets/coco8",
  "state": "analyzing_dataset",
  "dataset_analysis": {
    "format": "yolo",
    "classes": [...]
  }
}
```

**í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œ**:
```
ğŸ“Š **ë°ì´í„°ì…‹ ë¶„ì„ ê²°ê³¼**\n\nê²½ë¡œ: C:/datasets/coco8
```
â† Markdownì´ ë Œë”ë§ë˜ì§€ ì•ŠìŒ!

### ì›ì¸ ë¶„ì„

**í™•ì¸ ë°©ë²•**:
```typescript
// mvp/frontend/components/ChatPanel.tsx
console.log("Backend response:", response);

// ì¶œë ¥:
{
  message: "ğŸ“Š **ë°ì´í„°ì…‹ ë¶„ì„ ê²°ê³¼**\n\nê²½ë¡œ: C:/datasets/coco8",
  // \nì´ escapeë˜ì–´ ìˆìŒ
}
```

**ê·¼ë³¸ ì›ì¸**:
1. ë°±ì—”ë“œê°€ `\n`ì„ ë¬¸ìì—´ë¡œ ì „ì†¡
2. í”„ë¡ íŠ¸ì—”ë“œê°€ Markdown íŒŒì‹± ì•ˆí•¨
3. `<ReactMarkdown>` ì»´í¬ë„ŒíŠ¸ ë¯¸ì‚¬ìš©

**í•´ê²° ë°©ë²•**:

**íŒŒì¼**: `mvp/frontend/components/ChatPanel.tsx`

í˜„ì¬:
```typescript
<div className="message">
  {message.content}
</div>
```

ê°œì„ :
```typescript
import ReactMarkdown from 'react-markdown';

<div className="message">
  <ReactMarkdown>{message.content}</ReactMarkdown>
</div>
```

**ìŠ¤íƒ€ì¼ ì¶”ê°€**:
```css
.message {
  /* Markdown ìŠ¤íƒ€ì¼ */
}

.message strong {
  font-weight: 700;
}

.message code {
  background: #f5f5f5;
  padding: 2px 4px;
  border-radius: 3px;
}

.message pre {
  background: #1e1e1e;
  color: #fff;
  padding: 12px;
  border-radius: 6px;
  overflow-x: auto;
}
```

---

### ë¬¸ì œ 2: dataset_analysisê°€ í‘œì‹œë˜ì§€ ì•ŠìŒ

**ë°±ì—”ë“œ ì‘ë‹µ**:
```json
{
  "message": "...",
  "dataset_analysis": {
    "format": "yolo",
    "classes": ["person", "car", ...]
  }
}
```

**í”„ë¡ íŠ¸ì—”ë“œ**:
- í´ë˜ìŠ¤ ëª©ë¡ì´ í‘œì‹œë˜ì§€ ì•ŠìŒ
- í†µê³„ê°€ í‘œì‹œë˜ì§€ ì•ŠìŒ

**ì›ì¸**:
- ChatPanelì´ `dataset_analysis` í•„ë“œë¥¼ ì½ì§€ ì•ŠìŒ

**í•´ê²° ë°©ë²•**:

**íŒŒì¼**: `mvp/frontend/components/ChatPanel.tsx`

```typescript
interface ChatResponse {
  message: string;
  state: string;
  dataset_analysis?: DatasetAnalysis;
  model_search_results?: ModelSearchResult[];
  training_job_id?: number;
}

function ChatPanel() {
  const [messages, setMessages] = useState<Message[]>([]);

  const handleSendMessage = async (text: string) => {
    const response = await fetch('/api/v1/chat/message', {
      method: 'POST',
      body: JSON.stringify({ message: text, session_id: sessionId })
    });

    const data: ChatResponse = await response.json();

    // Add assistant message
    setMessages([...messages, {
      role: 'assistant',
      content: data.message,
      metadata: {
        dataset_analysis: data.dataset_analysis,
        model_search_results: data.model_search_results,
        training_job_id: data.training_job_id
      }
    }]);
  };

  return (
    <div>
      {messages.map((msg, i) => (
        <div key={i}>
          <ReactMarkdown>{msg.content}</ReactMarkdown>

          {/* Dataset Analysis Card */}
          {msg.metadata?.dataset_analysis && (
            <DatasetAnalysisCard analysis={msg.metadata.dataset_analysis} />
          )}

          {/* Model Search Results */}
          {msg.metadata?.model_search_results && (
            <ModelSearchResults models={msg.metadata.model_search_results} />
          )}
        </div>
      ))}
    </div>
  );
}
```

---

## 5. í•™ìŠµ ì‹¤í–‰ ì˜¤ë¥˜

### ë¬¸ì œ ì¦ìƒ

**ì‚¬ìš©ì ë°œí™”**:
```
"ì˜ˆ" (í•™ìŠµ ì‹œì‘ í™•ì¸)
```

**ì˜ˆìƒ ë™ì‘**:
- TrainingJob ìƒì„±
- ë°±ê·¸ë¼ìš´ë“œë¡œ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
- "í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤! Job ID: 42"

**ì‹¤ì œ ë™ì‘**:
```
500 Internal Server Error

[ERROR] Failed to start training: [WinError 2] The system cannot find the file specified
```

### ì›ì¸ ë¶„ì„

**í™•ì¸ ë°©ë²•**:
```bash
# ë°±ì—”ë“œ ë¡œê·¸
tail -50 mvp/backend/app.log

# ì¶œë ¥:
ERROR: training_manager.start_training() failed
Traceback:
  File "app/utils/training_manager.py", line 123, in start_training
    process = subprocess.Popen(command, ...)
FileNotFoundError: [WinError 2] The system cannot find the file specified
Command: ['python', '-m', 'train', '--job-id', '42']
```

**ê·¼ë³¸ ì›ì¸**:
1. `train.py` ê²½ë¡œê°€ ì˜ëª»ë¨
2. Virtual environment Pythonì´ ì•„ë‹Œ ì‹œìŠ¤í…œ Python ì‚¬ìš©
3. ëª¨ë“ˆ import ì—ëŸ¬

**í•´ê²° ë°©ë²•**:

**íŒŒì¼**: `mvp/backend/app/utils/training_manager.py`

í˜„ì¬:
```python
def start_training(self, job_id: int):
    command = [
        'python',  # â† ì‹œìŠ¤í…œ Python!
        '-m', 'train',
        '--job-id', str(job_id)
    ]

    process = subprocess.Popen(command, ...)
```

ê°œì„ :
```python
def start_training(self, job_id: int):
    import sys
    from pathlib import Path

    # Use virtual environment Python
    python_executable = sys.executable  # venv/Scripts/python.exe

    # Find train.py path
    train_script = Path(__file__).parent.parent.parent / "training" / "train.py"

    if not train_script.exists():
        raise FileNotFoundError(f"Training script not found: {train_script}")

    command = [
        python_executable,
        str(train_script),
        '--job-id', str(job_id)
    ]

    logger.info(f"Starting training with command: {' '.join(command)}")

    try:
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            cwd=str(train_script.parent),  # Set working directory
            env=os.environ.copy()  # Inherit environment
        )

        # Save process ID
        job.process_id = process.pid
        self.db.commit()

        logger.info(f"Training started: PID {process.pid}")

    except Exception as e:
        logger.error(f"Failed to start training: {e}", exc_info=True)
        job.status = "failed"
        job.error_message = str(e)
        self.db.commit()
        raise
```

---

## 6. ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì•ˆë¨

### ë¬¸ì œ ì¦ìƒ

**ì˜ˆìƒ ë™ì‘**:
- í•™ìŠµ ì¤‘ Epochë§ˆë‹¤ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
- í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ Loss/Accuracy í‘œì‹œ

**ì‹¤ì œ ë™ì‘**:
- "í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!" ì´í›„ ì•„ë¬´ ì—…ë°ì´íŠ¸ ì—†ìŒ
- "job 42 ìƒíƒœ ì•Œë ¤ì¤˜"ë¥¼ í•´ë„ "Epoch 0/50"

### ì›ì¸ ë¶„ì„

**í™•ì¸ ë°©ë²•**:
```bash
# training.log í™•ì¸
cat mvp/data/logs/job_42/training.log

# ì¶œë ¥:
Epoch 1/50
Train Loss: 0.6931, Train Acc: 50.2%
Val Loss: 0.6895, Val Acc: 51.8%
# â† ë¡œê·¸ëŠ” ì“°ì—¬ì§€ê³  ìˆìŒ!
```

```sql
-- DB í™•ì¸
SELECT * FROM training_metrics WHERE job_id = 42;

-- ì¶œë ¥:
(empty)  â† ë©”íŠ¸ë¦­ì´ DBì— ì €ì¥ ì•ˆë¨!
```

**ê·¼ë³¸ ì›ì¸**:
- `train.py`ê°€ ë©”íŠ¸ë¦­ì„ stdoutì—ë§Œ ì¶œë ¥
- DBì— ì €ì¥í•˜ì§€ ì•ŠìŒ
- TrainingMetric ë ˆì½”ë“œ ìƒì„± ëˆ„ë½

**í•´ê²° ë°©ë²•**:

**íŒŒì¼**: `mvp/training/train.py`

í˜„ì¬:
```python
for epoch in range(epochs):
    train_loss, train_acc = train_one_epoch(...)
    val_loss, val_acc = validate(...)

    # Only print
    print(f"Epoch {epoch+1}/{epochs}")
    print(f"Train Loss: {train_loss}, Train Acc: {train_acc}")
    print(f"Val Loss: {val_loss}, Val Acc: {val_acc}")
```

ê°œì„ :
```python
from app.db.database import SessionLocal
from app.db.models import TrainingMetric, TrainingJob

def save_metrics_to_db(job_id, epoch, metrics):
    """Save metrics to database"""
    db = SessionLocal()
    try:
        # Create metric record
        metric = TrainingMetric(
            job_id=job_id,
            epoch=epoch,
            loss=metrics['train_loss'],
            accuracy=metrics['train_acc'],
            val_loss=metrics['val_loss'],
            val_accuracy=metrics['val_acc']
        )
        db.add(metric)

        # Update job current_epoch
        job = db.query(TrainingJob).filter(TrainingJob.id == job_id).first()
        if job:
            job.current_epoch = epoch
            if epoch == job.epochs:  # Final epoch
                job.final_accuracy = metrics['val_acc']

        db.commit()
        logger.info(f"Saved metrics for job {job_id}, epoch {epoch}")
    except Exception as e:
        logger.error(f"Failed to save metrics: {e}")
        db.rollback()
    finally:
        db.close()

# Training loop
for epoch in range(epochs):
    train_loss, train_acc = train_one_epoch(...)
    val_loss, val_acc = validate(...)

    metrics = {
        'train_loss': train_loss,
        'train_acc': train_acc,
        'val_loss': val_loss,
        'val_acc': val_acc
    }

    # Print to stdout (for logs)
    print(f"Epoch {epoch+1}/{epochs}")
    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2%}")
    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2%}")

    # Save to DB
    save_metrics_to_db(args.job_id, epoch + 1, metrics)
```

---

## 7. ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸

ë¬¸ì„œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì„ ë•Œ ìˆœì„œëŒ€ë¡œ í™•ì¸:

### Step 1: LLM íŒŒì‹± í™•ì¸
```bash
# LLM ì‘ë‹µ ë¡œê·¸ í™•ì¸
cat mvp/backend/llm_debug.log | tail -50

# í™•ì¸ ì‚¬í•­:
âœ… Actionì´ ì˜¬ë°”ë¥¸ê°€?
âœ… current_configì— íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ”ê°€?
âœ… messageê°€ ì ì ˆí•œê°€?
```

### Step 2: Fallback ì¶”ì¶œ í™•ì¸
```bash
# Fallback ë¡œê·¸ í™•ì¸
cat mvp/data/logs/fallback_debug.log | tail -20

# í™•ì¸ ì‚¬í•­:
âœ… Before/Afterì—ì„œ configê°€ ì¶”ì¶œë˜ì—ˆëŠ”ê°€?
âœ… dataset_path, model_name, job_id ë“±ì´ ìˆëŠ”ê°€?
```

### Step 3: Action Handler ì‹¤í–‰ í™•ì¸
```bash
# ë°±ì—”ë“œ ë¡œê·¸ í™•ì¸
tail -100 mvp/backend/app.log | grep "handle_action"

# í™•ì¸ ì‚¬í•­:
âœ… Handlerê°€ í˜¸ì¶œë˜ì—ˆëŠ”ê°€?
âœ… ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ëŠ”ê°€?
âœ… ì˜¬ë°”ë¥¸ ìƒíƒœë¡œ ì „í™˜ë˜ì—ˆëŠ”ê°€?
```

### Step 4: Tool ì‹¤í–‰ í™•ì¸
```bash
# Tool ì‹¤í–‰ ë¡œê·¸
tail -100 mvp/backend/app.log | grep "Executing tool"

# í™•ì¸ ì‚¬í•­:
âœ… Toolì´ í˜¸ì¶œë˜ì—ˆëŠ”ê°€?
âœ… íŒŒë¼ë¯¸í„°ê°€ ì˜¬ë°”ë¥¸ê°€?
âœ… ê²°ê³¼ê°€ ë°˜í™˜ë˜ì—ˆëŠ”ê°€?
```

### Step 5: DB ì €ì¥ í™•ì¸
```sql
-- Session temp_data í™•ì¸
SELECT id, state, temp_data FROM sessions WHERE id = 1;

-- í™•ì¸ ì‚¬í•­:
-- temp_dataì— configê°€ ìˆëŠ”ê°€?
-- dataset_analysisê°€ ì €ì¥ë˜ì—ˆëŠ”ê°€?
```

### Step 6: í”„ë¡ íŠ¸ì—”ë“œ ìˆ˜ì‹  í™•ì¸
```javascript
// Browser console
console.log("Response from backend:", response);

// í™•ì¸ ì‚¬í•­:
// messageê°€ ìˆëŠ”ê°€?
// dataset_analysis ë“± ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ”ê°€?
```

---

## 8. ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ TOP 5

### 1ìœ„: LLMì´ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œ ì•ˆí•¨ (60%)
**í•´ê²°**: í”„ë¡¬í”„íŠ¸ ê°œì„  + Few-shot ì˜ˆì‹œ ì¶”ê°€

### 2ìœ„: ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ (20%)
**í•´ê²°**: ë” ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ + ê²½ë¡œ ê²€ì¦ ê°•í™”

### 3ìœ„: temp_dataê°€ ì´ˆê¸°í™”ë¨ (10%)
**í•´ê²°**: flag_modified() ì‚¬ìš© + config ë³‘í•© ë¡œì§ í™•ì¸

### 4ìœ„: í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì‹¤íŒ¨ (5%)
**í•´ê²°**: Python executable ê²½ë¡œ ìˆ˜ì • + í™˜ê²½ë³€ìˆ˜ ì „ë‹¬

### 5ìœ„: ë©”íŠ¸ë¦­ì´ ì—…ë°ì´íŠ¸ ì•ˆë¨ (5%)
**í•´ê²°**: train.pyì—ì„œ DB ì €ì¥ ì¶”ê°€

---

## ìš”ì•½

ë¬¸ì„œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì„ ë•Œ ê°€ì¥ ë¨¼ì € í™•ì¸í•´ì•¼ í•  ê²ƒ:

1. **LLM íŒŒì‹±**: `llm_debug.log`ì—ì„œ `current_config` í™•ì¸
2. **Fallback ì¶”ì¶œ**: `fallback_debug.log`ì—ì„œ ì¶”ì¶œ ê²°ê³¼ í™•ì¸
3. **Tool ì‹¤í–‰**: `app.log`ì—ì„œ ì—ëŸ¬ í™•ì¸
4. **ìƒíƒœ ì „í™˜**: `conversation_manager.py`ì˜ TRACE ë¡œê·¸ í™•ì¸
5. **DB ì €ì¥**: SQLiteì—ì„œ `temp_data` í™•ì¸

90%ì˜ ë¬¸ì œëŠ” ìœ„ 5ê°€ì§€ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤!
