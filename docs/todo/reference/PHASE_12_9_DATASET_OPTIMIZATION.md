# Phase 12.9: Dataset Caching & Selective Download

**ë¸Œëœì¹˜**: `feature/phase-12.9-dataset-optimization`

**ëª©í‘œ**: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ íš¨ìœ¨í™” ë° ìºì‹± ì „ëµ êµ¬í˜„ìœ¼ë¡œ ë°˜ë³µ í•™ìŠµ ì†ë„ í–¥ìƒ

**ë‚ ì§œ**: 2025-12-02

**ì˜ˆìƒ ê¸°ê°„**: 1.5ì¼ (12ì‹œê°„)

---

## ë¬¸ì œ ì¸ì‹

### í˜„ì¬ ë¬¸ì œì 

1. **ë§¤ Jobë§ˆë‹¤ ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ**
   - Job 91, 92, 93... ëª¨ë‘ ê°™ì€ `ds_c75023ca76d7448b`ë¥¼ ê°ê° 3ë¶„ì”© ë‹¤ìš´ë¡œë“œ
   - 10ê°œ Job = 30ë¶„ ëŒ€ê¸° ì‹œê°„

2. **ë¶ˆí•„ìš”í•œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ**
   - ì „ì²´ `datasets/{id}/images/` prefixë¥¼ ë‹¤ìš´ë¡œë“œ
   - MVTec-AD: 163ê°œ labeled images vs 1000+ total images (6ë°° ë‚­ë¹„)
   - Annotation ì—†ëŠ” ì´ë¯¸ì§€ê¹Œì§€ ëª¨ë‘ ë‹¤ìš´ë¡œë“œ

3. **Completed/Failed Job Restart ë¶ˆê°€**
   - `status != "pending"`ì´ë©´ ì‹œì‘ ë¶ˆê°€
   - ë°˜ë³µ ì‹¤í—˜ì„ ìœ„í•´ ë§¤ë²ˆ ìƒˆ Job ìƒì„± í•„ìš”
   - Hyperparameter tuning ì‹œ ë¹„íš¨ìœ¨ì 

4. **ë””ìŠ¤í¬ ê³µê°„ ë‚­ë¹„**
   - ê° Jobë³„ë¡œ ë…ë¦½ì ì¸ dataset ë³µì‚¬ë³¸ ì €ì¥
   - `/tmp/training/91/dataset`, `/tmp/training/92/dataset` ... ëª¨ë‘ ë™ì¼ ë‚´ìš©

### ì¸¡ì •ëœ ì˜í–¥

```
í˜„ì¬ ìƒí™© (10 Jobs, ê°™ì€ dataset):
  - ì´ ëŒ€ê¸° ì‹œê°„: 30ë¶„ (ê° 3ë¶„ Ã— 10)
  - ì´ ë‹¤ìš´ë¡œë“œ: 15GB (ê° 1.5GB Ã— 10)
  - ì´ ë””ìŠ¤í¬ ì‚¬ìš©: 15GB (ì¤‘ë³µ ì €ì¥)

ëª©í‘œ (ìºì‹± ì ìš© í›„):
  - ì´ ëŒ€ê¸° ì‹œê°„: ~3ë¶„ (ì²« Job 3ë¶„, ë‚˜ë¨¸ì§€ < 1ì´ˆ)
  - ì´ ë‹¤ìš´ë¡œë“œ: 1.5GB (ìºì‹œ 1íšŒë§Œ)
  - ì´ ë””ìŠ¤í¬ ì‚¬ìš©: 1.5GB (ê³µìœ  ìºì‹œ)

ê°œì„  íš¨ê³¼:
  - ì‹œê°„: 90% ì ˆê° (30ë¶„ â†’ 3ë¶„)
  - ëŒ€ì—­í­: 90% ì ˆê° (15GB â†’ 1.5GB)
  - ë””ìŠ¤í¬: 90% ì ˆê° (15GB â†’ 1.5GB)
```

---

## ê¸°ìˆ ì  ë°°ê²½

### í˜„ì¬ Snapshot ì‹œìŠ¤í…œ (Phase 12.6)

Phase 12.6ì—ì„œ ì´ë¯¸ ì™„ë²½í•œ ë²„ì „ ê´€ë¦¬ ì¸í”„ë¼ êµ¬ì¶•:

```python
class DatasetSnapshot(Base):
    id = Column(String(100), primary_key=True)  # snap_2b2fca921e88
    dataset_id = Column(String(100), nullable=False)  # ds_c75023ca76d7448b
    storage_path = Column(String(500), nullable=False)  # datasets/ds_c75023ca76d7448b/
    dataset_version_hash = Column(String(64), nullable=True)  # SHA256 hash
    created_at = Column(DateTime, nullable=False)
```

**Hash ê³„ì‚° ë°©ì‹** (`snapshot_service.py:143-199`):
```python
def _calculate_dataset_hash(dataset_path: str) -> str:
    """
    Calculate SHA256 hash of metadata files only (not images).

    Files included:
    - annotations_detection.json
    - metadata.json
    - data.yaml
    - *.txt files

    Images NOT included:
    - Fast computation (no GB hashing)
    - Annotation changes are what matter
    """
```

**ì¥ì **:
- Immutable snapshot = ìºì‹œ ì•ˆì „
- Hash ë³€ê²½ = ë°ì´í„°ì…‹ ë‚´ìš© ë³€ê²½ ìë™ ê°ì§€
- Collision detection: ê°™ì€ hash = ê°™ì€ ë‚´ìš©

---

## í•´ê²° ë°©ì•ˆ

### 12.9.1 Snapshot ê¸°ë°˜ ë°ì´í„°ì…‹ ìºì‹±

**ì•„í‚¤í…ì²˜**: Snapshot ID + Hash ê¸°ë°˜ ê³µìœ  ìºì‹œ

#### Cache Key êµ¬ì¡°

```
Cache Key = {snapshot_id}_{dataset_version_hash[:8]}

ì˜ˆì‹œ:
  snap_2b2fca921e88_1bb25f37
  snap_abc123def456_3ca92d81
```

#### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
/tmp/datasets/  â† Shared cache directory
  snap_2b2fca921e88_1bb25f37/  â† Cached dataset
    annotations_detection.json
    images/
      images/wood/scratch/000.png
      images/zipper/good/001.png
      ...
  snap_abc123def456_3ca92d81/  â† Different version
    annotations_detection.json
    images/...

/tmp/training/  â† Job-specific directories
  91/
    dataset -> /tmp/datasets/snap_2b2fca921e88_1bb25f37  â† Symlink
  92/
    dataset -> /tmp/datasets/snap_2b2fca921e88_1bb25f37  â† Reuse
  93/
    dataset -> /tmp/datasets/snap_abc123def456_3ca92d81  â† New version
```

#### ìºì‹± í”Œë¡œìš°

```python
# trainer_sdk.py

SHARED_DATASET_CACHE = Path("/tmp/datasets")
CACHE_MAX_SIZE_GB = 50  # LRU eviction

def download_dataset_with_cache(
    self,
    snapshot_id: str,
    dataset_id: str,
    dataset_version_hash: str,
    dest_dir: str
) -> str:
    """
    Download dataset with caching support.

    Args:
        snapshot_id: Snapshot ID (snap_abc123)
        dataset_id: Original dataset ID (ds_c75023ca76d7448b)
        dataset_version_hash: SHA256 hash from SnapshotService
        dest_dir: Job working directory (/tmp/training/92)

    Returns:
        Local dataset directory path
    """
    # 1. Build cache key
    cache_key = f"{snapshot_id}_{dataset_version_hash[:8]}"
    cache_dir = SHARED_DATASET_CACHE / cache_key

    # 2. Check cache
    if cache_dir.exists():
        if self._verify_cache_integrity(cache_dir, dataset_version_hash):
            logger.info(f"âœ… Cache HIT: {cache_key}")
            self._update_last_accessed(cache_key)
            return self._link_to_cache(cache_dir, dest_dir)
        else:
            logger.warning(f"âš ï¸ Cache corrupted: {cache_key}, re-downloading")
            shutil.rmtree(cache_dir)

    # 3. Cache MISS - Download dataset
    logger.info(f"âŒ Cache MISS: {cache_key}, downloading...")

    # Create cache directory
    cache_dir.mkdir(parents=True, exist_ok=True)

    # Download dataset (selective download)
    self.download_dataset_selective(
        dataset_id=dataset_id,
        dest_dir=str(cache_dir)
    )

    # 4. Verify downloaded data
    if not self._verify_cache_integrity(cache_dir, dataset_version_hash):
        raise RuntimeError(f"Downloaded data hash mismatch for {cache_key}")

    # 5. Update cache metadata
    self._update_cache_metadata(cache_key, {
        'snapshot_id': snapshot_id,
        'dataset_id': dataset_id,
        'dataset_version_hash': dataset_version_hash,
        'created_at': datetime.utcnow().isoformat(),
        'last_accessed': datetime.utcnow().isoformat(),
        'size_bytes': self._calculate_dir_size(cache_dir)
    })

    # 6. Check cache size and evict if needed
    self._enforce_cache_size_limit()

    # 7. Link to job directory
    return self._link_to_cache(cache_dir, dest_dir)
```

#### Helper ë©”ì„œë“œ

```python
def _verify_cache_integrity(
    self,
    cache_dir: Path,
    expected_hash: str
) -> bool:
    """
    Verify cache integrity by recalculating hash.

    Matches SnapshotService logic:
    - Only hash metadata files (.json, .yaml, .txt)
    - Skip images for performance
    """
    hasher = hashlib.sha256()

    # Find all metadata files
    metadata_files = sorted([
        f for f in cache_dir.rglob('*')
        if f.is_file() and f.suffix in ['.json', '.yaml', '.yml', '.txt']
    ])

    for file_path in metadata_files:
        with open(file_path, 'rb') as f:
            hasher.update(f.read())

    calculated_hash = hasher.hexdigest()

    if calculated_hash != expected_hash:
        logger.error(
            f"Cache integrity check failed:\n"
            f"  Expected: {expected_hash}\n"
            f"  Calculated: {calculated_hash}"
        )
        return False

    return True


def _link_to_cache(self, cache_dir: Path, dest_dir: str) -> str:
    """
    Create symlink from job directory to cache.

    /tmp/training/92/dataset -> /tmp/datasets/snap_2b2fca921e88_1bb25f37
    """
    job_dataset_dir = Path(dest_dir) / "dataset"

    if job_dataset_dir.exists():
        if job_dataset_dir.is_symlink():
            job_dataset_dir.unlink()
        else:
            shutil.rmtree(job_dataset_dir)

    job_dataset_dir.symlink_to(cache_dir, target_is_directory=True)

    logger.info(f"ğŸ“ Linked: {job_dataset_dir} -> {cache_dir}")

    return str(job_dataset_dir)


def _update_cache_metadata(self, cache_key: str, metadata: dict):
    """Update cache metadata JSON file"""
    cache_metadata_file = SHARED_DATASET_CACHE / ".cache_metadata.json"

    if cache_metadata_file.exists():
        with open(cache_metadata_file) as f:
            all_metadata = json.load(f)
    else:
        all_metadata = {}

    all_metadata[cache_key] = metadata

    with open(cache_metadata_file, 'w') as f:
        json.dump(all_metadata, f, indent=2)


def _enforce_cache_size_limit(self):
    """
    Enforce cache size limit using LRU eviction.

    Strategy:
    1. Calculate total cache size
    2. If > CACHE_MAX_SIZE_GB, evict least recently used
    3. Keep evicting until under limit
    """
    cache_metadata_file = SHARED_DATASET_CACHE / ".cache_metadata.json"

    if not cache_metadata_file.exists():
        return

    with open(cache_metadata_file) as f:
        metadata = json.load(f)

    # Calculate total size
    total_size_gb = sum(
        item['size_bytes'] for item in metadata.values()
    ) / (1024 ** 3)

    if total_size_gb <= CACHE_MAX_SIZE_GB:
        return

    logger.info(
        f"Cache size ({total_size_gb:.2f} GB) exceeds limit "
        f"({CACHE_MAX_SIZE_GB} GB), evicting LRU entries"
    )

    # Sort by last accessed (oldest first)
    sorted_items = sorted(
        metadata.items(),
        key=lambda x: x[1]['last_accessed']
    )

    # Evict until under limit
    for cache_key, item in sorted_items:
        cache_dir = SHARED_DATASET_CACHE / cache_key

        if cache_dir.exists():
            logger.info(f"ğŸ—‘ï¸ Evicting cache: {cache_key}")
            shutil.rmtree(cache_dir)

        del metadata[cache_key]

        # Recalculate total size
        total_size_gb = sum(
            item['size_bytes'] for item in metadata.values()
        ) / (1024 ** 3)

        if total_size_gb <= CACHE_MAX_SIZE_GB:
            break

    # Save updated metadata
    with open(cache_metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
```

#### Cache Metadata êµ¬ì¡°

```json
// /tmp/datasets/.cache_metadata.json
{
  "snap_2b2fca921e88_1bb25f37": {
    "snapshot_id": "snap_2b2fca921e88",
    "dataset_id": "ds_c75023ca76d7448b",
    "dataset_version_hash": "1bb25f372b040280...",
    "created_at": "2025-12-02T08:00:00",
    "last_accessed": "2025-12-02T09:15:00",
    "size_bytes": 1572864000,  // 1.5 GB
    "num_jobs_used": 5
  },
  "snap_abc123def456_3ca92d81": {
    "snapshot_id": "snap_abc123def456",
    "dataset_id": "ds_c75023ca76d7448b",
    "dataset_version_hash": "3ca92d81e5f12abc...",
    "created_at": "2025-12-02T10:00:00",
    "last_accessed": "2025-12-02T10:05:00",
    "size_bytes": 1610612736,  // 1.6 GB
    "num_jobs_used": 1
  }
}
```

#### Backend Integration

**1. training.py: Pass hash to workflow**
```python
# training.py: start_training_job()

snapshot = db.query(DatasetSnapshot).filter(
    DatasetSnapshot.id == job.snapshot_id
).first()

await temporal_client.start_workflow(
    TrainingWorkflow.run,
    args=[{
        'job_id': job_id,
        'snapshot_id': snapshot.id,
        'dataset_id': snapshot.dataset_id,
        'dataset_version_hash': snapshot.dataset_version_hash,  # â† ì¶”ê°€
        'storage_path': snapshot.storage_path
    }],
    ...
)
```

**2. training_workflow.py: Forward to activity**
```python
# training_workflow.py: execute_training activity

training_result = await workflow.execute_activity(
    "execute_training",
    {
        'job_id': params['job_id'],
        'snapshot_id': params['snapshot_id'],
        'dataset_id': params['dataset_id'],
        'dataset_version_hash': params['dataset_version_hash'],  # â† ì¶”ê°€
        ...
    },
    ...
)
```

**3. subprocess_manager.py: Add env var**
```python
# subprocess_manager.py: start_training()

env = {
    'JOB_ID': str(job_id),
    'SNAPSHOT_ID': snapshot_id,
    'DATASET_ID': dataset_id,
    'DATASET_VERSION_HASH': dataset_version_hash,  # â† ì¶”ê°€
    ...
}
```

**4. train.py: Use caching**
```python
# train.py

snapshot_id = os.getenv('SNAPSHOT_ID')
dataset_id = os.getenv('DATASET_ID')
dataset_version_hash = os.getenv('DATASET_VERSION_HASH')

# Use caching
local_dataset_dir = sdk.download_dataset_with_cache(
    snapshot_id=snapshot_id,
    dataset_id=dataset_id,
    dataset_version_hash=dataset_version_hash,
    dest_dir=working_dir
)
```

#### ì‘ì—… í•­ëª©

- [ ] `download_dataset_with_cache()` ë©”ì„œë“œ êµ¬í˜„
- [ ] `_verify_cache_integrity()` - Hash-based verification
- [ ] `_link_to_cache()` - Symlink creation
- [ ] `_update_cache_metadata()` - Metadata management
- [ ] `_enforce_cache_size_limit()` - LRU eviction
- [ ] Cache metadata JSON íŒŒì¼ ê´€ë¦¬
- [ ] Backend integration (hash ì „ë‹¬ ê²½ë¡œ)
- [ ] Lock file for race condition (`/tmp/datasets/.lock_{cache_key}`)

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

```python
def test_cache_hit_miss():
    # Job 91: First run
    result = sdk.download_dataset_with_cache(
        snapshot_id="snap_2b2fca921e88",
        dataset_id="ds_c75023ca76d7448b",
        dataset_version_hash="1bb25f372b040280...",
        dest_dir="/tmp/training/91"
    )
    assert result.endswith("/tmp/training/91/dataset")
    assert cache_miss == True

    # Job 92: Same dataset
    result = sdk.download_dataset_with_cache(
        snapshot_id="snap_2b2fca921e88",
        dataset_id="ds_c75023ca76d7448b",
        dataset_version_hash="1bb25f372b040280...",
        dest_dir="/tmp/training/92"
    )
    assert cache_hit == True
    assert link_time < 2  # seconds

def test_hash_mismatch_detection():
    # Corrupt cache
    cache_dir = Path("/tmp/datasets/snap_2b2fca921e88_1bb25f37")
    (cache_dir / "annotations_detection.json").write_text("corrupted")

    # Should detect corruption and re-download
    result = sdk.download_dataset_with_cache(...)
    assert re_downloaded == True
```

---

### 12.9.2 Annotation ê¸°ë°˜ ì„ íƒì  ë‹¤ìš´ë¡œë“œ

**í˜„ì¬ ë¬¸ì œ**:
```python
# trainer_sdk.py:811-825
def download_dataset(self, dataset_id: str, dest_dir: str) -> str:
    prefix = f"datasets/{dataset_id}/"
    local_dir = self.external_storage.download_directory(prefix, dest_dir)
    return local_dir
```

**ë¬¸ì œì **:
- MVTec-AD: 163 labeled images vs 1000+ total images
- 3ë¶„ ë‹¤ìš´ë¡œë“œ â†’ 30ì´ˆë©´ ì¶©ë¶„ (6ë°° ë¹ ë¦„)

**í•´ê²°**:

```python
def download_dataset_selective(self, dataset_id: str, dest_dir: str) -> str:
    """
    Download only images listed in annotations.

    Flow:
    1. Download annotations_detection.json first
    2. Parse and extract image file_name list
    3. Download only those images (parallel)
    4. Return dataset directory
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed

    # Step 1: Download annotation file
    annotation_key = f"datasets/{dataset_id}/annotations_detection.json"
    annotation_local_path = Path(dest_dir) / "annotations_detection.json"
    annotation_local_path.parent.mkdir(parents=True, exist_ok=True)

    self.external_storage.download_file(
        annotation_key,
        str(annotation_local_path)
    )

    # Step 2: Parse annotation
    with open(annotation_local_path) as f:
        data = json.load(f)

    images_to_download = []
    for img in data['images']:
        images_to_download.append(img['file_name'])

    logger.info(f"Found {len(images_to_download)} images to download")

    # Step 3: Download required images only
    storage_info = data.get('storage_info', {})
    image_root = storage_info.get('image_root', f'datasets/{dataset_id}/images/')

    # Parallel download
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = []

        for file_name in images_to_download:
            s3_key = f"{image_root}{file_name}"
            local_path = Path(dest_dir) / file_name
            local_path.parent.mkdir(parents=True, exist_ok=True)

            future = executor.submit(
                self._download_single_file,
                s3_key,
                str(local_path)
            )
            futures.append((file_name, future))

        # Wait for completion with progress
        completed = 0
        for file_name, future in futures:
            try:
                future.result()
                completed += 1
                if completed % 10 == 0:
                    logger.info(f"Downloaded {completed}/{len(images_to_download)} images")
            except Exception as e:
                logger.error(f"Failed to download {file_name}: {e}")
                raise

    logger.info(f"âœ… Downloaded {len(images_to_download)} images")
    return dest_dir


def _download_single_file(self, s3_key: str, local_path: str):
    """Download single file from S3"""
    self.external_storage.download_file(s3_key, local_path)
```

#### ì‘ì—… í•­ëª©

- [ ] `download_dataset_selective()` êµ¬í˜„
- [ ] `_download_single_file()` helper ë©”ì„œë“œ
- [ ] ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
- [ ] Progress logging (downloaded X / Y images)
- [ ] Error handling (partial download ë³µêµ¬)
- [ ] Negative sample ì²˜ë¦¬ ê²€ì¦ (ì´ë¯¸ ì˜¬ë°”ë¦„, í…ŒìŠ¤íŠ¸ë§Œ í•„ìš”)

---

### 12.9.3 Completed/Failed Job Restart ê¸°ëŠ¥

**í˜„ì¬ ë¬¸ì œ**:
```python
# training.py:511-515
if job.status != "pending":
    raise HTTPException(
        status_code=400,
        detail=f"Cannot start job with status '{job.status}'",
    )
```

**í•´ê²°**:
```python
# training.py:start_training_job()

# Allow restart for completed/failed jobs
if job.status not in ["pending", "completed", "failed"]:
    raise HTTPException(
        status_code=400,
        detail=f"Cannot start job with status '{job.status}'. Only pending, completed, or failed jobs can be started.",
    )

# If completed/failed, reset to pending
if job.status in ["completed", "failed"]:
    logger.info(f"[JOB {job_id}] Restarting {job.status} job, resetting to pending")

    job.status = "pending"
    job.started_at = None
    job.completed_at = None
    job.error_message = None

    # Optional: Clear history if requested
    clear_history = request.query_params.get('clear_history', 'false').lower() == 'true'

    if clear_history:
        # Clear existing metrics/logs for fresh start
        # Implementation depends on metrics storage strategy
        pass

    db.commit()
    db.refresh(job)
```

#### ì‘ì—… í•­ëª©

- [ ] Status ì²´í¬ ë¡œì§ ìˆ˜ì •
- [ ] Job ìƒíƒœ ë¦¬ì…‹ ë¡œì§
- [ ] `clear_history` ì˜µì…˜ êµ¬í˜„ (ì„ íƒì )
- [ ] Frontend: Restart ë²„íŠ¼ ì¶”ê°€
- [ ] í…ŒìŠ¤íŠ¸: Restart í›„ ì •ìƒ ì‹¤í–‰ í™•ì¸

---

## êµ¬í˜„ ê³„íš

### Day 1 (8ì‹œê°„)

**ì˜¤ì „ (4ì‹œê°„): ìºì‹± ì¸í”„ë¼**
- [ ] `download_dataset_with_cache()` êµ¬í˜„
- [ ] `_verify_cache_integrity()` êµ¬í˜„
- [ ] `_link_to_cache()` êµ¬í˜„
- [ ] Cache metadata ê´€ë¦¬

**ì˜¤í›„ (4ì‹œê°„): Backend Integration**
- [ ] Backend hash ì „ë‹¬ ê²½ë¡œ êµ¬í˜„
- [ ] Lock file for race condition
- [ ] LRU eviction êµ¬í˜„
- [ ] ì´ˆê¸° í…ŒìŠ¤íŠ¸

### Day 2 (4ì‹œê°„)

**ì˜¤ì „ (2ì‹œê°„): ì„ íƒì  ë‹¤ìš´ë¡œë“œ**
- [ ] `download_dataset_selective()` êµ¬í˜„
- [ ] ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ìµœì í™”

**ì˜¤í›„ (2ì‹œê°„): Restart + í…ŒìŠ¤íŠ¸**
- [ ] Job restart ê¸°ëŠ¥ êµ¬í˜„
- [ ] Integration testing
- [ ] Documentation

---

## ì„±ê³µ ê¸°ì¤€

### ê¸°ëŠ¥

- [ ] Cache hit ì‹œ < 1ì´ˆì— dataset ì¤€ë¹„
- [ ] Cache miss ì‹œ selective downloadë¡œ 6ë°° ë¹ ë¦„ (3ë¶„ â†’ 30ì´ˆ)
- [ ] Hash verificationìœ¼ë¡œ cache corruption ê°ì§€
- [ ] LRU evictionìœ¼ë¡œ disk space ìë™ ê´€ë¦¬
- [ ] Completed/Failed job restart ê°€ëŠ¥

### ì„±ëŠ¥

```
Before (10 Jobs, ê°™ì€ dataset):
  - ì´ ì‹œê°„: 30ë¶„
  - ì´ ë‹¤ìš´ë¡œë“œ: 15GB
  - ë””ìŠ¤í¬ ì‚¬ìš©: 15GB

After:
  - ì´ ì‹œê°„: ~3ë¶„ (90% ì ˆê°)
  - ì´ ë‹¤ìš´ë¡œë“œ: 1.5GB (90% ì ˆê°)
  - ë””ìŠ¤í¬ ì‚¬ìš©: 1.5GB (90% ì ˆê°)
```

### ì•ˆì •ì„±

- [ ] Hash collision ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬
- [ ] ë™ì‹œ ë‹¤ìš´ë¡œë“œ race condition ë°©ì§€
- [ ] Cache corruption ìë™ ë³µêµ¬
- [ ] Symlink ë¬¸ì œ ì—†ìŒ (Windows/Linux)

---

## ëª¨ë‹ˆí„°ë§

### Prometheus ë©”íŠ¸ë¦­

```python
# Cache metrics
cache_hit_total = Counter('dataset_cache_hit_total')
cache_miss_total = Counter('dataset_cache_miss_total')
cache_size_bytes = Gauge('dataset_cache_size_bytes')
cache_eviction_total = Counter('dataset_cache_eviction_total')

# Download metrics
dataset_download_duration_seconds = Histogram('dataset_download_duration_seconds')
dataset_download_files_total = Counter('dataset_download_files_total')
```

### Grafana ëŒ€ì‹œë³´ë“œ

- Cache hit rate (%)
- Average download time (cache hit vs miss)
- Cache size over time
- Eviction events

---

## References

- [caching_strategy.md](../../../debug/caching_strategy.md) - ìƒì„¸ ì„¤ê³„
- [problems_analysis.md](../../../debug/problems_analysis.md) - ë¬¸ì œ ë¶„ì„
- [PHASE_12_6_SNAPSHOT.md](PHASE_12_6_SNAPSHOT.md) - Snapshot ì‹œìŠ¤í…œ
- [snapshot_service.py](../../../platform/backend/app/services/snapshot_service.py) - Hash ê³„ì‚°
