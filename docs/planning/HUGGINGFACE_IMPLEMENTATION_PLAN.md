# HuggingFace Transformers Implementation Plan (Week 5-6)

**Document Version:** 2.0
**Created:** 2025-10-30
**Updated:** 2025-10-30
**Status:** Implementation Plan

---

## Executive Summary

Week 5-6ì— HuggingFace Transformers í”„ë ˆì„ì›Œí¬ë¥¼ ì¶”ê°€í•˜ì—¬ í”Œë«í¼ì˜ task ë‹¤ì–‘ì„±ì„ í™•ì¥í•©ë‹ˆë‹¤.

**ëª©í‘œ:**
- âœ… 4ê°œì˜ ë‹¤ì–‘í•œ task type ì§€ì› (Classification, Detection, Segmentation, Super-Resolution)
- âœ… SOTA ìµœì‹  ëª¨ë¸ ì ìš© (D-FINE, EoMT - CVPR 2025)
- âœ… Docker ê²©ë¦¬ í™˜ê²½ êµ¬ì¶•
- âœ… TransformersAdapter êµ¬í˜„

**ëª¨ë¸ ì„ ì • (4ê°œ):**
1. **ViT** - Image Classification (ê¸°ë³¸)
2. **D-FINE** - Object Detection (SOTA, 2025-04 ì¶”ê°€)
3. **EoMT** - Semantic Segmentation (CVPR 2025 Highlight)
4. **Swin2SR** - Super-Resolution (Image Restoration)

---

## ëª¨ë¸ ìƒì„¸ ê²€í† 

### 1. ViT (Vision Transformer) - Image Classification

**ê¸°ë³¸ ì •ë³´:**
- **Paper:** "An Image is Worth 16x16 Words" (ICLR 2021)
- **Model ID:** `google/vit-base-patch16-224`
- **Parameters:** 86M
- **Task:** Image Classification
- **Input Size:** 224Ã—224
- **HuggingFace Support:** âœ… Full (AutoModelForImageClassification)

**ì£¼ìš” íŠ¹ì§•:**
- ì´ë¯¸ì§€ë¥¼ 16Ã—16 íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ì–´ transformerì— ì…ë ¥
- ImageNet-21k pretrained â†’ ImageNet-1k fine-tuned
- Attention ê¸°ë°˜ global context ëª¨ë¸ë§

**êµ¬í˜„ ë‚œì´ë„:** â­ Low (ê°€ì¥ ê¸°ë³¸ì ì¸ ëª¨ë¸)

**API ì‚¬ìš©:**
```python
from transformers import ViTImageProcessor, ViTForImageClassification

processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
model = ViTForImageClassification.from_pretrained(
    'google/vit-base-patch16-224',
    num_labels=num_classes
)
```

**í•™ìŠµ ë°©ë²•:**
- HuggingFace Trainer API ì‚¬ìš©
- ImageFolder â†’ HF Dataset ë³€í™˜
- Standard cross-entropy loss

**ì˜ˆìƒ í•™ìŠµ ì‹œê°„:** ~30 min (10 epochs, sample dataset)

---

### 2. D-FINE - Object Detection

**ê¸°ë³¸ ì •ë³´:**
- **Paper:** "D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement"
- **Model ID:** `ustc-community/dfine_x_coco`
- **Release:** 2024-10-17, HF ì¶”ê°€: 2025-04-29
- **Task:** Object Detection
- **Input Size:** 640Ã—640
- **HuggingFace Support:** âœ… Full (DFineForObjectDetection)

**ì£¼ìš” íŠ¹ì§•:**
- DETR ê¸°ë°˜ì˜ real-time detector
- Fine-grained Distribution Refinement (FDR)
- Global Optimal Localization Self-Distillation (GO-LSD)
- **Performance:** 57.1% / 59.3% AP (SOTA in real-time detection)

**êµ¬í˜„ ë‚œì´ë„:** â­â­ Medium (DETR êµ¬ì¡°, bbox regression)

**API ì‚¬ìš©:**
```python
from transformers import AutoImageProcessor, DFineForObjectDetection

processor = AutoImageProcessor.from_pretrained("ustc-community/dfine_x_coco")
model = DFineForObjectDetection.from_pretrained("ustc-community/dfine_x_coco")
```

**í•™ìŠµ ë°©ë²•:**
- Dataset: COCO format
- Loss: FDR loss + GO-LSD distillation
- Metrics: mAP50, mAP50-95

**Dataset Format ìš”êµ¬ì‚¬í•­:**
- COCO JSON annotations
- ImageFolder â†’ COCO converter í•„ìš”

**ì˜ˆìƒ í•™ìŠµ ì‹œê°„:** ~2 hours (50 epochs, detection dataset)

---

### 3. EoMT (Encoder-only Mask Transformer) - Segmentation

**ê¸°ë³¸ ì •ë³´:**
- **Paper:** "Your ViT is Secretly an Image Segmentation Model" (CVPR 2025 Highlight)
- **Model Range:** 0.3B ~ 7B parameters
- **Task:** Semantic Segmentation, Panoptic Segmentation
- **Input Size:** Flexible (ViT-based)
- **HuggingFace Support:** âœ… Main branch (transformers>=4.48)

**ì£¼ìš” íŠ¹ì§•:**
- **í˜ì‹ :** Task-specific components ì—†ì´ ìˆœìˆ˜ ViTë¡œ segmentation
- **ì„±ëŠ¥:** SOTAì™€ ë™ë“±í•˜ë©´ì„œ 4x ë¹ ë¦„ (ViT-L)
- Encoder-only êµ¬ì¡°ë¡œ ë‹¨ìˆœí™”
- Large-scale pretrainingìœ¼ë¡œ inductive bias í•™ìŠµ

**êµ¬í˜„ ë‚œì´ë„:** â­â­â­ Medium-High (segmentation output, mask processing)

**API ì‚¬ìš©:**
```python
from transformers import EoMTModel, EoMTForSemanticSegmentation

model = EoMTForSemanticSegmentation.from_pretrained(
    "tue-mps/eomt-vit-large"
)
```

**í•™ìŠµ ë°©ë²•:**
- Dataset: Semantic segmentation masks
- Output: Per-pixel class predictions
- Metrics: mIoU, pixel accuracy

**Dataset Format ìš”êµ¬ì‚¬í•­:**
- Images + segmentation masks (PNG)
- Class labels per pixel

**ì˜ˆìƒ í•™ìŠµ ì‹œê°„:** ~3 hours (100 epochs, segmentation dataset)

**ë„ì „ ê³¼ì œ:**
- Mask í˜•íƒœì˜ label ì²˜ë¦¬
- Large model size (7Bê¹Œì§€)
- Post-processing (mask refinement)

---

### 4. Swin2SR - Super-Resolution

**ê¸°ë³¸ ì •ë³´:**
- **Paper:** "Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution" (ECCV 2022)
- **Model ID:** `caidas/swin2sr-classicalsr-x2-64`
- **Parameters:** 11.9M
- **Task:** Super-Resolution, Image Restoration
- **Upscale Factor:** x2, x3, x4, x8
- **HuggingFace Support:** âœ… Full (Swin2SRModel)

**ì£¼ìš” íŠ¹ì§•:**
- SwinTransformer v2 ê¸°ë°˜
- 3ê°€ì§€ task ì§€ì›:
  1. Image Super-Resolution (2x/3x/4x/8x)
  2. JPEG Compression Artifact Removal
  3. Image Denoising
- Training stability ê°œì„  (SwinV2 layers)

**êµ¬í˜„ ë‚œì´ë„:** â­â­ Medium (ìƒˆë¡œìš´ task typeì´ì§€ë§Œ êµ¬ì¡° ë‹¨ìˆœ)

**API ì‚¬ìš©:**
```python
from transformers import Swin2SRModel, Swin2SRImageProcessor

processor = Swin2SRImageProcessor.from_pretrained("caidas/swin2sr-classicalsr-x2-64")
model = Swin2SRModel.from_pretrained("caidas/swin2sr-classicalsr-x2-64")
```

**í•™ìŠµ ë°©ë²•:**
- Dataset: HR-LR image pairs
- Loss: L1 loss (pixel-wise)
- Metrics: PSNR, SSIM

**Dataset Format ìš”êµ¬ì‚¬í•­:**
- High-resolution images (target)
- Low-resolution images (input) - downscaled
- Paired structure: `HR/`, `LR_x2/`

**ì˜ˆìƒ í•™ìŠµ ì‹œê°„:** ~4 hours (500 epochs, SR dataset)

**ìƒˆë¡œìš´ Task Type ì¶”ê°€ í•„ìš”:**
```python
# platform_sdk/base.py
class TaskType(Enum):
    ...
    SUPER_RESOLUTION = "super_resolution"  # ìƒˆë¡œ ì¶”ê°€
```

---

## êµ¬í˜„ ìš°ì„ ìˆœìœ„ ë° ì¼ì •

### Week 5 (Day 1-7): ê¸°ë°˜ êµ¬ì¶• + ViT + Swin2SR

**Day 1-2: ê¸°ë°˜ êµ¬ì¶•**
- [ ] `requirements-huggingface.txt` ì‘ì„±
- [ ] `huggingface_models.py` ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‘ì„± (4ê°œ ëª¨ë¸)
- [ ] `TaskType.SUPER_RESOLUTION` ì¶”ê°€
- [ ] Dockerfile.huggingface ì‘ì„±
- [ ] Docker ì´ë¯¸ì§€ ë¹Œë“œ í…ŒìŠ¤íŠ¸

**Day 3-4: ViT (Classification)**
- [ ] `TransformersAdapter` ê¸°ë³¸ êµ¬ì¡° ì‘ì„±
- [ ] Classification task êµ¬í˜„
- [ ] ImageFolder â†’ HF Dataset ë³€í™˜
- [ ] HF Trainer API í†µí•©
- [ ] ViT í•™ìŠµ í…ŒìŠ¤íŠ¸ (sample_dataset)

**Day 5-7: Swin2SR (Super-Resolution)**
- [ ] Super-Resolution task êµ¬í˜„
- [ ] HR-LR dataset ë¡œë” ì‘ì„±
- [ ] PSNR/SSIM metric ê³„ì‚°
- [ ] Swin2SR í•™ìŠµ í…ŒìŠ¤íŠ¸
- [ ] Frontend SR task UI ì¶”ê°€

**Milestone:** ViT + Swin2SR ê²€ì¦ ì™„ë£Œ

---

### Week 6 (Day 8-12): D-FINE + EoMT + í†µí•©

**Day 8-9: D-FINE (Detection)**
- [ ] Detection task êµ¬í˜„
- [ ] COCO format dataset ì§€ì›
- [ ] Bounding box visualization
- [ ] mAP metric ê³„ì‚°
- [ ] D-FINE í•™ìŠµ í…ŒìŠ¤íŠ¸

**Day 10-11: EoMT (Segmentation)**
- [ ] Segmentation task êµ¬í˜„
- [ ] Mask dataset ë¡œë” ì‘ì„±
- [ ] mIoU metric ê³„ì‚°
- [ ] Mask visualization
- [ ] EoMT í•™ìŠµ í…ŒìŠ¤íŠ¸

**Day 12: í†µí•© ë° ê²€ì¦**
- [ ] Backend API í†µí•©
- [ ] Frontend 4ê°œ task ì§€ì›
- [ ] Docker í™˜ê²½ ì „ì²´ í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸

**Milestone:** 4ê°œ ëª¨ë¸ ì „ì²´ ê²€ì¦ ì™„ë£Œ

---

## ê¸°ìˆ  ì•„í‚¤í…ì²˜

### 1. Requirements (requirements-huggingface.txt)

```txt
# Base PyTorch (from requirements-base.txt)
# torch==2.1.0
# torchvision==0.16.0

# HuggingFace Core
transformers==4.48.0
accelerate==0.25.0
datasets==2.16.0

# Vision Processing
opencv-python==4.8.1.78
albumentations==1.3.1

# Evaluation
evaluate==0.4.1
scikit-learn==1.3.2

# Metrics
scikit-image==0.22.0  # PSNR, SSIM for SR
pycocotools==2.0.7    # COCO evaluation for detection
```

**Size Estimate:** ~2.5GB (transformers ë§¤ìš° í¼)

---

### 2. Model Registry (huggingface_models.py)

```python
"""HuggingFace Transformers model registry."""

from typing import Dict, Any

HUGGINGFACE_MODEL_REGISTRY: Dict[str, Dict[str, Any]] = {

    # ========== Image Classification ==========

    "google/vit-base-patch16-224": {
        "display_name": "Vision Transformer (ViT) Base",
        "description": "Transformer-based image classification - Attention-based global context",
        "framework": "huggingface",
        "task_type": "image_classification",
        "model_id": "google/vit-base-patch16-224",
        "params": "86M",
        "input_size": 224,
        "pretrained_available": True,
        "recommended_batch_size": 32,
        "recommended_lr": 3e-4,
        "tags": ["p1", "transformer", "attention", "imagenet", "2021"],
        "priority": 1,

        "architecture": {
            "type": "Vision Transformer",
            "patch_size": 16,
            "hidden_size": 768,
            "num_layers": 12,
            "num_heads": 12,
        },

        "performance": {
            "imagenet_top1": "81.3%",
            "imagenet_top5": "96.5%",
            "inference_speed": "~50 images/sec (V100)",
        },

        "use_cases": [
            {
                "title": "General Image Classification",
                "description": "Fine-grained classification with global context understanding",
                "dataset": "Custom ImageNet-style dataset",
                "metrics": {
                    "before": "ResNet-50: 76.1% accuracy",
                    "after": "ViT-Base: 81.3% accuracy with attention visualization"
                }
            }
        ]
    },

    # ========== Object Detection ==========

    "ustc-community/dfine_x_coco": {
        "display_name": "D-FINE (Detection Fine-grained)",
        "description": "SOTA real-time detector - Fine-grained bbox refinement (57.1% AP)",
        "framework": "huggingface",
        "task_type": "object_detection",
        "model_id": "ustc-community/dfine_x_coco",
        "params": "67M",
        "input_size": 640,
        "pretrained_available": True,
        "recommended_batch_size": 8,
        "recommended_lr": 1e-4,
        "tags": ["p1", "detection", "detr", "real-time", "sota", "2024"],
        "priority": 1,

        "architecture": {
            "type": "DETR-based",
            "backbone": "ResNet-50",
            "features": ["FDR", "GO-LSD"],
        },

        "performance": {
            "coco_map50": "57.1%",
            "coco_map50-95": "40.8%",
            "inference_speed": "Real-time (>30 FPS)",
        },

        "use_cases": [
            {
                "title": "Precise Object Localization",
                "description": "High-precision bounding box detection for industrial inspection",
                "dataset": "Custom COCO-format dataset",
                "metrics": {
                    "before": "YOLOv8: 50.2% mAP50",
                    "after": "D-FINE: 57.1% mAP50 with fine-grained localization"
                }
            }
        ]
    },

    # ========== Semantic Segmentation ==========

    "tue-mps/eomt-vit-large": {
        "display_name": "EoMT (Encoder-only Mask Transformer)",
        "description": "CVPR 2025 Highlight - Segmentation without task-specific components",
        "framework": "huggingface",
        "task_type": "semantic_segmentation",
        "model_id": "tue-mps/eomt-vit-large",
        "params": "304M",
        "input_size": 518,
        "pretrained_available": True,
        "recommended_batch_size": 4,
        "recommended_lr": 1e-4,
        "tags": ["p1", "segmentation", "vit", "encoder-only", "cvpr2025"],
        "priority": 1,

        "architecture": {
            "type": "Encoder-only ViT",
            "backbone": "ViT-Large",
            "innovation": "No task-specific decoder",
        },

        "performance": {
            "ade20k_miou": "53.0%",
            "inference_speed": "4x faster than Mask2Former",
        },

        "use_cases": [
            {
                "title": "Fast Semantic Segmentation",
                "description": "Efficient pixel-wise classification for autonomous driving",
                "dataset": "Custom segmentation masks",
                "metrics": {
                    "before": "Mask2Former: 50.1% mIoU, 2.5s/image",
                    "after": "EoMT: 53.0% mIoU, 0.6s/image (4x faster)"
                }
            }
        ]
    },

    # ========== Super-Resolution ==========

    "caidas/swin2sr-classicalsr-x2-64": {
        "display_name": "Swin2SR (2x Super-Resolution)",
        "description": "Image restoration and super-resolution - 2x upscaling",
        "framework": "huggingface",
        "task_type": "super_resolution",
        "model_id": "caidas/swin2sr-classicalsr-x2-64",
        "params": "11.9M",
        "input_size": "variable",
        "upscale_factor": 2,
        "pretrained_available": True,
        "recommended_batch_size": 16,
        "recommended_lr": 2e-4,
        "tags": ["p1", "super-resolution", "restoration", "swin", "2022"],
        "priority": 1,

        "architecture": {
            "type": "Swin Transformer V2",
            "window_size": 8,
            "features": ["Residual Swin Transformer Blocks"],
        },

        "performance": {
            "psnr": "33.89 dB (Set5 dataset)",
            "ssim": "0.9195",
            "inference_speed": "~20 images/sec (512x512 â†’ 1024x1024)",
        },

        "use_cases": [
            {
                "title": "Image Quality Enhancement",
                "description": "Upscale low-resolution medical images for better diagnosis",
                "dataset": "HR-LR paired images",
                "metrics": {
                    "before": "Bicubic: 30.1 dB PSNR",
                    "after": "Swin2SR: 33.9 dB PSNR with artifact removal"
                }
            }
        ]
    },
}
```

---

### 3. TransformersAdapter Structure

```python
# mvp/training/adapters/transformers_adapter.py

from transformers import (
    AutoModelForImageClassification,
    AutoImageProcessor,
    Trainer,
    TrainingArguments,
    TrainerCallback,
)

class TransformersAdapter(TrainingAdapter):
    """HuggingFace Transformers adapter for vision tasks."""

    def __init__(self, ...):
        super().__init__(...)
        self.task_type = model_config.task_type
        self.processor = None
        self.trainer = None

    def prepare_model(self):
        """Load model based on task type."""
        if self.task_type == TaskType.IMAGE_CLASSIFICATION:
            self.model = AutoModelForImageClassification.from_pretrained(
                self.model_config.model_name,
                num_labels=self.model_config.num_classes
            )
            self.processor = AutoImageProcessor.from_pretrained(
                self.model_config.model_name
            )

        elif self.task_type == TaskType.OBJECT_DETECTION:
            from transformers import DFineForObjectDetection
            self.model = DFineForObjectDetection.from_pretrained(
                self.model_config.model_name
            )
            self.processor = AutoImageProcessor.from_pretrained(
                self.model_config.model_name
            )

        elif self.task_type == TaskType.SEMANTIC_SEGMENTATION:
            from transformers import EoMTForSemanticSegmentation
            self.model = EoMTForSemanticSegmentation.from_pretrained(
                self.model_config.model_name
            )
            # ... processor setup

        elif self.task_type == TaskType.SUPER_RESOLUTION:
            from transformers import Swin2SRModel
            self.model = Swin2SRModel.from_pretrained(
                self.model_config.model_name
            )
            # ... processor setup

    def prepare_dataset(self):
        """Convert ImageFolder to HF Dataset."""
        from datasets import Dataset, Image

        if self.task_type == TaskType.IMAGE_CLASSIFICATION:
            # ImageFolder â†’ HF Dataset with image processor
            self.train_dataset = self._create_classification_dataset("train")
            self.val_dataset = self._create_classification_dataset("val")

        elif self.task_type == TaskType.OBJECT_DETECTION:
            # COCO format â†’ HF Dataset
            self.train_dataset = self._create_detection_dataset("train")
            self.val_dataset = self._create_detection_dataset("val")

        # ... other tasks

    def train_epoch(self, epoch: int) -> MetricsResult:
        """Train using HF Trainer API."""
        # Configure TrainingArguments
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=1,  # Single epoch
            per_device_train_batch_size=self.training_config.batch_size,
            learning_rate=self.training_config.learning_rate,
            # ...
        )

        # Create Trainer
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.val_dataset,
            callbacks=[MLflowCallback(self.job_id)],  # Custom callback
        )

        # Train
        self.trainer.train()

        # Extract metrics
        metrics = self.trainer.state.log_history[-1]
        return self._convert_metrics(metrics, epoch)

    def validate(self, epoch: int) -> MetricsResult:
        """Evaluate using HF Trainer."""
        eval_results = self.trainer.evaluate()
        return self._convert_metrics(eval_results, epoch)

    def _convert_metrics(self, hf_metrics: dict, epoch: int) -> MetricsResult:
        """Convert HF metrics to platform MetricsResult."""
        # Task-specific metric conversion
        if self.task_type == TaskType.IMAGE_CLASSIFICATION:
            return MetricsResult(
                epoch=epoch,
                step=0,
                train_loss=hf_metrics.get('loss', 0.0),
                val_loss=hf_metrics.get('eval_loss', 0.0),
                metrics={
                    'accuracy': hf_metrics.get('eval_accuracy', 0.0),
                }
            )
        # ... other tasks
```

---

### 4. Dockerfile.huggingface

```dockerfile
# mvp/docker/Dockerfile.huggingface
FROM vision-platform-base:latest

# Set working directory
WORKDIR /workspace

# Copy HuggingFace requirements
COPY training/requirements/requirements-base.txt /tmp/
COPY training/requirements/requirements-huggingface.txt /tmp/

# Install HuggingFace dependencies
RUN pip install --no-cache-dir -r /tmp/requirements-huggingface.txt && \
    rm /tmp/requirements-huggingface.txt /tmp/requirements-base.txt

# Copy HuggingFace adapter
COPY training/adapters/__init__.py /opt/vision-platform/adapters/
COPY training/adapters/transformers_adapter.py /opt/vision-platform/adapters/

# Copy HuggingFace model registry
COPY training/model_registry/__init__.py /opt/vision-platform/model_registry/
COPY training/model_registry/huggingface_models.py /opt/vision-platform/model_registry/

# Copy validators
COPY training/validators/ /opt/vision-platform/validators/

# Verify installation
RUN python -c "import transformers; print(f'transformers: {transformers.__version__}')"

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import transformers; print('OK')" || exit 1
```

**Image Size Estimate:** ~12GB (transformers + models ë§¤ìš° í¼)

---

## ì£¼ìš” êµ¬í˜„ ë„ì „ ê³¼ì œ

### 1. HuggingFace Trainer API í†µí•©

**Challenge:** PyTorch training loopê³¼ ì™„ì „íˆ ë‹¤ë¥¸ êµ¬ì¡°

**Solution:**
- TrainerCallbackìœ¼ë¡œ MLflow ë¡œê¹… ì—°ê²°
- `trainer.state.log_history`ì—ì„œ metrics ì¶”ì¶œ
- Custom callbackìœ¼ë¡œ epochë§ˆë‹¤ DB ì €ì¥

```python
class MLflowCallback(TrainerCallback):
    """Custom callback to log metrics to MLflow."""

    def __init__(self, job_id):
        self.job_id = job_id

    def on_log(self, args, state, control, logs=None, **kwargs):
        """Called when metrics are logged."""
        # logsì—ì„œ metrics ì¶”ì¶œ â†’ MLflow ë¡œê¹…
        mlflow.log_metrics(logs, step=state.global_step)
```

---

### 2. Dataset ë³€í™˜

**Challenge:** ImageFolder â†’ HuggingFace Dataset ë³€í™˜

**Solution:**
```python
from datasets import Dataset, Image as HFImage
from PIL import Image
import os

def _create_classification_dataset(self, split: str):
    """Convert ImageFolder to HF Dataset."""
    data_dir = os.path.join(self.dataset_config.dataset_path, split)

    # Collect image paths and labels
    images = []
    labels = []
    for label_name in os.listdir(data_dir):
        label_dir = os.path.join(data_dir, label_name)
        for img_name in os.listdir(label_dir):
            images.append(os.path.join(label_dir, img_name))
            labels.append(label_name)

    # Create HF Dataset
    dataset = Dataset.from_dict({
        'image': images,
        'label': labels
    })

    # Cast image column to HF Image type
    dataset = dataset.cast_column('image', HFImage())

    # Apply image processor
    def preprocess(examples):
        images = [img.convert("RGB") for img in examples['image']]
        inputs = self.processor(images, return_tensors="pt")
        inputs['labels'] = examples['label']
        return inputs

    dataset = dataset.map(preprocess, batched=True)
    return dataset
```

---

### 3. Super-Resolution Dataset

**Challenge:** HR-LR paired images ë¡œë”©

**Dataset Structure:**
```
dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ HR/
â”‚   â”‚   â”œâ”€â”€ 0001.png (1024Ã—1024)
â”‚   â”‚   â””â”€â”€ 0002.png
â”‚   â””â”€â”€ LR_x2/
â”‚       â”œâ”€â”€ 0001.png (512Ã—512)
â”‚       â””â”€â”€ 0002.png
â””â”€â”€ val/
    â”œâ”€â”€ HR/
    â””â”€â”€ LR_x2/
```

**Solution:**
```python
def _create_sr_dataset(self, split: str):
    """Create SR dataset with HR-LR pairs."""
    hr_dir = os.path.join(self.dataset_config.dataset_path, split, "HR")
    lr_dir = os.path.join(self.dataset_config.dataset_path, split, f"LR_x{self.upscale_factor}")

    # Collect paired images
    hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir)])
    lr_images = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir)])

    dataset = Dataset.from_dict({
        'lr_image': lr_images,
        'hr_image': hr_images
    })

    dataset = dataset.cast_column('lr_image', HFImage())
    dataset = dataset.cast_column('hr_image', HFImage())

    return dataset
```

---

### 4. Detection Dataset (COCO Format)

**Challenge:** COCO JSON annotations â†’ HF Dataset

**Solution:**
```python
from pycocotools.coco import COCO

def _create_detection_dataset(self, split: str):
    """Convert COCO format to HF Dataset."""
    image_dir = os.path.join(self.dataset_config.dataset_path, split)
    ann_file = os.path.join(self.dataset_config.dataset_path, f"{split}.json")

    coco = COCO(ann_file)
    image_ids = coco.getImgIds()

    images = []
    annotations = []

    for img_id in image_ids:
        img_info = coco.loadImgs(img_id)[0]
        ann_ids = coco.getAnnIds(imgIds=img_id)
        anns = coco.loadAnns(ann_ids)

        images.append(os.path.join(image_dir, img_info['file_name']))
        annotations.append({
            'boxes': [ann['bbox'] for ann in anns],
            'labels': [ann['category_id'] for ann in anns]
        })

    dataset = Dataset.from_dict({
        'image': images,
        'annotations': annotations
    })

    return dataset
```

---

## TaskType ì¶”ê°€

**platform_sdk/base.py ì—…ë°ì´íŠ¸:**
```python
class TaskType(Enum):
    """Supported task types."""
    # Vision
    IMAGE_CLASSIFICATION = "image_classification"
    OBJECT_DETECTION = "object_detection"
    INSTANCE_SEGMENTATION = "instance_segmentation"
    SEMANTIC_SEGMENTATION = "semantic_segmentation"
    POSE_ESTIMATION = "pose_estimation"
    DEPTH_ESTIMATION = "depth_estimation"
    SUPER_RESOLUTION = "super_resolution"  # ìƒˆë¡œ ì¶”ê°€ âœ…

    # Vision-Language
    IMAGE_CAPTIONING = "image_captioning"
    VISUAL_QA = "visual_qa"
    OCR = "ocr"
    DOCUMENT_UNDERSTANDING = "document_understanding"

    # Zero-Shot
    ZERO_SHOT_CLASSIFICATION = "zero_shot_classification"
    ZERO_SHOT_DETECTION = "zero_shot_detection"
```

**Metrics ì •ì˜ ì¶”ê°€:**
```python
TASK_PRIMARY_METRICS = {
    TaskType.IMAGE_CLASSIFICATION: 'accuracy',
    TaskType.OBJECT_DETECTION: 'mAP50',
    TaskType.SEMANTIC_SEGMENTATION: 'miou',
    TaskType.SUPER_RESOLUTION: 'psnr',  # ìƒˆë¡œ ì¶”ê°€ âœ…
}

TASK_STANDARD_METRICS = {
    # ... existing ...

    TaskType.SUPER_RESOLUTION: {  # ìƒˆë¡œ ì¶”ê°€ âœ…
        'psnr': MetricDefinition(
            label='PSNR',
            format='float',
            higher_is_better=True,
            description='Peak Signal-to-Noise Ratio (dB)'
        ),
        'ssim': MetricDefinition(
            label='SSIM',
            format='percent',
            higher_is_better=True,
            description='Structural Similarity Index'
        ),
        'lpips': MetricDefinition(
            label='LPIPS',
            format='float',
            higher_is_better=False,
            description='Learned Perceptual Image Patch Similarity'
        ),
    },
}
```

---

## Backend í†µí•©

### TrainingManager ì—…ë°ì´íŠ¸

```python
# mvp/backend/app/utils/training_manager.py

IMAGE_MAP = {
    "timm": "vision-platform-timm:latest",
    "ultralytics": "vision-platform-ultralytics:latest",
    "huggingface": "vision-platform-huggingface:latest",  # ì¶”ê°€ âœ…
}
```

### Adapter Registry ì—…ë°ì´íŠ¸

```python
# mvp/training/adapters/__init__.py

try:
    from .transformers_adapter import TransformersAdapter
except ImportError:
    TransformersAdapter = None

# Adapter registry
ADAPTER_REGISTRY = {}
if TimmAdapter is not None:
    ADAPTER_REGISTRY['timm'] = TimmAdapter
if UltralyticsAdapter is not None:
    ADAPTER_REGISTRY['ultralytics'] = UltralyticsAdapter
if TransformersAdapter is not None:
    ADAPTER_REGISTRY['huggingface'] = TransformersAdapter  # ì¶”ê°€ âœ…
```

---

## Frontend ì§€ì›

### Task Type UI ì¶”ê°€

**TrainingConfigPanel.tsx ì—…ë°ì´íŠ¸:**
```typescript
const TASK_TYPES = [
  { value: 'image_classification', label: 'ì´ë¯¸ì§€ ë¶„ë¥˜', icon: 'ğŸ–¼ï¸' },
  { value: 'object_detection', label: 'ê°ì²´ ê²€ì¶œ', icon: 'ğŸ¯' },
  { value: 'semantic_segmentation', label: 'ì‹œë§¨í‹± ë¶„í• ', icon: 'ğŸ—ºï¸' },
  { value: 'super_resolution', label: 'ì´ˆí•´ìƒë„', icon: 'ğŸ”' },  // ì¶”ê°€ âœ…
];
```

---

## í…ŒìŠ¤íŠ¸ ê³„íš

### 1. Unit Tests

**ViT Classification:**
```python
def test_vit_classification():
    adapter = TransformersAdapter(
        model_config=ModelConfig(
            framework='huggingface',
            task_type=TaskType.IMAGE_CLASSIFICATION,
            model_name='google/vit-base-patch16-224',
            num_classes=2
        ),
        # ...
    )
    adapter.prepare_model()
    adapter.prepare_dataset()
    metrics = adapter.train_epoch(1)
    assert metrics.metrics['accuracy'] > 0
```

**Swin2SR Super-Resolution:**
```python
def test_swin2sr():
    adapter = TransformersAdapter(
        model_config=ModelConfig(
            framework='huggingface',
            task_type=TaskType.SUPER_RESOLUTION,
            model_name='caidas/swin2sr-classicalsr-x2-64',
        ),
        # ...
    )
    adapter.prepare_model()
    metrics = adapter.train_epoch(1)
    assert metrics.metrics['psnr'] > 0
```

### 2. Integration Tests

**Docker Environment:**
```bash
# Build image
cd mvp/docker
./build.sh

# Test ViT training
docker run --rm \
  -v $(pwd)/data/datasets/sample_dataset:/workspace/dataset:ro \
  -v $(pwd)/data/outputs:/workspace/output:rw \
  vision-platform-huggingface:latest \
  python /opt/vision-platform/train.py \
    --framework huggingface \
    --task_type image_classification \
    --model_name google/vit-base-patch16-224 \
    --dataset_path /workspace/dataset \
    --epochs 2 \
    --job_id 200
```

### 3. E2E Tests

**Frontend â†’ Backend â†’ Training:**
1. Frontendì—ì„œ ViT ëª¨ë¸ ì„ íƒ
2. í•™ìŠµ ì‹œì‘ (sample_dataset)
3. ì‹¤ì‹œê°„ metrics í™•ì¸
4. í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
5. Inference í…ŒìŠ¤íŠ¸

---

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì˜ˆìƒ)

| Model | Task | Dataset | Epochs | Time | Metric |
|-------|------|---------|--------|------|--------|
| ViT | Classification | sample (64 imgs) | 10 | 30 min | 85% acc |
| D-FINE | Detection | COCO subset (500 imgs) | 50 | 2 hours | 50% mAP50 |
| EoMT | Segmentation | ADE20K subset | 100 | 3 hours | 45% mIoU |
| Swin2SR | Super-Res | DIV2K subset (800 imgs) | 500 | 4 hours | 33 dB PSNR |

---

## ë¦¬ìŠ¤í¬ ë° ì™„í™” ë°©ì•ˆ

### 1. Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í¬ê¸°

**Risk:** transformers íŒ¨í‚¤ì§€ê°€ ë§¤ìš° í¬ê³  ì˜ì¡´ì„± ë§ìŒ

**Mitigation:**
- Docker layer caching í™œìš©
- Base imageì— ê³µí†µ ì˜ì¡´ì„± í¬í•¨
- `--no-cache-dir` ì˜µì…˜ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ íŒŒì¼ ì œê±°

### 2. Model Download ì‹œê°„

**Risk:** Pretrained weights ë‹¤ìš´ë¡œë“œì— ì‹œê°„ ì†Œìš”

**Mitigation:**
- Docker ì´ë¯¸ì§€ ë¹Œë“œ ì‹œ ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ
- HuggingFace cache í™œìš© (`~/.cache/huggingface/`)

### 3. Super-Resolution Dataset ë¶€ì¡±

**Risk:** HR-LR paired dataset ì¤€ë¹„ ì–´ë ¤ì›€

**Mitigation:**
- DIV2K dataset ì‚¬ìš© (ê³µê°œ ë°ì´í„°ì…‹)
- On-the-fly downsamplingìœ¼ë¡œ LR ì´ë¯¸ì§€ ìƒì„±
- Sample dataset ì œê³µ

### 4. EoMT ëª¨ë¸ í¬ê¸°

**Risk:** 7B ëª¨ë¸ì€ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥

**Mitigation:**
- ì‘ì€ ëª¨ë¸ë¶€í„° ì‹œì‘ (ViT-Base: 0.3B)
- Gradient checkpointing í™œìš©
- Mixed precision training (fp16)

---

## Deliverables

**Week 5 ì™„ë£Œ ì‹œ:**
- [ ] TransformersAdapter êµ¬í˜„
- [ ] ViT classification ê²€ì¦
- [ ] Swin2SR super-resolution ê²€ì¦
- [ ] Dockerfile.huggingface
- [ ] Docker ì´ë¯¸ì§€ ë¹Œë“œ ì™„ë£Œ

**Week 6 ì™„ë£Œ ì‹œ:**
- [ ] D-FINE detection ê²€ì¦
- [ ] EoMT segmentation ê²€ì¦
- [ ] 4ê°œ ëª¨ë¸ ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸
- [ ] Backend API ì™„ì „ ì§€ì›
- [ ] Frontend 4ê°œ task UI
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸ (ì´ ê³„íšì„œ)

---

## ë‹¤ìŒ ë‹¨ê³„ (Week 7-8)

Week 7-8ì—ëŠ” Phase 3 (Custom Models)ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤:
- ConvNeXt (GitHub custom)
- YOLOv7 (GitHub custom)
- PP-YOLO (PaddlePaddle)
- ViTPose (MMPose)

ì´ë¥¼ í†µí•´ í”Œë«í¼ì˜ ê·¹í•œ í™•ì¥ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.

---

## ì°¸ê³  ë¬¸ì„œ

- [HuggingFace Transformers Docs](https://huggingface.co/docs/transformers)
- [D-FINE Paper](https://arxiv.org/abs/2410.13842)
- [EoMT Paper](https://arxiv.org/abs/2503.19108)
- [Swin2SR Paper](https://arxiv.org/abs/2209.11345)
- [IMPLEMENTATION_PRIORITY_ANALYSIS.md](./IMPLEMENTATION_PRIORITY_ANALYSIS.md)

---

*Document Version: 2.0*
*Created: 2025-10-30*
*Author: Claude Code*
