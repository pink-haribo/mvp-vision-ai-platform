# DICE Format: ë©€í‹°ëª¨ë‹¬(Vision+Text) í™•ì¥

**Version**: v1.1 (Multimodal Extension)
**Date**: 2025-01-04
**Status**: Design Complete

---

## ğŸ“‹ ëª©ì°¨

1. [ë°°ê²½ ë° í•„ìš”ì„±](#ë°°ê²½-ë°-í•„ìš”ì„±)
2. [ì§€ì›í•  ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬](#ì§€ì›í• -ë©€í‹°ëª¨ë‹¬-íƒœìŠ¤í¬)
3. [í™•ì¥ëœ ìŠ¤í‚¤ë§ˆ ì„¤ê³„](#í™•ì¥ëœ-ìŠ¤í‚¤ë§ˆ-ì„¤ê³„)
4. [Taskë³„ Annotation ì˜ˆì‹œ](#taskë³„-annotation-ì˜ˆì‹œ)
5. [í•˜ìœ„ í˜¸í™˜ì„±](#í•˜ìœ„-í˜¸í™˜ì„±)
6. [êµ¬í˜„ ê³„íš](#êµ¬í˜„-ê³„íš)

---

## ë°°ê²½ ë° í•„ìš”ì„±

### í˜„ì¬ DICE Format v1.0ì˜ í•œê³„

v1.0ì€ ìˆœìˆ˜ ë¹„ì „ íƒœìŠ¤í¬ë§Œ ì§€ì›:
- Image Classification
- Object Detection
- Instance Segmentation
- Semantic Segmentation
- Pose Estimation
- Super-Resolution

**ë¬¸ì œì **: í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” íƒœìŠ¤í¬ ë¯¸ì§€ì›
- Image Captioning: ì´ë¯¸ì§€ â†’ í…ìŠ¤íŠ¸ ì„¤ëª…
- VQA (Visual Question Answering): ì´ë¯¸ì§€ + ì§ˆë¬¸ â†’ ë‹µë³€
- Visual Grounding: ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ â†’ Bounding Box
- OCR: ì´ë¯¸ì§€ â†’ í…ìŠ¤íŠ¸ + ìœ„ì¹˜
- Vision-Language Pre-training: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ

### v1.1 ëª©í‘œ

âœ… **í…ìŠ¤íŠ¸ ë°ì´í„° í†µí•© ì €ì¥**
âœ… **ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬ 8ì¢… ì§€ì›**
âœ… **v1.0 í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€**
âœ… **Framework í˜¸í™˜ (HuggingFace datasets, CLIP, BLIP, LLaVA)**

---

## ì§€ì›í•  ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬

### 1. Image Captioning
**ì…ë ¥**: ì´ë¯¸ì§€
**ì¶œë ¥**: í…ìŠ¤íŠ¸ ì„¤ëª… (1ê°œ ì´ìƒ)

**ì˜ˆì‹œ**:
- ì´ë¯¸ì§€: ê³ ì–‘ì´ê°€ ì†ŒíŒŒì— ì•‰ì•„ìˆìŒ
- Caption: "A fluffy orange cat sitting on a gray sofa"

**í™œìš© ëª¨ë¸**: BLIP-2, GIT, ClipCap

---

### 2. Visual Question Answering (VQA)
**ì…ë ¥**: ì´ë¯¸ì§€ + ì§ˆë¬¸
**ì¶œë ¥**: ë‹µë³€

**ì˜ˆì‹œ**:
- ì´ë¯¸ì§€: ê³µì›ì—ì„œ ë›°ì–´ë…¸ëŠ” ê°œ
- ì§ˆë¬¸: "What is the dog doing?"
- ë‹µë³€: "Playing fetch in the park"

**í™œìš© ëª¨ë¸**: BLIP-2, LLaVA, InstructBLIP

---

### 3. Visual Grounding (Referring Expression)
**ì…ë ¥**: ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì„¤ëª…
**ì¶œë ¥**: Bounding Box

**ì˜ˆì‹œ**:
- ì´ë¯¸ì§€: ì—¬ëŸ¬ ì‚¬ëŒì´ ìˆëŠ” ì‚¬ì§„
- ì„¤ëª…: "The person wearing a red hat on the left"
- ì¶œë ¥: [100, 50, 200, 300] (bbox)

**í™œìš© ëª¨ë¸**: GLIP, MDETR, OWL-ViT

---

### 4. OCR (Optical Character Recognition)
**ì…ë ¥**: ì´ë¯¸ì§€
**ì¶œë ¥**: í…ìŠ¤íŠ¸ + Bounding Box

**ì˜ˆì‹œ**:
- ì´ë¯¸ì§€: ê°„íŒ ì‚¬ì§„
- ì¶œë ¥:
  - "COFFEE SHOP" at [120, 50, 300, 100]
  - "Open 9AM-6PM" at [130, 110, 290, 140]

**í™œìš© ëª¨ë¸**: PaddleOCR, TrOCR, Donut

---

### 5. Dense Captioning
**ì…ë ¥**: ì´ë¯¸ì§€
**ì¶œë ¥**: Regionë³„ ì„¤ëª… (Bbox + Caption)

**ì˜ˆì‹œ**:
- Region 1: [100, 50, 200, 150] â†’ "A red car"
- Region 2: [300, 100, 400, 250] â†’ "A person walking"

**í™œìš© ëª¨ë¸**: Dense Captioning models

---

### 6. Image-Text Matching
**ì…ë ¥**: ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸
**ì¶œë ¥**: Match score (0-1)

**ì˜ˆì‹œ**:
- ì´ë¯¸ì§€: ê°•ì•„ì§€ ì‚¬ì§„
- í…ìŠ¤íŠ¸: "A cute puppy playing with a ball"
- Score: 0.92

**í™œìš© ëª¨ë¸**: CLIP, ALIGN

---

### 7. Text-to-Image Retrieval
**ì…ë ¥**: í…ìŠ¤íŠ¸ ì¿¼ë¦¬
**ì¶œë ¥**: ê´€ë ¨ ì´ë¯¸ì§€ ID ë¦¬ìŠ¤íŠ¸

**ì˜ˆì‹œ**:
- ì¿¼ë¦¬: "sunset over the ocean"
- ê²°ê³¼: [img_001, img_045, img_123]

**í™œìš© ëª¨ë¸**: CLIP-based retrieval

---

### 8. Visual Dialogue
**ì…ë ¥**: ì´ë¯¸ì§€ + ëŒ€í™” íˆìŠ¤í† ë¦¬
**ì¶œë ¥**: ì‘ë‹µ

**ì˜ˆì‹œ**:
- ì´ë¯¸ì§€: ì£¼ë°© ì‚¬ì§„
- Q1: "What's on the table?" â†’ A1: "A bowl of fruit"
- Q2: "What kind of fruit?" â†’ A2: "Apples and bananas"

**í™œìš© ëª¨ë¸**: VisDial models

---

## í™•ì¥ëœ ìŠ¤í‚¤ë§ˆ ì„¤ê³„

### annotations.json ìµœìƒìœ„ í•„ë“œ ì¶”ê°€

```json
{
  "format_version": "1.1",  // â† v1.0ì—ì„œ v1.1ë¡œ ì—…ê·¸ë ˆì´ë“œ
  "dataset_id": "user123-vqa-dataset",
  "dataset_name": "VQA Dataset v2.0",

  "task_type": "visual_question_answering",  // â† ìƒˆë¡œìš´ íƒœìŠ¤í¬ íƒ€ì…

  "modalities": ["image", "text"],  // â† NEW: ì‚¬ìš©ë˜ëŠ” ëª¨ë‹¬ë¦¬í‹°

  "text_config": {  // â† NEW: í…ìŠ¤íŠ¸ ë°ì´í„° ì„¤ì •
    "language": "en",  // ko, en, multi
    "tokenizer": "bert-base-uncased",  // Optional
    "max_length": 512,  // Optional
    "vocab_size": 30522  // Optional
  },

  "classes": [/* ... */],
  "images": [/* í™•ì¥ëœ annotation êµ¬ì¡° */],
  "statistics": {/* ... */}
}
```

### ì§€ì› Task Types (í™•ì¥)

```python
# v1.0 (Pure Vision)
VISION_TASKS = [
    "image_classification",
    "object_detection",
    "instance_segmentation",
    "semantic_segmentation",
    "pose_estimation",
    "super_resolution"
]

# v1.1 (Multimodal: Vision + Text)
MULTIMODAL_TASKS = [
    "image_captioning",
    "visual_question_answering",
    "visual_grounding",
    "ocr",
    "dense_captioning",
    "image_text_matching",
    "text_to_image_retrieval",
    "visual_dialogue"
]
```

---

## Taskë³„ Annotation ì˜ˆì‹œ

### 1. Image Captioning

```json
{
  "id": 1,
  "file_name": "cat_on_sofa.jpg",
  "width": 1920,
  "height": 1080,
  "split": "train",

  "annotation": {
    "captions": [
      {
        "caption_id": 1,
        "text": "A fluffy orange cat sitting on a gray sofa",
        "language": "en",
        "labeled_by": "user123",
        "labeled_at": "2025-01-15T10:00:00Z"
      },
      {
        "caption_id": 2,
        "text": "An orange cat relaxing on a couch",
        "language": "en",
        "labeled_by": "user456",
        "labeled_at": "2025-01-15T10:05:00Z"
      }
    ],
    "primary_caption": "A fluffy orange cat sitting on a gray sofa"  // Optional
  },

  "metadata": {
    "num_captions": 2,
    "avg_caption_length": 42
  }
}
```

---

### 2. Visual Question Answering (VQA)

```json
{
  "id": 2,
  "file_name": "park_scene.jpg",
  "width": 1920,
  "height": 1080,
  "split": "train",

  "annotation": {
    "qa_pairs": [
      {
        "qa_id": 1,
        "question": "What is the dog doing?",
        "answer": "Playing fetch",
        "answer_type": "activity",  // activity, object, color, counting, yes/no
        "confidence": 1.0,
        "labeled_by": "user123"
      },
      {
        "qa_id": 2,
        "question": "How many people are in the image?",
        "answer": "3",
        "answer_type": "counting",
        "confidence": 1.0,
        "labeled_by": "user123"
      },
      {
        "qa_id": 3,
        "question": "Is it daytime?",
        "answer": "yes",
        "answer_type": "yes/no",
        "confidence": 1.0,
        "labeled_by": "user456"
      }
    ]
  },

  "metadata": {
    "num_qa_pairs": 3,
    "answer_types": {
      "activity": 1,
      "counting": 1,
      "yes/no": 1
    }
  }
}
```

---

### 3. Visual Grounding (Referring Expression)

```json
{
  "id": 3,
  "file_name": "people_crowd.jpg",
  "width": 3000,
  "height": 2000,
  "split": "train",

  "annotation": {
    "referring_expressions": [
      {
        "ref_id": 1,
        "expression": "The person wearing a red hat on the left",
        "bbox": [100, 50, 200, 300],
        "bbox_format": "xywh",
        "labeled_by": "user123",
        "labeled_at": "2025-01-15T10:00:00Z"
      },
      {
        "ref_id": 2,
        "expression": "The woman in blue dress holding a phone",
        "bbox": [500, 100, 180, 350],
        "bbox_format": "xywh",
        "labeled_by": "user123",
        "labeled_at": "2025-01-15T10:02:00Z"
      }
    ]
  },

  "metadata": {
    "num_referring_expressions": 2
  }
}
```

---

### 4. OCR (Optical Character Recognition)

```json
{
  "id": 4,
  "file_name": "sign_board.jpg",
  "width": 2400,
  "height": 1600,
  "split": "train",

  "annotation": {
    "text_regions": [
      {
        "text_id": 1,
        "text": "COFFEE SHOP",
        "bbox": [120, 50, 300, 100],
        "bbox_format": "xywh",
        "confidence": 0.98,
        "language": "en",
        "font_size": "large",
        "orientation": 0  // degrees
      },
      {
        "text_id": 2,
        "text": "Open 9AM-6PM",
        "bbox": [130, 110, 290, 140],
        "bbox_format": "xywh",
        "confidence": 0.95,
        "language": "en",
        "font_size": "medium",
        "orientation": 0
      },
      {
        "text_id": 3,
        "text": "ë§¤ì¼ ì˜ì—…",
        "bbox": [140, 150, 280, 180],
        "bbox_format": "xywh",
        "confidence": 0.92,
        "language": "ko",
        "font_size": "small",
        "orientation": 0
      }
    ]
  },

  "metadata": {
    "num_text_regions": 3,
    "languages": ["en", "ko"],
    "total_characters": 28
  }
}
```

---

### 5. Dense Captioning

```json
{
  "id": 5,
  "file_name": "street_view.jpg",
  "width": 3000,
  "height": 2000,
  "split": "train",

  "annotation": {
    "region_captions": [
      {
        "region_id": 1,
        "bbox": [100, 50, 200, 150],
        "bbox_format": "xywh",
        "caption": "A red car parked on the street",
        "confidence": 0.95
      },
      {
        "region_id": 2,
        "bbox": [300, 100, 400, 250],
        "bbox_format": "xywh",
        "caption": "A person walking with an umbrella",
        "confidence": 0.92
      },
      {
        "region_id": 3,
        "bbox": [800, 20, 200, 100],
        "bbox_format": "xywh",
        "caption": "A traffic light showing green",
        "confidence": 0.88
      }
    ]
  },

  "metadata": {
    "num_regions": 3,
    "avg_caption_length": 35
  }
}
```

---

### 6. Image-Text Matching

```json
{
  "id": 6,
  "file_name": "puppy_playing.jpg",
  "width": 1920,
  "height": 1080,
  "split": "train",

  "annotation": {
    "positive_captions": [
      {
        "caption_id": 1,
        "text": "A cute puppy playing with a ball in the garden",
        "match_score": 1.0
      },
      {
        "caption_id": 2,
        "text": "A young dog having fun outdoors",
        "match_score": 1.0
      }
    ],
    "negative_captions": [
      {
        "caption_id": 3,
        "text": "A cat sleeping on a bed",
        "match_score": 0.0
      },
      {
        "caption_id": 4,
        "text": "People playing soccer in a park",
        "match_score": 0.0
      }
    ]
  },

  "metadata": {
    "num_positive": 2,
    "num_negative": 2
  }
}
```

---

### 7. Text-to-Image Retrieval

**ë°ì´í„°ì…‹ ë ˆë²¨ êµ¬ì¡°** (images ë°°ì—´ ì™¸ë¶€):

```json
{
  "format_version": "1.1",
  "task_type": "text_to_image_retrieval",

  "queries": [
    {
      "query_id": 1,
      "text": "sunset over the ocean",
      "relevant_image_ids": [5, 12, 34, 67],
      "language": "en"
    },
    {
      "query_id": 2,
      "text": "city skyline at night",
      "relevant_image_ids": [8, 23, 45],
      "language": "en"
    }
  ],

  "images": [
    {
      "id": 5,
      "file_name": "beach_sunset.jpg",
      "annotation": {
        "relevant_queries": [1],  // ì—­ë°©í–¥ ì°¸ì¡°
        "tags": ["sunset", "ocean", "beach", "sky"]
      }
    }
  ]
}
```

---

### 8. Visual Dialogue

```json
{
  "id": 8,
  "file_name": "kitchen_scene.jpg",
  "width": 1920,
  "height": 1080,
  "split": "train",

  "annotation": {
    "dialogues": [
      {
        "dialogue_id": 1,
        "turns": [
          {
            "turn_id": 1,
            "question": "What's on the table?",
            "answer": "A bowl of fruit",
            "questioner": "user123"
          },
          {
            "turn_id": 2,
            "question": "What kind of fruit?",
            "answer": "Apples and bananas",
            "questioner": "user123"
          },
          {
            "turn_id": 3,
            "question": "How many apples are there?",
            "answer": "Three red apples",
            "questioner": "user456"
          }
        ],
        "created_at": "2025-01-15T10:00:00Z"
      }
    ]
  },

  "metadata": {
    "num_dialogues": 1,
    "total_turns": 3,
    "avg_turns_per_dialogue": 3.0
  }
}
```

---

## í•˜ìœ„ í˜¸í™˜ì„±

### v1.0 â†’ v1.1 Migration

**ìë™ ê°ì§€ ë¡œì§**:

```python
def detect_format_version(annotations: dict) -> str:
    version = annotations.get("format_version", "1.0")

    # v1.0 dataset can be read as-is in v1.1
    if version == "1.0":
        # Pure vision tasks - no migration needed
        return "1.0_compatible"

    # v1.1 with multimodal
    if "modalities" in annotations or "text_config" in annotations:
        return "1.1"

    return "1.0"
```

**í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥**:

âœ… v1.0 ë°ì´í„°ì…‹ì€ v1.1 ì‹œìŠ¤í…œì—ì„œ ê·¸ëŒ€ë¡œ ë™ì‘
âœ… v1.1 multimodal í•„ë“œëŠ” Optional (ì—†ì–´ë„ ë¨)
âœ… Pure vision taskëŠ” v1.0 ìŠ¤í‚¤ë§ˆ ìœ ì§€
âœ… v1.0 â†’ v1.1 ì—…ê·¸ë ˆì´ë“œëŠ” `format_version` í•„ë“œë§Œ ë³€ê²½

---

## Framework í˜¸í™˜ì„±

### HuggingFace datasets ë³€í™˜

```python
from datasets import Dataset, Features, Value, Sequence

# VQA ì˜ˆì‹œ
def convert_to_hf_dataset(dice_annotations: dict) -> Dataset:
    features = Features({
        'image': Image(),
        'question': Value('string'),
        'answer': Value('string'),
        'answer_type': Value('string')
    })

    data = []
    for img in dice_annotations['images']:
        for qa in img['annotation']['qa_pairs']:
            data.append({
                'image': img['file_name'],
                'question': qa['question'],
                'answer': qa['answer'],
                'answer_type': qa['answer_type']
            })

    return Dataset.from_dict(data, features=features)
```

### CLIP / BLIP í˜•ì‹ ë³€í™˜

```python
# Image-Text Pair ë³€í™˜
def convert_to_clip_format(dice_annotations: dict):
    """DICE Format â†’ CLIP training pairs"""
    pairs = []

    for img in dice_annotations['images']:
        if 'captions' in img['annotation']:
            for cap in img['annotation']['captions']:
                pairs.append({
                    'image_path': img['file_name'],
                    'caption': cap['text']
                })

    return pairs
```

---

## êµ¬í˜„ ê³„íš

### Phase 1: ìŠ¤í‚¤ë§ˆ í™•ì¥ (1ì£¼)
- [ ] annotations.json ìŠ¤í‚¤ë§ˆì— multimodal í•„ë“œ ì¶”ê°€
- [ ] 8ê°€ì§€ ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬ ìŠ¤í‚¤ë§ˆ ì •ì˜
- [ ] Validation ë¡œì§ êµ¬í˜„ (pydantic)
- [ ] ì˜ˆì‹œ ë°ì´í„°ì…‹ ìƒì„±

### Phase 2: Backend API (1ì£¼)
- [ ] Dataset ì—…ë¡œë“œ ì‹œ multimodal í•„ë“œ íŒŒì‹±
- [ ] Task typeë³„ validation
- [ ] Text ë°ì´í„° ì €ì¥/ì¡°íšŒ API
- [ ] Statistics ê³„ì‚° (í…ìŠ¤íŠ¸ ê¸¸ì´, QA í˜ì–´ ìˆ˜ ë“±)

### Phase 3: Format Converter (2ì£¼)
- [ ] DICE â†’ HuggingFace datasets ë³€í™˜
- [ ] DICE â†’ CLIP/BLIP í˜•ì‹ ë³€í™˜
- [ ] DICE â†’ VQA v2.0 í˜•ì‹ ë³€í™˜
- [ ] DICE â†’ OCR í˜•ì‹ ë³€í™˜
- [ ] Cache ë©”ì»¤ë‹ˆì¦˜ (content_hash ê¸°ë°˜)

### Phase 4: UI/Labeler (3ì£¼)
- [ ] Caption ì…ë ¥ UI
- [ ] VQA ë ˆì´ë¸”ë§ UI (ì§ˆë¬¸+ë‹µë³€)
- [ ] Visual Grounding UI (í…ìŠ¤íŠ¸ + Bbox)
- [ ] OCR ë ˆì´ë¸”ë§ UI
- [ ] Multi-turn dialogue UI

### Phase 5: Training Pipeline (2ì£¼)
- [ ] Image Captioning í•™ìŠµ (BLIP-2)
- [ ] VQA í•™ìŠµ (LLaVA)
- [ ] Visual Grounding í•™ìŠµ (GLIP)
- [ ] OCR í•™ìŠµ (TrOCR)

**ì´ ì˜ˆìƒ ê¸°ê°„**: 9ì£¼ (2ê°œì›”)

---

## ì˜ˆì‹œ íŒŒì¼

### ì™„ì „í•œ VQA ë°ì´í„°ì…‹ ì˜ˆì‹œ

íŒŒì¼: `example-v1.1-vqa.json`

```json
{
  "format_version": "1.1",
  "dataset_id": "vqa-demo-001",
  "dataset_name": "VQA Demo Dataset",

  "task_type": "visual_question_answering",
  "modalities": ["image", "text"],

  "text_config": {
    "language": "en",
    "max_question_length": 50,
    "max_answer_length": 20
  },

  "created_at": "2025-01-15T10:00:00Z",
  "version": 1,
  "content_hash": "sha256:vqa123...",

  "classes": [],  // VQAëŠ” í´ë˜ìŠ¤ ì—†ìŒ

  "splits": {
    "train": 80,
    "val": 15,
    "test": 5
  },

  "images": [
    {
      "id": 1,
      "file_name": "images/park_001.jpg",
      "width": 1920,
      "height": 1080,
      "split": "train",

      "annotation": {
        "qa_pairs": [
          {
            "qa_id": 1,
            "question": "What is the dog doing?",
            "answer": "Playing fetch",
            "answer_type": "activity",
            "confidence": 1.0
          },
          {
            "qa_id": 2,
            "question": "How many people are visible?",
            "answer": "3",
            "answer_type": "counting",
            "confidence": 1.0
          }
        ]
      },

      "metadata": {
        "labeled_by": "user123",
        "labeled_at": "2025-01-15T10:00:00Z",
        "num_qa_pairs": 2
      }
    }
  ],

  "statistics": {
    "total_images": 100,
    "total_qa_pairs": 350,
    "avg_qa_per_image": 3.5,
    "answer_type_distribution": {
      "yes/no": 100,
      "counting": 50,
      "activity": 80,
      "object": 70,
      "color": 30,
      "other": 20
    },
    "avg_question_length": 8.5,
    "avg_answer_length": 2.3
  }
}
```

---

## ìš”ì•½

### v1.1 ì£¼ìš” ë³€ê²½ ì‚¬í•­

| í•­ëª© | v1.0 | v1.1 (Multimodal) |
|------|------|-------------------|
| **ì§€ì› íƒœìŠ¤í¬** | ìˆœìˆ˜ ë¹„ì „ 6ì¢… | ë¹„ì „ 6ì¢… + ë©€í‹°ëª¨ë‹¬ 8ì¢… |
| **format_version** | "1.0" | "1.1" |
| **ìƒˆ í•„ë“œ** | - | `modalities`, `text_config` |
| **Annotation íƒ€ì…** | ì´ë¯¸ì§€ ê¸°ë°˜ | ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ |
| **í•˜ìœ„ í˜¸í™˜** | N/A | âœ… v1.0 ì™„ì „ í˜¸í™˜ |

### ë©€í‹°ëª¨ë‹¬ ì§€ì› íƒœìŠ¤í¬ 8ì¢…

1. âœ… Image Captioning
2. âœ… Visual Question Answering (VQA)
3. âœ… Visual Grounding
4. âœ… OCR
5. âœ… Dense Captioning
6. âœ… Image-Text Matching
7. âœ… Text-to-Image Retrieval
8. âœ… Visual Dialogue

---

**Last Updated**: 2025-01-04
**Next Steps**: Phase 1 êµ¬í˜„ ì‹œì‘ (ìŠ¤í‚¤ë§ˆ í™•ì¥ ë° Validation)
