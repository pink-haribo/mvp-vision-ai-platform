# Week 1 Model Selection - Revised (2024 Latest)

**Document Version:** 2.0
**Updated:** 2025-10-30
**Status:** Implementation Ready

---

## ë³€ê²½ ì´ìœ 

**ê¸°ì¡´ ê³„íšì˜ ë¬¸ì œì **:
- âŒ YOLOv8ë§Œ í¬í•¨, YOLOv11 (2024ë…„ 9ì›” ìµœì‹ ) ëˆ„ë½
- âŒ YOLO-World (open-vocabulary detection) ê°™ì€ í˜ì‹ ì  ëª¨ë¸ ë¯¸í¬í•¨
- âŒ timm ìµœì‹  ëª¨ë¸ (NaFlexViT, MobileNetV4/V5) ëˆ„ë½
- âŒ EfficientNetV2 ëˆ„ë½

**ê°œì„ ëœ ì„ íƒ ê¸°ì¤€**:
1. â­ **ìµœì‹ ì„± ìš°ì„ **: 2024ë…„ ì¶œì‹œ ëª¨ë¸ í¬í•¨
2. ğŸš€ **íŠ¹ìˆ˜ ê¸°ëŠ¥**: Open-vocabulary, SAM ë“± ì°¨ë³„í™” ê¸°ëŠ¥
3. ğŸ“Š **ì‹¤ìš©ì„±**: ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œ ë§ì´ ì‚¬ìš©
4. ğŸ“ **í¬ê¸° ë²”ìœ„**: nano ~ xlarge ë‹¤ì–‘í•œ í¬ê¸°
5. ğŸ—ï¸ **ì•„í‚¤í…ì²˜ ë‹¤ì–‘ì„±**: CNN, ViT, Hybrid ê³¨ê³ ë£¨

---

## ìµœì‹  ëª¨ë¸ í˜„í™© (2024)

### Ultralytics ìµœì‹ 

| ëª¨ë¸ | ì¶œì‹œ | ì£¼ìš” íŠ¹ì§• |
|------|------|-----------|
| **YOLOv11** | 2024ë…„ 9ì›” | YOLOv8 ëŒ€ë¹„ 22% ì ì€ íŒŒë¼ë¯¸í„°, ë” ë†’ì€ mAP, C3k2/SPPF/C2PSA ë¸”ë¡ |
| **YOLO-World** | CVPR 2024 | Open-vocabulary detection, zero-shot, custom prompts |
| **YOLOv10** | 2024ë…„ 5ì›” | NMS-free, íš¨ìœ¨ì„± ê°œì„  |
| **YOLOv9** | 2024ë…„ ì´ˆ | Programmable gradient, generalized ELAN |

### timm ìµœì‹ 

| ëª¨ë¸ | ì¶œì‹œ | ì£¼ìš” íŠ¹ì§• |
|------|------|-----------|
| **NaFlexViT** | 2024ë…„ í›„ë°˜ | Variable aspect/resolution, FlexiViT + NaViT + NaFlex í†µí•© |
| **MobileNetV5** | 2024ë…„ | Gemma 3n encoder, ìµœì‹  ëª¨ë°”ì¼ ì•„í‚¤í…ì²˜ |
| **MobileNetV4** | 2024ë…„ | Universal Inverted Bottleneck |
| **SigLIP-2** | 2024ë…„ | NaFlex ViT encoder, 88.1% top-1 |
| **EfficientNetV2** | 2021ë…„ | EfficientNetV1ë³´ë‹¤ í›¨ì”¬ ê°œì„  (ëˆ„ë½ë˜ì–´ ìˆì—ˆìŒ) |

---

## Week 1 ì¶”ê°€ ëª¨ë¸ (ìµœì¢…ì•ˆ)

### ğŸ”¥ Priority 1: ë°˜ë“œì‹œ ì¶”ê°€ (ìµœì‹  + í•µì‹¬)

#### Ultralytics (12ê°œ)

##### YOLOv11 (ìµœì‹ , í•„ìˆ˜!) - 5ê°œ
```python
"yolo11n": {
    "display_name": "YOLOv11 Nano",
    "description": "Latest YOLO (Sep 2024) - Ultra-lightweight, 22% fewer params than YOLOv8",
    "params": "2.6M",
    "task_type": "object_detection",
    "input_size": 640,
    "recommended_batch_size": 64,
    "recommended_lr": 0.01,
    "tags": ["latest", "2024", "ultralight", "realtime", "sota"]
}

"yolo11s": {
    "display_name": "YOLOv11 Small",
    "params": "9.4M",
    "tags": ["latest", "2024", "lightweight", "fast", "sota"]
}

"yolo11m": {
    "display_name": "YOLOv11 Medium",
    "params": "20.1M",
    "tags": ["latest", "2024", "balanced", "sota"]
}

"yolo11l": {
    "display_name": "YOLOv11 Large",
    "params": "25.3M",
    "tags": ["latest", "2024", "accurate", "sota"]
}

"yolo11x": {
    "display_name": "YOLOv11 Extra-Large",
    "params": "56.9M",
    "tags": ["latest", "2024", "heavy", "maximum-accuracy", "sota"]
}
```

##### YOLO-World (í˜ì‹ ì !) - 2ê°œ
```python
"yolo_world_v2_s": {
    "display_name": "YOLO-World-v2 Small",
    "description": "Open-vocabulary detection (CVPR 2024) - Detect ANY object with text prompts",
    "params": "22M",
    "task_type": "open_vocabulary_detection",  # ì‹ ê·œ task!
    "input_size": 640,
    "recommended_batch_size": 16,
    "recommended_lr": 0.01,
    "tags": ["cvpr2024", "open-vocab", "zero-shot", "innovative", "text-prompt"]
}

"yolo_world_v2_m": {
    "display_name": "YOLO-World-v2 Medium",
    "description": "Open-vocabulary detection - 35.4 AP @ 52 FPS on LVIS",
    "params": "42M",
    "tags": ["cvpr2024", "open-vocab", "zero-shot", "accurate"]
}
```

##### Segmentation (ìµœì‹ ) - 3ê°œ
```python
"yolo11n_seg": {
    "display_name": "YOLOv11n-Seg",
    "description": "Latest segmentation model (Sep 2024)",
    "params": "2.9M",
    "task_type": "instance_segmentation",
    "tags": ["latest", "2024", "segmentation", "ultralight"]
}

"yolo11m_seg": {
    "display_name": "YOLOv11m-Seg",
    "params": "22.4M",
    "tags": ["latest", "2024", "segmentation", "balanced"]
}

"yolo11x_seg": {
    "display_name": "YOLOv11x-Seg",
    "params": "62.1M",
    "tags": ["latest", "2024", "segmentation", "accurate"]
}
```

##### Pose (ìµœì‹ ) - 2ê°œ
```python
"yolo11m_pose": {
    "display_name": "YOLOv11m-Pose",
    "description": "Latest pose estimation - 17 keypoints",
    "params": "21.8M",
    "task_type": "pose_estimation",
    "tags": ["latest", "2024", "pose", "keypoints"]
}

"yolo11l_pose": {
    "display_name": "YOLOv11l-Pose",
    "params": "26.9M",
    "tags": ["latest", "2024", "pose", "accurate"]
}
```

---

#### timm (15ê°œ)

##### ìµœì‹  Mobile (2024) - 3ê°œ
```python
"mobilenetv4_conv_medium": {
    "display_name": "MobileNetV4-Medium",
    "description": "Latest mobile architecture (2024) with Universal Inverted Bottleneck",
    "params": "9.7M",
    "input_size": 224,
    "pretrained_available": True,
    "recommended_batch_size": 128,
    "recommended_lr": 0.001,
    "tags": ["latest", "2024", "mobile", "efficient", "uib"]
}

"mobilenetv5_large": {
    "display_name": "MobileNetV5-Large",
    "description": "Cutting-edge mobile model for Gemma 3n (2024)",
    "params": "12M",
    "tags": ["latest", "2024", "mobile", "gemma", "sota"]
}

"mobilenetv3_large_100": {
    "display_name": "MobileNetV3-Large",
    "description": "Popular mobile CNN (baseline comparison)",
    "params": "5.5M",
    "tags": ["mobile", "efficient", "baseline"]
}
```

##### ìµœì‹  ViT (2024) - 3ê°œ
```python
"vit_so150m_patch16_224": {
    "display_name": "ViT-SO150M/16 (SigLIP-2)",
    "description": "SigLIP-2 NaFlex ViT (2024) - 88.1% top-1 accuracy",
    "params": "150M",
    "input_size": 224,
    "recommended_batch_size": 32,
    "recommended_lr": 0.0003,
    "tags": ["latest", "2024", "vit", "siglip", "sota", "88.1%"]
}

"vit_base_patch16_224": {
    "display_name": "ViT-Base/16",
    "description": "Standard Vision Transformer (baseline)",
    "params": "86M",
    "tags": ["vit", "transformer", "baseline"]
}

"vit_large_patch16_224": {
    "display_name": "ViT-Large/16",
    "description": "Large Vision Transformer",
    "params": "307M",
    "tags": ["vit", "transformer", "heavy"]
}
```

##### EfficientNet ê³„ì—´ - 4ê°œ
```python
"efficientnetv2_s": {
    "display_name": "EfficientNetV2-Small",
    "description": "Improved EfficientNet (2021) - Training efficiency++",
    "params": "21.5M",
    "input_size": 384,
    "recommended_batch_size": 64,
    "recommended_lr": 0.001,
    "tags": ["efficient", "modern", "v2", "fast-training"]
}

"efficientnetv2_m": {
    "display_name": "EfficientNetV2-Medium",
    "params": "54M",
    "input_size": 480,
    "tags": ["efficient", "modern", "v2"]
}

"efficientnet_b0": {
    "display_name": "EfficientNet-B0",
    "description": "Original EfficientNet (baseline)",
    "params": "5.3M",
    "input_size": 224,
    "tags": ["efficient", "lightweight", "baseline"]
}

"efficientnet_b4": {
    "display_name": "EfficientNet-B4",
    "params": "19M",
    "input_size": 380,
    "tags": ["efficient", "accurate"]
}
```

##### ResNet ê³„ì—´ (baseline) - 3ê°œ
```python
"resnet18": {
    "display_name": "ResNet-18",
    "description": "Classic lightweight CNN (baseline)",
    "params": "11.7M",
    "input_size": 224,
    "tags": ["classic", "lightweight", "baseline", "fast"]
}

"resnet50": {
    "display_name": "ResNet-50",
    "description": "Most popular baseline CNN",
    "params": "25.6M",
    "tags": ["classic", "baseline", "popular", "standard"]
}

"resnet101": {
    "display_name": "ResNet-101",
    "description": "Deep ResNet",
    "params": "44.5M",
    "tags": ["classic", "deep", "accurate"]
}
```

##### ConvNeXt (Modern CNN) - 2ê°œ
```python
"convnext_tiny": {
    "display_name": "ConvNeXt-Tiny",
    "description": "Modern CNN with Transformer design principles (2022)",
    "params": "28M",
    "input_size": 224,
    "recommended_batch_size": 64,
    "recommended_lr": 0.001,
    "tags": ["modern", "cnn", "transformer-style", "balanced"]
}

"convnext_base": {
    "display_name": "ConvNeXt-Base",
    "params": "89M",
    "tags": ["modern", "cnn", "transformer-style", "accurate"]
}
```

---

### ğŸ“Š Priority 2: ì„ íƒì  ì¶”ê°€ (ì‹œê°„ ì—¬ìœ  ì‹œ)

#### YOLOv10 (2ê°œ) - NMS-free íŠ¹ì§•
```python
"yolov10n": {
    "display_name": "YOLOv10 Nano",
    "description": "NMS-free detection (May 2024)",
    "params": "2.3M",
    "tags": ["2024", "nms-free", "efficient"]
}

"yolov10s": {
    "display_name": "YOLOv10 Small",
    "params": "7.2M",
    "tags": ["2024", "nms-free", "fast"]
}
```

#### MaxViT (Hybrid) - 2ê°œ
```python
"maxvit_tiny_tf_224": {
    "display_name": "MaxViT-Tiny",
    "description": "Hybrid CNN + ViT with multi-axis attention",
    "params": "31M",
    "tags": ["hybrid", "cnn+vit", "multi-axis-attention"]
}

"maxvit_small_tf_224": {
    "display_name": "MaxViT-Small",
    "params": "69M",
    "tags": ["hybrid", "cnn+vit"]
}
```

---

## ìµœì¢… ëª¨ë¸ ê°œìˆ˜ ìš”ì•½

| í”„ë ˆì„ì›Œí¬ | Priority 1 | Priority 2 | ì´í•© |
|-----------|-----------|-----------|------|
| **Ultralytics** | 12ê°œ | 2ê°œ | 14ê°œ |
| **timm** | 15ê°œ | 2ê°œ | 17ê°œ |
| **ì´í•©** | **27ê°œ** | **4ê°œ** | **31ê°œ** |

---

## ëª¨ë¸ ë¶„ë¥˜ (Tag ê¸°ë°˜)

### By Recency
- **2024 ìµœì‹ ** (9ê°œ): YOLOv11 (5), YOLO-World (2), MobileNetV4/V5 (2), SigLIP-2 (1)
- **2023-2024** (4ê°œ): YOLOv10 (2), EfficientNetV2 (2)
- **Baseline** (5ê°œ): ResNet (3), EfficientNet-B0/B4 (2)

### By Architecture
- **CNN**: ResNet (3), EfficientNet (4), ConvNeXt (2), MobileNet (3)
- **ViT**: ViT (3), SigLIP-2 (1)
- **Hybrid**: MaxViT (2)
- **YOLO**: YOLOv11 (10), YOLO-World (2), YOLOv10 (2)

### By Size
- **Nano/Tiny** (< 5M): YOLOv11n (2.6M), MobileNetV3 (5.5M), EfficientNet-B0 (5.3M)
- **Small/Medium** (5-30M): ëŒ€ë¶€ë¶„
- **Large** (30-100M): ViT-Large, ConvNeXt-Base
- **XLarge** (> 100M): SigLIP-2 (150M)

### By Special Features
- **Open-vocabulary**: YOLO-World (2)
- **Zero-shot**: YOLO-World
- **NMS-free**: YOLOv10 (2)
- **Mobile-optimized**: MobileNetV3/V4/V5 (3)
- **SOTA 2024**: YOLOv11, SigLIP-2, MobileNetV5

---

## êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Day 1-2: Core Infrastructure (ë°˜ë“œì‹œ)
1. ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•
2. API ì—”ë“œí¬ì¸íŠ¸ (`/models/list`)
3. Frontend ëª¨ë¸ ì„ íƒ UI

### Day 3-4: Priority 1 Models (27ê°œ)
1. **Ultralytics** (12ê°œ)
   - YOLOv11 ê³„ì—´ (10ê°œ): Detection (5) + Segmentation (3) + Pose (2)
   - YOLO-World (2ê°œ): íŠ¹ë³„ ì²˜ë¦¬ í•„ìš” (open-vocab task ì¶”ê°€)

2. **timm** (15ê°œ)
   - Mobile: MobileNetV3/V4/V5 (3ê°œ)
   - ViT: Standard + SigLIP-2 (3ê°œ)
   - EfficientNet: V1 + V2 (4ê°œ)
   - ResNet: 18/50/101 (3ê°œ)
   - ConvNeXt (2ê°œ)

### Day 5: Testing & Validation
- ê° ëª¨ë¸ë¡œ ê°„ë‹¨í•œ í•™ìŠµ ì‹¤í–‰
- UIì—ì„œ ëª¨ë¸ ì„ íƒ í…ŒìŠ¤íŠ¸
- ì •ìƒ ë™ì‘ í™•ì¸

### Day 6-7: Priority 2 (Optional, 4ê°œ)
- YOLOv10 (2ê°œ)
- MaxViT (2ê°œ)

---

## íŠ¹ë³„ ê³ ë ¤ì‚¬í•­

### 1. YOLO-World (Open-Vocabulary)

**ìƒˆë¡œìš´ Task Type ì¶”ê°€ í•„ìš”**:
```python
# mvp/training/adapters/base.py
class TaskType(Enum):
    # ... existing
    OPEN_VOCABULARY_DETECTION = "open_vocabulary_detection"  # ì‹ ê·œ!
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# ê¸°ì¡´ YOLO: ê³ ì •ëœ í´ë˜ìŠ¤
result = model.predict("image.jpg")  # â†’ ë¯¸ë¦¬ í•™ìŠµëœ 80ê°œ í´ë˜ìŠ¤ë§Œ

# YOLO-World: ë™ì  í´ë˜ìŠ¤
result = model.set_classes(["cat", "dog", "car"]).predict("image.jpg")
# â†’ ì„ì˜ì˜ í´ë˜ìŠ¤ ì§€ì • ê°€ëŠ¥! (zero-shot)
```

**êµ¬í˜„ ê³ ë ¤ì‚¬í•­**:
- TrainingConfigì— `custom_prompts` í•„ë“œ ì¶”ê°€
- UltralyticsAdapterì—ì„œ YOLO-World íŠ¹ìˆ˜ ì²˜ë¦¬
- Frontendì—ì„œ í…ìŠ¤íŠ¸ ì…ë ¥ UI ì¶”ê°€

### 2. YOLOv11 vs YOLOv8

**ë§ˆì´ê·¸ë ˆì´ì…˜ ê°„ë‹¨**:
```python
# YOLOv8 (ê¸°ì¡´)
model = YOLO("yolov8n.pt")

# YOLOv11 (ì‹ ê·œ)
model = YOLO("yolo11n.pt")  # ë™ì¼í•œ API!
```

**UltralyticsAdapter ìˆ˜ì • ë¶ˆí•„ìš”** - ëª¨ë¸ëª…ë§Œ ë³€ê²½í•˜ë©´ ìë™ ì§€ì› âœ…

### 3. timm ìµœì‹  ëª¨ë¸

**ëŒ€ë¶€ë¶„ ìë™ ì§€ì›**:
```python
model = timm.create_model("mobilenetv4_conv_medium", pretrained=True)
# TimmAdapter ìˆ˜ì • ë¶ˆí•„ìš” âœ…
```

**ë‹¨, ì£¼ì˜ì‚¬í•­**:
- MobileNetV4/V5: ìµœì‹  timm ë²„ì „ í•„ìš” (>= 0.9.0)
- SigLIP-2: íŠ¹ìˆ˜í•œ preprocessing í•„ìš”í•  ìˆ˜ ìˆìŒ

---

## ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

ê° ëª¨ë¸ì— ëŒ€í•´:
- [ ] ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì •ì˜ (ë ˆì§€ìŠ¤íŠ¸ë¦¬)
- [ ] APIë¡œ ëª©ë¡ ì¡°íšŒ ê°€ëŠ¥
- [ ] Frontendì—ì„œ ì„ íƒ ê°€ëŠ¥
- [ ] í•™ìŠµ ì •ìƒ ì‹¤í–‰ (ìµœì†Œ 3 epochs)
- [ ] Validation metrics ê³„ì‚°
- [ ] Checkpoint ì €ì¥/ë¡œë“œ
- [ ] Inference ìˆ˜í–‰

**íŠ¹ìˆ˜ ëª¨ë¸ ì¶”ê°€ ê²€ì¦**:
- [ ] YOLO-World: Custom prompts ë™ì‘
- [ ] YOLOv11: YOLOv8 ëŒ€ë¹„ ì„±ëŠ¥ í™•ì¸
- [ ] SigLIP-2: Preprocessing ì •ìƒ ë™ì‘

---

## ì˜ˆìƒ ì‹œê°„

| ì‘ì—… | ì‹œê°„ | ë¹„ê³  |
|------|------|------|
| Infrastructure | 2ì¼ | ë ˆì§€ìŠ¤íŠ¸ë¦¬, API, UI |
| Priority 1 (27ê°œ) | 2ì¼ | ëŒ€ë¶€ë¶„ ìë™ ì§€ì› |
| Testing | 1ì¼ | ì£¼ìš” ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸ |
| Priority 2 (4ê°œ) | 1ì¼ | Optional |
| Documentation | 0.5ì¼ | ë¦¬í¬íŠ¸ ì‘ì„± |
| **ì´ ì†Œìš”** | **6-7ì¼** | |

---

## ë‹¤ìŒ ë‹¨ê³„ (Week 2 ì¤€ë¹„)

Week 1 ì™„ë£Œ í›„:
1. **ê²€ì¦ ë¦¬í¬íŠ¸ ì‘ì„±**
   - ê° ëª¨ë¸ ì„±ëŠ¥ ê¸°ë¡
   - ë°œê²¬ëœ ë¬¸ì œì  ì •ë¦¬
   - Docker ë¶„ë¦¬ ì‹œ ê³ ë ¤ì‚¬í•­

2. **Docker ë¶„ë¦¬ ì¤€ë¹„**
   - Requirements ë¶„ì„ (YOLOv11, YOLO-World ë²„ì „)
   - Platform SDK ì„¤ê³„ ê²€í† 
   - ì˜ì¡´ì„± ì¶©ëŒ ì‚¬ì „ í™•ì¸

---

## ê²°ë¡ 

**ë³€ê²½ ìš”ì•½**:
- ê¸°ì¡´: 18ê°œ ëª¨ë¸ (YOLOv8, êµ¬í˜• timm)
- ê°œì„ : 27ê°œ ëª¨ë¸ (YOLOv11, YOLO-World, ìµœì‹  timm)
- ì¶”ê°€: Open-vocabulary detection, 2024 ìµœì‹  ëª¨ë¸ë“¤

**í•µì‹¬ ê°œì„ **:
1. âœ… YOLOv11 (2024 Sep) - 22% ë” íš¨ìœ¨ì 
2. âœ… YOLO-World (CVPR 2024) - Zero-shot detection
3. âœ… MobileNetV4/V5 (2024) - ìµœì‹  ëª¨ë°”ì¼
4. âœ… SigLIP-2 (2024) - 88.1% top-1 accuracy
5. âœ… EfficientNetV2 - ê¸°ì¡´ì— ëˆ„ë½ë¨

**ì˜ˆìƒ íš¨ê³¼**:
- í”Œë«í¼ì˜ ìµœì‹ ì„± ì…ì¦
- ì°¨ë³„í™”ëœ ê¸°ëŠ¥ ì œê³µ (open-vocab)
- ë‹¤ì–‘í•œ use case ì»¤ë²„

---

*Document Version: 2.0*
*Created: 2025-10-30*
*Author: Vision AI Platform Team*
